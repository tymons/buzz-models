{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Atmospheric Data - preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1300001 - hive\n",
    "# 1300002 - not hive?\n",
    "# 1400001 - hive szymanski\n",
    "# 1400002 - hive szymanski\n",
    "#hives_ids = [1300001, 1300002, 1400001, 1400002]\n",
    "hives_ids = [1300001]\n",
    "DATA_INIT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hive no. 1300001 | humidity temperature dataset size : (6776, 2) (6786, 2)\n",
      "Atmospheric data after merge: (6696, 2)\n",
      "Total atmoshpere dataset size: 6696\n"
     ]
    }
   ],
   "source": [
    "if DATA_INIT:\n",
    "    dfh_hives = [pd.read_csv(f\"measurements/{hive_id}/humidity.csv\") for hive_id in hives_ids]\n",
    "    dft_hives = [pd.read_csv(f\"measurements/{hive_id}/temperature.csv\") for hive_id in hives_ids]\n",
    "    dfh_hivesWithoutDuplicates = [dfh_hive.drop_duplicates(subset=['timestamp'], keep=False) for dfh_hive in dfh_hives]\n",
    "    dft_hivesWithoutDuplicates = [dft_hive.drop_duplicates(subset=['timestamp'], keep=False) for dft_hive in dft_hives]\n",
    "\n",
    "    for idx, hive_id in enumerate(hives_ids):\n",
    "        print(f\"Hive no. {hives_ids[idx]} | humidity temperature dataset size : {dfh_hivesWithoutDuplicates[idx].shape} {dft_hivesWithoutDuplicates[idx].shape}\")\n",
    "\n",
    "    df_hive = [pd.merge(\n",
    "        dfh_hivesWithoutDuplicates[idx], dft_hivesWithoutDuplicates[idx], on='timestamp', suffixes=(f\"_humidity_{hive_id}\",f\"_temperature_{hive_id}\"))\n",
    "               for idx, hive_id in enumerate(hives_ids)]\n",
    "\n",
    "    \n",
    "    total = 0\n",
    "    for atmosphere_data in df_hive:\n",
    "        atmosphere_data['timestamp'] = pd.to_datetime(atmosphere_data['timestamp'], format='%Y-%m-%dT%H-%M-%S').sort_values()\n",
    "        atmosphere_data.set_index('timestamp', inplace=True)\n",
    "        print(f\"Atmospheric data after merge: {atmosphere_data.shape}\")\n",
    "        total += atmosphere_data.shape[0]\n",
    "    \n",
    "    print(f\"Total atmoshpere dataset size: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "atmosphere_data.head()\n",
    "\n",
    "atmosphere_night = atmosphere_data.between_time(\"23:00\", \"3:30\")\n",
    "atmosphere_day = atmosphere_data.between_time(\"3:30\", \"23:00\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sound Data - preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import glob\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import preprocessing\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_INIT = True\n",
    "hives_ids = ['1300001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preparation for hive: 1300001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1664/1664 [00:14<00:00, 112.59it/s]\n"
     ]
    }
   ],
   "source": [
    "if DATA_INIT:\n",
    "    hive_sounds = []\n",
    "    hive_timestamps = []\n",
    "    for idx, hive_id in enumerate(hives_ids):\n",
    "        print(f\"Data preparation for hive: {hive_id}\")\n",
    "\n",
    "        sound_files = [f for f in glob.glob(f\"measurements\\\\{hive_id}\\\\sound*.csv\")]\n",
    "        sound_hive_list = []\n",
    "        for file in tqdm(sound_files):\n",
    "            df_samples = pd.read_csv(file)\n",
    "            pd_timestamp = pd.to_datetime(file.split(\"sound-\")[1].split(\".csv\")[0], format='%Y-%m-%dT%H-%M-%S')\n",
    "            if(len(df_samples.index) == 3000 and max(df_samples['samples'].values) < 4500):\n",
    "                np_samples = np.array(df_samples['samples'].values, dtype=\"float32\")\n",
    "                np_samples = np_samples / 4080\n",
    "                #np_samples = scaler.fit_transform(np_samples.reshape(-1, 1))\n",
    "                hive_sounds.append([pd_timestamp, np_samples])\n",
    "    sound_pd = pd.DataFrame(hive_sounds, columns=['timestamp', 'samples'])\n",
    "    sound_pd = sound_pd.set_index('timestamp')\n",
    "    np.save(\"night_day_sound_training.npy\", sound_pd)\n",
    "else:\n",
    "    sound_pd = np.load(\"night_day_sound_training.npy\", allow_pickle=True)\n",
    "    print(f\"Loaded: {len(hive_sounds)} sound recordings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_INIT:\n",
    "    sound_night = sound_pd.between_time(\"23:00\", \"3:30\")\n",
    "    sound_day = sound_pd.between_time(\"3:30\", \"23:00\")\n",
    "\n",
    "    mfccs_avg_labeled = []\n",
    "    for index, row in sound_night.iterrows():\n",
    "        full_mfccs = librosa.feature.mfcc(y=row['samples'], sr=3000, n_fft=512, hop_length=256, n_mfcc=14)\n",
    "        mfccs_avg_labeled.append([np.mean(full_mfccs,axis=1), 0])\n",
    "\n",
    "    for index, row in sound_day.iterrows():\n",
    "        full_mfccs = librosa.feature.mfcc(y=row['samples'], sr=3000, n_fft=512, hop_length=256, n_mfcc=14)\n",
    "        mfccs_avg_labeled.append([np.mean(full_mfccs,axis=1), 1])\n",
    "\n",
    "    np.random.shuffle(mfccs_avg_labeled)\n",
    "    np.save(\"night_day_mfcc_training.npy\", mfccs_avg_labeled)\n",
    "else:\n",
    "    mfccs_avg_labeled = np.load(\"night_day_mfcc_training.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge sound with atmospheric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in sound_night.iterrows():\n",
    "    atmosphere_nearest = atmosphere_night.iloc[atmosphere_night.index.get_loc(index, method='nearest')]\n",
    "    sound_night.loc[index, 'humidity'] = atmosphere_nearest[f\"value_humidity_{hives_ids[0]}\"]\n",
    "    sound_night.loc[index, 'temperature'] = atmosphere_nearest[f\"value_temperature_{hives_ids[0]}\"]\n",
    "    \n",
    "for index, row in sound_day.iterrows():\n",
    "    atmosphere_nearest = atmosphere_day.iloc[atmosphere_day.index.get_loc(index, method='nearest')]\n",
    "    sound_day.loc[index, 'humidity'] = atmosphere_nearest[f\"value_humidity_{hives_ids[0]}\"]\n",
    "    sound_day.loc[index, 'temperature'] = atmosphere_nearest[f\"value_temperature_{hives_ids[0]}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>humidity</th>\n",
       "      <th>temperature</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2019-08-02 23:01:37</td>\n",
       "      <td>[0.6110294, 0.60514706, 0.6022059, 0.60490197,...</td>\n",
       "      <td>56.50</td>\n",
       "      <td>34.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-02 23:17:37</td>\n",
       "      <td>[0.58235294, 0.5875, 0.59460783, 0.59338236, 0...</td>\n",
       "      <td>56.31</td>\n",
       "      <td>34.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-02 23:35:37</td>\n",
       "      <td>[0.58235294, 0.58210784, 0.5769608, 0.58235294...</td>\n",
       "      <td>56.37</td>\n",
       "      <td>34.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-02 23:50:37</td>\n",
       "      <td>[0.61764705, 0.62058824, 0.6019608, 0.5872549,...</td>\n",
       "      <td>56.18</td>\n",
       "      <td>34.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-03 00:05:37</td>\n",
       "      <td>[0.6044118, 0.6012255, 0.60539216, 0.5953431, ...</td>\n",
       "      <td>56.06</td>\n",
       "      <td>33.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-11 00:29:10</td>\n",
       "      <td>[0.5995098, 0.6039216, 0.59411764, 0.58137256,...</td>\n",
       "      <td>56.68</td>\n",
       "      <td>33.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-11 00:47:10</td>\n",
       "      <td>[0.5948529, 0.5852941, 0.5708333, 0.5860294, 0...</td>\n",
       "      <td>56.56</td>\n",
       "      <td>33.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-11 01:03:10</td>\n",
       "      <td>[0.5987745, 0.5894608, 0.58235294, 0.5767157, ...</td>\n",
       "      <td>56.68</td>\n",
       "      <td>33.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-11 01:18:10</td>\n",
       "      <td>[0.58235294, 0.5747549, 0.5833333, 0.5953431, ...</td>\n",
       "      <td>56.68</td>\n",
       "      <td>33.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-11 01:33:10</td>\n",
       "      <td>[0.56960785, 0.5656863, 0.5806373, 0.5897059, ...</td>\n",
       "      <td>56.56</td>\n",
       "      <td>33.87</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                               samples  \\\n",
       "timestamp                                                                \n",
       "2019-08-02 23:01:37  [0.6110294, 0.60514706, 0.6022059, 0.60490197,...   \n",
       "2019-08-02 23:17:37  [0.58235294, 0.5875, 0.59460783, 0.59338236, 0...   \n",
       "2019-08-02 23:35:37  [0.58235294, 0.58210784, 0.5769608, 0.58235294...   \n",
       "2019-08-02 23:50:37  [0.61764705, 0.62058824, 0.6019608, 0.5872549,...   \n",
       "2019-08-03 00:05:37  [0.6044118, 0.6012255, 0.60539216, 0.5953431, ...   \n",
       "...                                                                ...   \n",
       "2019-08-11 00:29:10  [0.5995098, 0.6039216, 0.59411764, 0.58137256,...   \n",
       "2019-08-11 00:47:10  [0.5948529, 0.5852941, 0.5708333, 0.5860294, 0...   \n",
       "2019-08-11 01:03:10  [0.5987745, 0.5894608, 0.58235294, 0.5767157, ...   \n",
       "2019-08-11 01:18:10  [0.58235294, 0.5747549, 0.5833333, 0.5953431, ...   \n",
       "2019-08-11 01:33:10  [0.56960785, 0.5656863, 0.5806373, 0.5897059, ...   \n",
       "\n",
       "                     humidity  temperature  \n",
       "timestamp                                   \n",
       "2019-08-02 23:01:37     56.50        34.06  \n",
       "2019-08-02 23:17:37     56.31        34.06  \n",
       "2019-08-02 23:35:37     56.37        34.06  \n",
       "2019-08-02 23:50:37     56.18        34.00  \n",
       "2019-08-03 00:05:37     56.06        33.93  \n",
       "...                       ...          ...  \n",
       "2019-08-11 00:29:10     56.68        33.96  \n",
       "2019-08-11 00:47:10     56.56        33.87  \n",
       "2019-08-11 01:03:10     56.68        33.93  \n",
       "2019-08-11 01:18:10     56.68        33.87  \n",
       "2019-08-11 01:33:10     56.56        33.87  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sound_night.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classification PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "standarized_mfcc_avg = StandardScaler().fit_transform([mfcc[0] for mfcc in mfccs_avg_labeled])\n",
    "pca = PCA(n_components=2)\n",
    "pc_data = pca.fit_transform(standarized_mfcc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classification t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(standarized_mfcc_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_data_labeled = list(zip(X_embedded, [mfcc[1] for mfcc in mfccs_avg_labeled]))\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "for data, label in tqdm(pc_data_labeled):\n",
    "    x, y = data\n",
    "    ax.scatter(x, y, c=colors[label], alpha=0.3)\n",
    "\n",
    "plt.title(\"Mfcc scatter plot\")\n",
    "plt.savefig('foo.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODER - BASIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-4\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3000, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 256))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2048, 3000), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as utils\n",
    "\n",
    "sound_dataset = utils.TensorDataset(torch.Tensor([x[0] for x in training_data_sound]),\n",
    "                                   torch.Tensor([x[1] for x in training_data_sound])) # create your datset\n",
    "                                    \n",
    "print(\"Length of complete sound dataset is\", len(sound_dataset))\n",
    "sound_trainset = torch.utils.data.DataLoader(sound_dataset, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for data in tqdm(sound_trainset):\n",
    "        X, y = data\n",
    "        model.zero_grad()\n",
    "        output = model(X)\n",
    "        loss = criterion(output, X)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # ===================log========================\n",
    "    print(f\"epoch [{epoch}/{num_epochs}], loss:{loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'autoencoder-basic-model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('autoencoder-basic-model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for data in tqdm(sound_trainset):\n",
    "        X, y = data\n",
    "        output = model(X)\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODER CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5,6,7,7,8,98,90,0]\n",
    "print(a[-3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=20, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    def weight_reset(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            m.reset_parameters()\n",
    "        \n",
    "        \n",
    "net = Net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-3)\n",
    "EPOCHS = 6\n",
    "\n",
    "net.weight_reset()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 784))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(\"Current loss is: \", loss.item())\n",
    "    if loss < 1e-5:\n",
    "        print(\"Loss threshold obtained!\")\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "print(\"Accuracy on test data: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "print(\"Accuracy on train data: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
