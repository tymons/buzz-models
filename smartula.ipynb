{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepare "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment prepares dataset containig normalized sound recordings extended with temperature and humidity values. For now we support only one hive for data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1300001 - hive\n",
    "# 1300002 - not hive?\n",
    "# 1400001 - hive szymanski\n",
    "# 1400002 - hive szymanski\n",
    "#hives_ids = [1300001, 1300002, 1400001, 1400002]\n",
    "\n",
    "hives_ids = [1300001, 1400001, 1400002]\n",
    "DATA_INIT = False\n",
    "\n",
    "night_start_hour = 21\n",
    "night_end_hour = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hive with id: 1300001 has atmospheric data with size: (6696, 2)\n",
      "Hive with id: 1400001 has atmospheric data with size: (2418, 2)\n",
      "Hive with id: 1400002 has atmospheric data with size: (1936, 2)\n",
      "Total atmoshpere dataset size: 11050\n"
     ]
    }
   ],
   "source": [
    "if DATA_INIT:\n",
    "    hives_temperature = [pd.read_csv(f\"measurements/{hive_id}/humidity.csv\") for hive_id in hives_ids]\n",
    "    hives_humidity = [pd.read_csv(f\"measurements/{hive_id}/temperature.csv\") for hive_id in hives_ids]\n",
    "    hives_humidity = [hive_hum.drop_duplicates(subset=['timestamp'], keep = False) for hive_hum in hives_humidity]\n",
    "    hives_temperature = [hive_tem.drop_duplicates(subset=['timestamp'], keep = False) for hive_tem in hives_temperature]\n",
    "    \n",
    "    hives_atmosphere = [pd.merge(\n",
    "        hives_humidity[idx], hives_temperature[idx], on='timestamp',\n",
    "        suffixes=(f\"_humidity_{hive_id}\",f\"_temperature_{hive_id}\"))\n",
    "               for idx, hive_id in enumerate(hives_ids)]\n",
    "   \n",
    "    total = 0\n",
    "    for idx, hive_atmosphere in enumerate(hives_atmosphere):\n",
    "        hive_atmosphere['timestamp'] = pd.to_datetime(\n",
    "            hive_atmosphere['timestamp'], format = '%Y-%m-%dT%H-%M-%S').sort_values()\n",
    "        hive_atmosphere.set_index('timestamp', inplace = True)\n",
    "        print(f\"Hive with id: {hives_ids[idx]} has atmospheric data with size: {hive_atmosphere.shape}\")\n",
    "        total += hive_atmosphere.shape[0]\n",
    "    \n",
    "    print(f\"Total atmoshpere dataset size: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with size 2936.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from scipy.fftpack import rfft\n",
    "from scipy.signal import blackman\n",
    "import librosa\n",
    "\n",
    "if DATA_INIT:\n",
    "    total_night_samples = 0\n",
    "    hives_data = []\n",
    "    for idx, hive_id in enumerate(hives_ids):\n",
    "        sound_files = [f for f in glob.glob(f\"measurements\\\\{hive_id}\\\\sound*.csv\")]\n",
    "        print(f\"Sound data preparation for hive: {hive_id} which has {len(sound_files)} recordings\")\n",
    "        for file in tqdm(sound_files):\n",
    "            df_samples = pd.read_csv(file)\n",
    "            pd_timestamp = pd.to_datetime(file.split(\"sound-\")[1].split(\".csv\")[0], format='%Y-%m-%dT%H-%M-%S')\n",
    "            if(len(df_samples.index) == 3000 and max(df_samples.values) < 4500):\n",
    "                np_samples = np.array(df_samples['samples'].values, dtype=\"float32\")\n",
    "                np_samples = np_samples / 4080\n",
    "                \n",
    "                # FFT, get rid of DC compoment by subtracting average\n",
    "                w = blackman(3000)\n",
    "                np_spectogram = rfft(w*(np_samples - np.mean(np_samples)))\n",
    "                np_spectogram = np.abs(np_spectogram)\n",
    "                \n",
    "                # MFCC\n",
    "                mfccs = librosa.feature.mfcc(y=np_samples, sr=3000, n_fft=512, hop_length=256, n_mfcc=14)\n",
    "                np_mfcc_avg = np.mean(mfccs, axis=1)\n",
    "                np_mfcc_var = np.var(mfccs, axis=1)\n",
    "\n",
    "                # Night label\n",
    "                is_night = (pd_timestamp.hour > night_start_hour or pd_timestamp.hour < night_end_hour)\n",
    "                if is_night: \n",
    "                    total_night_samples = total_night_samples + 1\n",
    "                \n",
    "                # Atmosphere\n",
    "                df_nerest = hives_atmosphere[idx].iloc[\n",
    "                    hives_atmosphere[idx].index.get_loc(pd_timestamp, method='nearest')]\n",
    "                \n",
    "                hives_data.append([pd_timestamp, hive_id, is_night,\n",
    "                                    df_nerest[f\"value_temperature_{hive_id}\"],\n",
    "                                    df_nerest[f\"value_humidity_{hive_id}\"],\n",
    "                                    np_samples, np_spectogram, np_mfcc_avg, np_mfcc_var])\n",
    "    \n",
    "    print(f'Creating dataset with {total_night_samples} night samples for data size: {len(hives_data)}')\n",
    "    print(f'Night percentage: {round(100*total_night_samples/len(hives_data), 2)}')\n",
    "    np.save('hives-data.npy', hives_data)\n",
    "else:\n",
    "    hives_data = np.load('hives-data.npy', allow_pickle = True)\n",
    "    print(f\"Loaded dataset with size {len(hives_data)}.\")\n",
    "    \n",
    "df_hives_data = pd.DataFrame(hives_data,\n",
    "                           columns=['timestamp', 'hive_id', 'is_night',\n",
    "                                    'temperature', 'humidity',\n",
    "                                    'samples', 'fft', 'mfcc_avg', 'mfcc_var'])\n",
    "df_hives_data = df_hives_data.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hive_id</th>\n",
       "      <th>is_night</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>samples</th>\n",
       "      <th>fft</th>\n",
       "      <th>mfcc_avg</th>\n",
       "      <th>mfcc_var</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2019-08-02 20:06:39</td>\n",
       "      <td>1300001</td>\n",
       "      <td>False</td>\n",
       "      <td>56.93</td>\n",
       "      <td>34.21</td>\n",
       "      <td>[0.5887255, 0.5865196, 0.5926471, 0.589951, 0....</td>\n",
       "      <td>[0.05721917318219877, 0.04508206282285264, 0.0...</td>\n",
       "      <td>[-358.4177321177189, 76.5997417203608, 7.55132...</td>\n",
       "      <td>[112.49110283839512, 97.55593689073913, 75.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-02 20:21:37</td>\n",
       "      <td>1300001</td>\n",
       "      <td>False</td>\n",
       "      <td>56.93</td>\n",
       "      <td>34.09</td>\n",
       "      <td>[0.5835784, 0.57916665, 0.57990193, 0.5901961,...</td>\n",
       "      <td>[0.11594218183563729, 0.10258726867713885, 0.1...</td>\n",
       "      <td>[-347.96401130058393, 83.9858585486864, 5.4264...</td>\n",
       "      <td>[123.00966802113419, 83.35277047728276, 37.325...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-02 20:37:37</td>\n",
       "      <td>1300001</td>\n",
       "      <td>False</td>\n",
       "      <td>56.81</td>\n",
       "      <td>34.09</td>\n",
       "      <td>[0.5776961, 0.5710784, 0.56397057, 0.5629902, ...</td>\n",
       "      <td>[0.11176600647627982, 0.0832705908633216, 0.20...</td>\n",
       "      <td>[-350.6411699488833, 76.15949173451945, 3.8318...</td>\n",
       "      <td>[151.86167025001237, 111.03726806371903, 146.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-02 20:55:37</td>\n",
       "      <td>1300001</td>\n",
       "      <td>False</td>\n",
       "      <td>56.75</td>\n",
       "      <td>34.18</td>\n",
       "      <td>[0.5897059, 0.5897059, 0.59387255, 0.6004902, ...</td>\n",
       "      <td>[0.09971700226105876, 0.07125987352405888, 0.1...</td>\n",
       "      <td>[-364.4164596121262, 94.39407039067773, 0.7766...</td>\n",
       "      <td>[76.72127394608977, 54.66498033200147, 31.0858...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-02 21:11:37</td>\n",
       "      <td>1300001</td>\n",
       "      <td>False</td>\n",
       "      <td>56.75</td>\n",
       "      <td>34.21</td>\n",
       "      <td>[0.59068626, 0.595098, 0.60269606, 0.6022059, ...</td>\n",
       "      <td>[0.14414751545603455, 0.12432094886995068, 0.1...</td>\n",
       "      <td>[-373.6403367366103, 84.52025568332569, 0.5508...</td>\n",
       "      <td>[158.66819376120802, 99.0386932970312, 84.2150...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-03 21:25:37</td>\n",
       "      <td>1300001</td>\n",
       "      <td>False</td>\n",
       "      <td>55.68</td>\n",
       "      <td>34.15</td>\n",
       "      <td>[0.5754902, 0.5715686, 0.5588235, 0.55710787, ...</td>\n",
       "      <td>[0.056908112580822054, 0.0689487417652858, 0.1...</td>\n",
       "      <td>[-363.26793389563426, 83.70740543719504, 1.276...</td>\n",
       "      <td>[157.2730706654439, 119.01149711448284, 73.813...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-03 21:43:37</td>\n",
       "      <td>1300001</td>\n",
       "      <td>False</td>\n",
       "      <td>55.75</td>\n",
       "      <td>34.09</td>\n",
       "      <td>[0.60539216, 0.60514706, 0.5921569, 0.595098, ...</td>\n",
       "      <td>[0.17881282780045848, 0.13517365079102106, 0.1...</td>\n",
       "      <td>[-340.2865850217681, 63.439747523888634, -1.26...</td>\n",
       "      <td>[50.859583042176496, 25.17963425025422, 33.853...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-03 21:59:37</td>\n",
       "      <td>1300001</td>\n",
       "      <td>False</td>\n",
       "      <td>55.93</td>\n",
       "      <td>34.18</td>\n",
       "      <td>[0.62058824, 0.6276961, 0.61446077, 0.59044117...</td>\n",
       "      <td>[0.12176968970515319, 0.13142249378404114, 0.1...</td>\n",
       "      <td>[-328.6537645279214, 63.60982191354608, -13.68...</td>\n",
       "      <td>[558.6972224075247, 206.94427968116545, 61.783...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-03 22:15:37</td>\n",
       "      <td>1300001</td>\n",
       "      <td>True</td>\n",
       "      <td>55.68</td>\n",
       "      <td>34.09</td>\n",
       "      <td>[0.5833333, 0.577451, 0.58210784, 0.5889706, 0...</td>\n",
       "      <td>[0.05162019850470621, 0.044085736864532904, 0....</td>\n",
       "      <td>[-351.62045406825695, 68.47484969627199, -1.88...</td>\n",
       "      <td>[194.94986028025892, 197.85678250418198, 30.98...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2019-08-03 22:30:37</td>\n",
       "      <td>1300001</td>\n",
       "      <td>True</td>\n",
       "      <td>55.75</td>\n",
       "      <td>34.12</td>\n",
       "      <td>[0.60269606, 0.59411764, 0.5948529, 0.5955882,...</td>\n",
       "      <td>[0.10638125697601766, 0.09035855447061651, 0.1...</td>\n",
       "      <td>[-354.33540503993635, 76.90382768570305, -5.83...</td>\n",
       "      <td>[111.58099677218172, 73.7902334913612, 19.3174...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     hive_id is_night temperature humidity  \\\n",
       "timestamp                                                    \n",
       "2019-08-02 20:06:39  1300001    False       56.93    34.21   \n",
       "2019-08-02 20:21:37  1300001    False       56.93    34.09   \n",
       "2019-08-02 20:37:37  1300001    False       56.81    34.09   \n",
       "2019-08-02 20:55:37  1300001    False       56.75    34.18   \n",
       "2019-08-02 21:11:37  1300001    False       56.75    34.21   \n",
       "...                      ...      ...         ...      ...   \n",
       "2019-08-03 21:25:37  1300001    False       55.68    34.15   \n",
       "2019-08-03 21:43:37  1300001    False       55.75    34.09   \n",
       "2019-08-03 21:59:37  1300001    False       55.93    34.18   \n",
       "2019-08-03 22:15:37  1300001     True       55.68    34.09   \n",
       "2019-08-03 22:30:37  1300001     True       55.75    34.12   \n",
       "\n",
       "                                                               samples  \\\n",
       "timestamp                                                                \n",
       "2019-08-02 20:06:39  [0.5887255, 0.5865196, 0.5926471, 0.589951, 0....   \n",
       "2019-08-02 20:21:37  [0.5835784, 0.57916665, 0.57990193, 0.5901961,...   \n",
       "2019-08-02 20:37:37  [0.5776961, 0.5710784, 0.56397057, 0.5629902, ...   \n",
       "2019-08-02 20:55:37  [0.5897059, 0.5897059, 0.59387255, 0.6004902, ...   \n",
       "2019-08-02 21:11:37  [0.59068626, 0.595098, 0.60269606, 0.6022059, ...   \n",
       "...                                                                ...   \n",
       "2019-08-03 21:25:37  [0.5754902, 0.5715686, 0.5588235, 0.55710787, ...   \n",
       "2019-08-03 21:43:37  [0.60539216, 0.60514706, 0.5921569, 0.595098, ...   \n",
       "2019-08-03 21:59:37  [0.62058824, 0.6276961, 0.61446077, 0.59044117...   \n",
       "2019-08-03 22:15:37  [0.5833333, 0.577451, 0.58210784, 0.5889706, 0...   \n",
       "2019-08-03 22:30:37  [0.60269606, 0.59411764, 0.5948529, 0.5955882,...   \n",
       "\n",
       "                                                                   fft  \\\n",
       "timestamp                                                                \n",
       "2019-08-02 20:06:39  [0.05721917318219877, 0.04508206282285264, 0.0...   \n",
       "2019-08-02 20:21:37  [0.11594218183563729, 0.10258726867713885, 0.1...   \n",
       "2019-08-02 20:37:37  [0.11176600647627982, 0.0832705908633216, 0.20...   \n",
       "2019-08-02 20:55:37  [0.09971700226105876, 0.07125987352405888, 0.1...   \n",
       "2019-08-02 21:11:37  [0.14414751545603455, 0.12432094886995068, 0.1...   \n",
       "...                                                                ...   \n",
       "2019-08-03 21:25:37  [0.056908112580822054, 0.0689487417652858, 0.1...   \n",
       "2019-08-03 21:43:37  [0.17881282780045848, 0.13517365079102106, 0.1...   \n",
       "2019-08-03 21:59:37  [0.12176968970515319, 0.13142249378404114, 0.1...   \n",
       "2019-08-03 22:15:37  [0.05162019850470621, 0.044085736864532904, 0....   \n",
       "2019-08-03 22:30:37  [0.10638125697601766, 0.09035855447061651, 0.1...   \n",
       "\n",
       "                                                              mfcc_avg  \\\n",
       "timestamp                                                                \n",
       "2019-08-02 20:06:39  [-358.4177321177189, 76.5997417203608, 7.55132...   \n",
       "2019-08-02 20:21:37  [-347.96401130058393, 83.9858585486864, 5.4264...   \n",
       "2019-08-02 20:37:37  [-350.6411699488833, 76.15949173451945, 3.8318...   \n",
       "2019-08-02 20:55:37  [-364.4164596121262, 94.39407039067773, 0.7766...   \n",
       "2019-08-02 21:11:37  [-373.6403367366103, 84.52025568332569, 0.5508...   \n",
       "...                                                                ...   \n",
       "2019-08-03 21:25:37  [-363.26793389563426, 83.70740543719504, 1.276...   \n",
       "2019-08-03 21:43:37  [-340.2865850217681, 63.439747523888634, -1.26...   \n",
       "2019-08-03 21:59:37  [-328.6537645279214, 63.60982191354608, -13.68...   \n",
       "2019-08-03 22:15:37  [-351.62045406825695, 68.47484969627199, -1.88...   \n",
       "2019-08-03 22:30:37  [-354.33540503993635, 76.90382768570305, -5.83...   \n",
       "\n",
       "                                                              mfcc_var  \n",
       "timestamp                                                               \n",
       "2019-08-02 20:06:39  [112.49110283839512, 97.55593689073913, 75.592...  \n",
       "2019-08-02 20:21:37  [123.00966802113419, 83.35277047728276, 37.325...  \n",
       "2019-08-02 20:37:37  [151.86167025001237, 111.03726806371903, 146.1...  \n",
       "2019-08-02 20:55:37  [76.72127394608977, 54.66498033200147, 31.0858...  \n",
       "2019-08-02 21:11:37  [158.66819376120802, 99.0386932970312, 84.2150...  \n",
       "...                                                                ...  \n",
       "2019-08-03 21:25:37  [157.2730706654439, 119.01149711448284, 73.813...  \n",
       "2019-08-03 21:43:37  [50.859583042176496, 25.17963425025422, 33.853...  \n",
       "2019-08-03 21:59:37  [558.6972224075247, 206.94427968116545, 61.783...  \n",
       "2019-08-03 22:15:37  [194.94986028025892, 197.85678250418198, 30.98...  \n",
       "2019-08-03 22:30:37  [111.58099677218172, 73.7902334913612, 19.3174...  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hives_data.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-mean clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# X_fft_values = StandardScaler().fit_transform(np.stack(df_data['fft'].values))\n",
    "# X_temperature = StandardScaler().fit_transform(np.array(df_data['temperature'].values).reshape(-1, 1))\n",
    "# X_humidity = StandardScaler().fit_transform(np.array(df_data['humidity'].values).reshape(-1, 1))\n",
    "X_mfcc_avg = StandardScaler().fit_transform(np.stack(df_data['mfcc_avg'].values))\n",
    "X_mfcc_var = StandardScaler().fit_transform(np.stack(df_data['mfcc_var'].values))\n",
    "X_lpcs = StandardScaler().fit_transform(np.stack(df_data['lpcs'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temperature = np.array(df_data['temperature'].values)\n",
    "X_humidity = np.array(df_data['humidity'].values)\n",
    "X_mfcc_avg = np.stack(df_data['mfcc_avg'].values)\n",
    "X_mfcc_var = np.stack(df_data['mfcc_var'].values)\n",
    "X_lpcs = np.stack(df_data['lpcs'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for idx, samples in enumerate(X_lpcs):\n",
    "    X.append(np.append(samples, np.append(X_temperature[idx], X_humidity[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_lpcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mfcc_con = np.concatenate((X_mfcc_avg, X_mfcc_var), axis = 1)\n",
    "X_mfcc_con = StandardScaler().fit_transform(X_mfcc_con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit( )\n",
    "predicted = kmeans.predict(X_fft_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fft_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_kmean = sum(predicted)\n",
    "actual = int(sum(df_data['night_label'].values))\n",
    "size_data = df_data.shape[0]\n",
    "print(f'Predicted with kmean: {predicted_kmean} vs {actual}/{size_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "reduced = PCA(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = []\n",
    "for idx, pca_reduced in enumerate(reduced): \n",
    "    X_reduced.append(np.append(pca_reduced, np.append(X_temperature[idx], X_humidity[idx])))\n",
    "\n",
    "X_reduced = StandardScaler().fit_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_sec = PCA(n_components=2).fit_transform(X_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "reduced = TSNE(n_components=2, perplexity=100, learning_rate=500, verbose=1).fit_transform(X_mfcc_avg)\n",
    "reduced_sec = reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "colors = ['red', 'green']\n",
    "\n",
    "x = [data[0] for data in reduced]\n",
    "y = [data[1] for data in reduced]\n",
    "values = [colors[int(data)] for data in df_data['night_label'].values]\n",
    "scatter = plt.scatter(x, y,c=values, alpha = 0.3)\n",
    "#plt.legend([1, 0], ['night', 'day'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "# Configure Plotly to be rendered inline in the notebook.\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "colors = [data for data in df_data['night_label'].values]\n",
    "# Configure the trace.\n",
    "trace = go.Scatter3d(\n",
    "    x=[data[0] for data in reduced],  # <-- Put your data instead\n",
    "    y=[data[1] for data in reduced],  # <-- Put your data instead\n",
    "    z=[data[2] for data in reduced],  # <-- Put your data instead\n",
    "    mode='markers',\n",
    "    marker={\n",
    "        'size': 10,\n",
    "        'opacity': 0.8,\n",
    "        'color': colors\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure the layout.\n",
    "layout = go.Layout(\n",
    "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "plot_figure = go.Figure(data=data, layout=layout)\n",
    "\n",
    "# Render the plot.\n",
    "plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block assumes that we have data in <code>df_data</code> variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "data_mfcc_labeled = []\n",
    "night_timestamps = df_data.between_time(\"23:00\", \"3:30\").index.values.tolist()\n",
    "night_timestamps = pd.to_datetime(night_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_labeled = df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_labeled['labeled'] = [int(index in night_timestamps) for index in df_data.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(df_data.iterrows(), total=df_data.shape[0]):\n",
    "    full_mfccs = librosa.feature.mfcc(y=row['samples'], sr=3000, n_fft=512, hop_length=256, n_mfcc=14)\n",
    "    data_mfcc_labeled.append([np.mean(full_mfccs,axis=1), row['temperature'], row['humidity'],\n",
    "                              int(index in night_timestamps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODER - BASIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_INIT = True\n",
    "import glob \n",
    "\n",
    "auto_hives = [1300001, 1300002, 1400001, 1400002]\n",
    "if DATA_INIT:\n",
    "    auto_max_sample_value = 0\n",
    "    auto_hive_sounds = []\n",
    "    auto_hive_timestamps = []\n",
    "    for idx, hive_id in enumerate(auto_hives):\n",
    "        print(f\"Data preparation for hive: {hive_id}\")\n",
    "\n",
    "        auto_sound_files = [f for f in glob.glob(f\"measurements\\\\{hive_id}\\\\sound*.csv\")]\n",
    "        auto_sound_hive_list = []\n",
    "        for file in tqdm(auto_sound_files):\n",
    "            df_samples = pd.read_csv(file)\n",
    "            pd_timestamp = pd.to_datetime(file.split(\"sound-\")[1].split(\".csv\")[0], format='%Y-%m-%dT%H-%M-%S')\n",
    "            if(len(df_samples.index) == 3000 and max(df_samples['samples'].values) < 4500):\n",
    "                np_samples = np.array(df_samples['samples'].values, dtype=\"float32\")\n",
    "                np_samples = np_samples / 4080\n",
    "                auto_hive_sounds.append([pd_timestamp, np_samples])\n",
    "    np.random.shuffle(auto_hive_sounds)\n",
    "    sound_pd = pd.DataFrame(auto_hive_sounds, columns=['timestamp', 'samples'])\n",
    "    auto_df_data = sound_pd.set_index('timestamp')\n",
    "    auto_df_data = sound_pd\n",
    "else:\n",
    "    sound_pd = np.load(f\"{hives_ids[0]}-data.npy\", allow_pickle=True)\n",
    "    print(f\"Loaded dataset with size {len(hive_sounds)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'We are running on : {device}')\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-5\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3000, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2048, 3000), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data = torch.FloatTensor([auto_df_data['samples'].values])\n",
    "tensor_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder()\n",
    "model.to(device)\n",
    "\n",
    "losses = []\n",
    "\n",
    "def train(net):\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    optimizer = torch.optim.RMSprop(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for i in tqdm(range(0, tensor_data.size()[1], BATCH_SIZE)):\n",
    "            batch_X = tensor_data[:, i:i+BATCH_SIZE, :]\n",
    "            batch_X = batch_X.to(device)\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output = net(batch_X)\n",
    "            loss = criterion(output, batch_X)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            train_loss += loss.item()\n",
    "        # ===================log========================\n",
    "        loss_per_epoch = train_loss/tensor_data.size()[1]\n",
    "        print(f\"epoch [{epoch}/{num_epochs}], loss:{loss_per_epoch}\")\n",
    "        losses.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Autoencoder loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'autoencoder-06-11-2019-model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_tensors = torch.FloatTensor([df_data_labeled['samples'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = list(zip(sound_tensors, df_data_labeled['labeled'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sound_tensors = sound_tensors.to(device)\n",
    "    output = model.encoder(sound_tensors)\n",
    "    output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cpu = output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list_val = df_data_labeled['labeled'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classification PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarized_mfcc_avg = StandardScaler().fit_transform([data[0] for data in data_mfcc_labeled])\n",
    "#standarized_temp = StandardScaler().fit_transform([[data[1]] for data in data_mfcc_labeled])\n",
    "#standarized_hum = StandardScaler().fit_transform([[data[2]] for data in data_mfcc_labeled])\n",
    "\n",
    "standarized_autoencoder = StandardScaler().fit_transform(output.cpu())\n",
    "\n",
    "#zipped_data = list(zip(standarized_mfcc_avg, standarized_temp, standarized_hum))\n",
    "#merged_data = [np.concatenate(list_to_con) for list_to_con in zipped_data]\n",
    "#standarized_merged_data = StandardScaler().fit_transform(merged_data)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pc_data = pca.fit_transform(standarized_autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classification t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(output_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pc_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pc_data_labeled = list(zip(pc_data, [mfcc_data[3] for mfcc_data in data_mfcc_labeled]))\n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "labels = ['day', 'night']\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# ax.scatter(x=[data[0][0] for data in pc_data_labeled],\n",
    "#            y=[data[0][1] for data in pc_data_labeled],\n",
    "#            c=[colors[int(data[1])] for data in pc_data_labeled],\n",
    "#           alpha=0.3) \n",
    "ax.scatter(x=[data[0] for data in pc_data],\n",
    "           y=[data[1] for data in pc_data],\n",
    "           #c=[colors[data] for data in labels_list_val],\n",
    "          alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.title(\"Autoencoder scatter plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('autoencoder-06-11-2019-model.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.MNIST(root='mnist', train=True,\n",
    "                                   download=False, transform=transform)\n",
    "test_data = datasets.MNIST(root='mnist', train=False,\n",
    "                                  download=False, transform=transform)\n",
    "\n",
    "# Create training and test dataloaders\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        ## encoder ##\n",
    "        # linear layer (784 -> encoding_dim)\n",
    "        self.fc1 = nn.Linear(28 * 28, encoding_dim)\n",
    "        \n",
    "        ## decoder ##\n",
    "        # linear layer (encoding_dim -> input size)\n",
    "        self.fc2 = nn.Linear(encoding_dim, 28*28)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # add layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # output layer (sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "encoding_dim = 32\n",
    "model = Autoencoder(encoding_dim).to(device)\n",
    "print(model)\n",
    "\n",
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        images, _ = data\n",
    "        # flatten images\n",
    "        images = images.view(images.size(0), -1).to(device)\n",
    "        print(images.shape)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        print(images.size(0))\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpc(y, m):\n",
    "    \"Return m linear predictive coefficients for sequence y using Levinson-Durbin prediction algorithm\"\n",
    "    #step 1: compute autoregression coefficients R_0, ..., R_m\n",
    "    R = [y.dot(y)] \n",
    "    if R[0] == 0:\n",
    "        return [1] + [0] * (m-2) + [-1]\n",
    "    else:\n",
    "        for i in range(1, m + 1):\n",
    "            r = y[i:].dot(y[:-i])\n",
    "            R.append(r)\n",
    "        R = np.array(R)\n",
    "    #step 2: \n",
    "        A = np.array([1, -R[1] / R[0]])\n",
    "        E = R[0] + R[1] * A[1]\n",
    "        for k in range(1, m):\n",
    "            if (E == 0):\n",
    "                E = 10e-17\n",
    "            alpha = - A[:k+1].dot(R[k+1:0:-1]) / E\n",
    "            A = np.hstack([A,0])\n",
    "            A = A + alpha * A[::-1]\n",
    "            E *= (1 - alpha**2)\n",
    "        return A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
