{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data prepare "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This segment prepares dataset containig normalized sound recordings extended with temperature and humidity values. For now we support only one hive for data preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1300001 - hive\n",
    "# 1300002 - not hive?\n",
    "# 1400001 - hive szymanski\n",
    "# 1400002 - hive szymanski\n",
    "#hives_ids = [1300001, 1300002, 1400001, 1400002]\n",
    "\n",
    "hives_ids = [1300001]\n",
    "DATA_INIT = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hive no. 1300001 | humidity temperature dataset size : (6776, 2) (6786, 2)\n",
      "Atmospheric data after merge: (6696, 2)\n",
      "Total atmoshpere dataset size: 6696\n"
     ]
    }
   ],
   "source": [
    "if DATA_INIT:\n",
    "    dfh_hives = [pd.read_csv(f\"measurements/{hive_id}/humidity.csv\") for hive_id in hives_ids]\n",
    "    dft_hives = [pd.read_csv(f\"measurements/{hive_id}/temperature.csv\") for hive_id in hives_ids]\n",
    "    dfh_hivesWithoutDuplicates = [dfh_hive.drop_duplicates(subset=['timestamp'], keep=False) for dfh_hive in dfh_hives]\n",
    "    dft_hivesWithoutDuplicates = [dft_hive.drop_duplicates(subset=['timestamp'], keep=False) for dft_hive in dft_hives]\n",
    "\n",
    "    for idx, hive_id in enumerate(hives_ids):\n",
    "        print(f\"Hive no. {hives_ids[idx]} | humidity temperature dataset size : {dfh_hivesWithoutDuplicates[idx].shape} {dft_hivesWithoutDuplicates[idx].shape}\")\n",
    "\n",
    "    df_hive = [pd.merge(\n",
    "        dfh_hivesWithoutDuplicates[idx], dft_hivesWithoutDuplicates[idx], on='timestamp', suffixes=(f\"_humidity_{hive_id}\",f\"_temperature_{hive_id}\"))\n",
    "               for idx, hive_id in enumerate(hives_ids)]\n",
    "\n",
    "    \n",
    "    total = 0\n",
    "    for atmosphere_data in df_hive:\n",
    "        atmosphere_data['timestamp'] = pd.to_datetime(atmosphere_data['timestamp'], format='%Y-%m-%dT%H-%M-%S').sort_values()\n",
    "        atmosphere_data.set_index('timestamp', inplace=True)\n",
    "        print(f\"Atmospheric data after merge: {atmosphere_data.shape}\")\n",
    "        total += atmosphere_data.shape[0]\n",
    "    \n",
    "    print(f\"Total atmoshpere dataset size: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound data preparation for hive: 1300001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1664/1664 [00:04<00:00, 340.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending night-labels for hive: 1300001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [00:00<00:00, 2634.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending atmospheric data for hive: 1300001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1652/1652 [00:01<00:00, 967.69it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from scipy.fftpack import rfft\n",
    "from scipy.signal import blackman\n",
    "\n",
    "if DATA_INIT:\n",
    "    max_val = 0\n",
    "    hive_sounds = []\n",
    "    hive_timestamps = []\n",
    "    for idx, hive_id in enumerate(hives_ids):\n",
    "        print(f\"Sound data preparation for hive: {hive_id}\")\n",
    "\n",
    "        sound_files = [f for f in glob.glob(f\"measurements\\\\{hive_id}\\\\sound*.csv\")]\n",
    "        sound_hive_list = []\n",
    "        for file in tqdm(sound_files):\n",
    "            df_samples = pd.read_csv(file)\n",
    "            pd_timestamp = pd.to_datetime(file.split(\"sound-\")[1].split(\".csv\")[0], format='%Y-%m-%dT%H-%M-%S')\n",
    "            if(len(df_samples.index) == 3000 and max(df_samples.values) < 4500):\n",
    "                np_samples = np.array(df_samples['samples'].values, dtype=\"float32\")\n",
    "                np_samples = np_samples / 4080\n",
    "                \n",
    "                # FFT, get rid of DC compoment by subtracting average\n",
    "                w = blackman(3000)\n",
    "                np_spectogram = rfft(w*(np_samples - np.mean(np_samples)))\n",
    "                np_spectogram = np.abs(np_spectogram)\n",
    "                \n",
    "                hive_sounds.append([pd_timestamp, np_samples, np_spectogram])\n",
    "    df_data = pd.DataFrame(hive_sounds, columns=['timestamp', 'samples', 'fft'])\n",
    "    df_data = df_data.set_index('timestamp')\n",
    "        \n",
    "    # Append label\n",
    "    print(f\"Appending night-labels for hive: {hive_id}\")\n",
    "    night_timestamps = df_data.between_time(\"23:00\", \"3:30\").index.values.tolist()\n",
    "    night_timestamps = pd.to_datetime(night_timestamps)\n",
    "    for index, row in tqdm(df_data.iterrows(), total=df_data.shape[0]):\n",
    "        df_data.loc[index, 'night_label'] = int(index in night_timestamps)\n",
    "    \n",
    "    # Merge with atmosphere data\n",
    "    print(f\"Appending atmospheric data for hive: {hive_id}\")\n",
    "    for index, row in tqdm(df_data.iterrows(), total=df_data.shape[0]):\n",
    "        atmosphere_nearest = atmosphere_data.iloc[atmosphere_data.index.get_loc(index, method='nearest')]\n",
    "        df_data.loc[index, 'humidity'] = atmosphere_nearest[f\"value_humidity_{hives_ids[0]}\"]\n",
    "        df_data.loc[index, 'temperature'] = atmosphere_nearest[f\"value_temperature_{hives_ids[0]}\"]\n",
    "    \n",
    "    #np.save(f\"{hives_ids[0]}-data.npy\", sound_pd)\n",
    "else:\n",
    "    sound_pd = np.load(f\"{hives_ids[0]}-data.npy\", allow_pickle=True)\n",
    "    print(f\"Loaded dataset with size {len(hive_sounds)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-mean clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.fftpack import fftfreq, fftshift\n",
    "\n",
    "N = 3000\n",
    "T = 1.0 / float(N)\n",
    "x = np.linspace(0.0, N*T, N)\n",
    "\n",
    "yf = df_data['fft'].loc[df_data.index[0]]\n",
    "xf = np.linspace(0.0, 1.0/(2.0*T), N//2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(yf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_fft_values = StandardScaler().fit_transform(np.stack(df_data['fft'].values))\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X_fft_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = kmeans.predict(X_fft_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1652, 3000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_fft_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "reduced = PCA(n_components=3).fit_transform(X_fft_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TSNE\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "reduced = TSNE(n_components=3, perplexity=10, learning_rate=100, verbose=1).fit_transform(X_fft_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-692238d3f553>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m plt.scatter(\n\u001b[0;32m      8\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreduced\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreduced\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'night_label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-692238d3f553>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m plt.scatter(\n\u001b[0;32m      8\u001b[0m     \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreduced\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreduced\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'night_label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "plt.scatter(\n",
    "    x=[data[0] for data in reduced],\n",
    "    y=[data[1] for data in reduced],\n",
    "    c=[colors[int(data)] for data in df_data['night_label'].values],\n",
    "    alpha = 0.3\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "# Configure Plotly to be rendered inline in the notebook.\n",
    "plotly.offline.init_notebook_mode()\n",
    "\n",
    "colors = [data for data in df_data['night_label'].values]\n",
    "# Configure the trace.\n",
    "trace = go.Scatter3d(\n",
    "    x=[data[0] for data in reduced],  # <-- Put your data instead\n",
    "    y=[data[1] for data in reduced],  # <-- Put your data instead\n",
    "    z=[data[2] for data in reduced],  # <-- Put your data instead\n",
    "    mode='markers',\n",
    "    marker={\n",
    "        'size': 10,\n",
    "        'opacity': 0.8,\n",
    "        'color': colors\n",
    "    }\n",
    ")\n",
    "\n",
    "# Configure the layout.\n",
    "layout = go.Layout(\n",
    "    margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
    ")\n",
    "\n",
    "data = [trace]\n",
    "\n",
    "plot_figure = go.Figure(data=data, layout=layout)\n",
    "\n",
    "# Render the plot.\n",
    "plotly.offline.iplot(plot_figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block assumes that we have data in <code>df_data</code> variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "data_mfcc_labeled = []\n",
    "night_timestamps = df_data.between_time(\"23:00\", \"3:30\").index.values.tolist()\n",
    "night_timestamps = pd.to_datetime(night_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_labeled = df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_labeled['labeled'] = [int(index in night_timestamps) for index in df_data.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in tqdm(df_data.iterrows(), total=df_data.shape[0]):\n",
    "    full_mfccs = librosa.feature.mfcc(y=row['samples'], sr=3000, n_fft=512, hop_length=256, n_mfcc=14)\n",
    "    data_mfcc_labeled.append([np.mean(full_mfccs,axis=1), row['temperature'], row['humidity'],\n",
    "                              int(index in night_timestamps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOENCODER - BASIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_INIT = True\n",
    "import glob \n",
    "\n",
    "auto_hives = [1300001, 1300002, 1400001, 1400002]\n",
    "if DATA_INIT:\n",
    "    auto_max_sample_value = 0\n",
    "    auto_hive_sounds = []\n",
    "    auto_hive_timestamps = []\n",
    "    for idx, hive_id in enumerate(auto_hives):\n",
    "        print(f\"Data preparation for hive: {hive_id}\")\n",
    "\n",
    "        auto_sound_files = [f for f in glob.glob(f\"measurements\\\\{hive_id}\\\\sound*.csv\")]\n",
    "        auto_sound_hive_list = []\n",
    "        for file in tqdm(auto_sound_files):\n",
    "            df_samples = pd.read_csv(file)\n",
    "            pd_timestamp = pd.to_datetime(file.split(\"sound-\")[1].split(\".csv\")[0], format='%Y-%m-%dT%H-%M-%S')\n",
    "            if(len(df_samples.index) == 3000 and max(df_samples['samples'].values) < 4500):\n",
    "                np_samples = np.array(df_samples['samples'].values, dtype=\"float32\")\n",
    "                np_samples = np_samples / 4080\n",
    "                auto_hive_sounds.append([pd_timestamp, np_samples])\n",
    "    np.random.shuffle(auto_hive_sounds)\n",
    "    sound_pd = pd.DataFrame(auto_hive_sounds, columns=['timestamp', 'samples'])\n",
    "    auto_df_data = sound_pd.set_index('timestamp')\n",
    "    auto_df_data = sound_pd\n",
    "else:\n",
    "    sound_pd = np.load(f\"{hives_ids[0]}-data.npy\", allow_pickle=True)\n",
    "    print(f\"Loaded dataset with size {len(hive_sounds)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'We are running on : {device}')\n",
    "\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-5\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(3000, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 256)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(2048, 3000), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_data = torch.FloatTensor([auto_df_data['samples'].values])\n",
    "tensor_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder()\n",
    "model.to(device)\n",
    "\n",
    "losses = []\n",
    "\n",
    "def train(net):\n",
    "    criterion = nn.MSELoss(reduction='sum')\n",
    "    optimizer = torch.optim.RMSprop(net.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for i in tqdm(range(0, tensor_data.size()[1], BATCH_SIZE)):\n",
    "            batch_X = tensor_data[:, i:i+BATCH_SIZE, :]\n",
    "            batch_X = batch_X.to(device)\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output = net(batch_X)\n",
    "            loss = criterion(output, batch_X)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            train_loss += loss.item()\n",
    "        # ===================log========================\n",
    "        loss_per_epoch = train_loss/tensor_data.size()[1]\n",
    "        print(f\"epoch [{epoch}/{num_epochs}], loss:{loss_per_epoch}\")\n",
    "        losses.append(loss_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.title(\"Autoencoder loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'autoencoder-06-11-2019-model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sound_tensors = torch.FloatTensor([df_data_labeled['samples'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sss = list(zip(sound_tensors, df_data_labeled['labeled'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    sound_tensors = sound_tensors.to(device)\n",
    "    output = model.encoder(sound_tensors)\n",
    "    output = output.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cpu = output.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_list_val = df_data_labeled['labeled'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classification PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standarized_mfcc_avg = StandardScaler().fit_transform([data[0] for data in data_mfcc_labeled])\n",
    "#standarized_temp = StandardScaler().fit_transform([[data[1]] for data in data_mfcc_labeled])\n",
    "#standarized_hum = StandardScaler().fit_transform([[data[2]] for data in data_mfcc_labeled])\n",
    "\n",
    "standarized_autoencoder = StandardScaler().fit_transform(output.cpu())\n",
    "\n",
    "#zipped_data = list(zip(standarized_mfcc_avg, standarized_temp, standarized_hum))\n",
    "#merged_data = [np.concatenate(list_to_con) for list_to_con in zipped_data]\n",
    "#standarized_merged_data = StandardScaler().fit_transform(merged_data)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pc_data = pca.fit_transform(standarized_autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic classification t-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(output_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pc_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pc_data_labeled = list(zip(pc_data, [mfcc_data[3] for mfcc_data in data_mfcc_labeled]))\n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "labels = ['day', 'night']\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# ax.scatter(x=[data[0][0] for data in pc_data_labeled],\n",
    "#            y=[data[0][1] for data in pc_data_labeled],\n",
    "#            c=[colors[int(data[1])] for data in pc_data_labeled],\n",
    "#           alpha=0.3) \n",
    "ax.scatter(x=[data[0] for data in pc_data],\n",
    "           y=[data[1] for data in pc_data],\n",
    "           #c=[colors[data] for data in labels_list_val],\n",
    "          alpha=0.3)\n",
    "ax.legend()\n",
    "\n",
    "plt.title(\"Autoencoder scatter plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('autoencoder-06-11-2019-model.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# load the training and test datasets\n",
    "train_data = datasets.MNIST(root='mnist', train=True,\n",
    "                                   download=False, transform=transform)\n",
    "test_data = datasets.MNIST(root='mnist', train=False,\n",
    "                                  download=False, transform=transform)\n",
    "\n",
    "# Create training and test dataloaders\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# get one image from the batch\n",
    "img = np.squeeze(images[0])\n",
    "\n",
    "fig = plt.figure(figsize = (5,5)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        ## encoder ##\n",
    "        # linear layer (784 -> encoding_dim)\n",
    "        self.fc1 = nn.Linear(28 * 28, encoding_dim)\n",
    "        \n",
    "        ## decoder ##\n",
    "        # linear layer (encoding_dim -> input size)\n",
    "        self.fc2 = nn.Linear(encoding_dim, 28*28)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # add layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # output layer (sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "encoding_dim = 32\n",
    "model = Autoencoder(encoding_dim).to(device)\n",
    "print(model)\n",
    "\n",
    "# specify loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# specify loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 20\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in train_loader:\n",
    "        # _ stands in for labels, here\n",
    "        images, _ = data\n",
    "        # flatten images\n",
    "        images = images.view(images.size(0), -1).to(device)\n",
    "        print(images.shape)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = model(images)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, images)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        print(images.size(0))\n",
    "        train_loss += loss.item()*images.size(0)\n",
    "            \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(\n",
    "        epoch, \n",
    "        train_loss\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
