{
    "features": {
        "spectrogram": {
            "nfft": 4096,
            "hop_len": 1395,
            "fmax": 2750
        },
        "melspectrogram": {
            "nfft": 4096,
            "hop_len": 300,
            "mels": 13
        },
        "periodogram": {
            "slice_frequency_start": 0,
            "slice_frequency_stop": 2048,
            "scale_db": false,
            "scale": true
        },
        "mfcc": {
            "nfft": 4096,
            "hop_len": 1395,
            "mels": 64
        },
        "sound_indicies": {
            "type": "aci",
            "config": {
                "nfft": 4096,
                "hop_len": 1395,
                "scale_db": true,
                "j_samples": 10
            }
        }
    },
    "model_architecture": {
        "ae": {
            "encoder_layer_sizes": [1024, 256, 32],
            "latent_size": 16,
            "decoder_layer_sizes": [32, 256, 1024]
        },
        "vae": {
            "encoder_layer_sizes": [1024, 256, 32],
            "latent_size": 16,
            "decoder_layer_sizes": [32, 256, 1024]
        },
        "cvae": {
            "encoder_layer_sizes": [1024, 256, 32],
            "latent_size": 16,
            "decoder_layer_sizes": [32, 256, 1024]
        },
        "conv_ae": {
            "encoder_no_feature_maps": [128, 64, 32, 16],
            "encoder_mlp_layer_sizes": [1024, 512, 128],
            "decoder_no_feature_maps": [16, 32, 64, 128],
            "decoder_mlp_layer_sizes": [128, 512, 1024],
            "latent_size": 16
        },
        "conv_vae": {
            "encoder_no_feature_maps": [128, 64, 32, 16],
            "encoder_mlp_layer_sizes": [1024, 512, 128],
            "decoder_no_feature_maps": [16, 32, 64, 128],
            "decoder_mlp_layer_sizes": [128, 512, 1024],
            "latent_size": 16
        },
        "conv_cvae": {
            "encoder_no_feature_maps": [128, 64, 32, 16],
            "encoder_mlp_layer_sizes": [1024, 512, 128],
            "decoder_no_feature_maps": [16, 32, 64, 128],
            "decoder_mlp_layer_sizes": [128, 512, 1024],
            "latent_size": 16
        },
        "discriminator": {
            "layers": [16, 4]
        }
    },
    "learning": {
        "batch_size": 32,
        "learning_rate": 1e-3,
        "weight_decay": 1e-2,
        "epochs": 100,
        "patience": 20,
        "batch_normalize": false,
        "batch_standarize": false,
        "discriminator": {
            "alpha": 0.1,
            "learning_rate": 1e-3,
            "weight_decay": 1e-2
        }
    }
}