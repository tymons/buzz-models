{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "hives_ids = [\"smrpiclient7\", \"smrpiclient6\", \"smrpiclient3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "def plot_spectrogram(frequency, time_x, spectrocgram, title):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.title(title)\n",
    "    plt.pcolormesh(time_x, frequency, spectrocgram)\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 1 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        # output layer (with sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class autoencoder_basic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder_basic, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1499, 512),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 1499),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load train data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound data preparation for hive: smrpiclient7 which has 3367 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3367/3367 [00:41<00:00, 80.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "Sound data preparation for hive: smrpiclient6 which has 3172 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3172/3172 [00:40<00:00, 77.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "Sound data preparation for hive: smrpiclient3 which has 602 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 602/602 [00:08<00:00, 73.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "Got 7141 sound samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "sound_time_ms = 2000\n",
    "# ~93 ms for fft window\n",
    "nfft = 4096\n",
    "# ~34% overlapping\n",
    "hop_len = (nfft//3) + 30\n",
    "# This can be manipulated to adjust number of bins for conv layer\n",
    "fmax = 2750\n",
    "\n",
    "hives_data = []\n",
    "max_to_norm = 0\n",
    "for idx, hive_id in enumerate(hives_ids):\n",
    "    sound_files = [f for f in glob.glob(f\"..\\\\measurements\\\\smartulav2\\\\{hive_id}_*\\\\*.wav\")]\n",
    "    print(f\"Sound data preparation for hive: {hive_id} which has {len(sound_files)} recordings...\", end=' ', flush=True)\n",
    "    for file in tqdm(sound_files):\n",
    "        sample_rate, sound_samples = wavfile.read(file)\n",
    "        sound_samples = sound_samples.T[0]/(2.0**31)\n",
    "    \n",
    "        spectrogram = librosa.core.stft(sound_samples, n_fft=nfft, hop_length=hop_len)\n",
    "        spectrogram_magnitude = np.abs(spectrogram)\n",
    "        spectrogram_phase = np.angle(spectrogram)\n",
    "        spectrogram_db = librosa.amplitude_to_db(spectrogram_magnitude, ref=np.max)\n",
    "        frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "        times = (np.arange(0, spectrogram_magnitude.shape[1])*hop_len)/sample_rate\n",
    "        \n",
    "        freq_slice = np.where((frequencies < fmax))\n",
    "        frequencies = frequencies[freq_slice]\n",
    "        spectrogram_db = spectrogram_db[freq_slice, :][0]    \n",
    "    \n",
    "        filename = file.rsplit('\\\\', 1)[-1]\n",
    "        timestamp = filename[filename.index('-')+1:].rsplit(\".wav\")[0]\n",
    "        datetime = datetime.strptime(timestamp, '%Y-%m-%dT%H-%M-%S')\n",
    "        hives_data.append([datetime, hive_id, sound_samples, [frequencies, times, spectrogram_db]])\n",
    "    print(\" done.\")\n",
    "\n",
    "print(f\"Got {len(hives_data)} sound samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for autoencoder train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random_idx = random.randint(0, len(hives_data) - 1)\n",
    "\n",
    "plot_spectrogram(hives_data[random_idx][3][0],\n",
    "                 hives_data[random_idx][3][1],\n",
    "                 hives_data[random_idx][3][2],\n",
    "                 f\"hive: {hives_data[random_idx][1]}, time: {hives_data[random_idx][0]}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal as sig\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "                      \n",
    "pd_data = pd.DataFrame(\n",
    "    {\n",
    "        'samples': [hive_data[2] for hive_data in hives_data],\n",
    "        'periodogram' : [hive_data[3] for hive_data in hives_data],\n",
    "        'sn': [hive_data[1] for hive_data in hives_data]\n",
    "    }\n",
    ")\n",
    "\n",
    "pd_data['periodogram'] = list(np.transpose(MinMaxScaler().fit_transform(np.transpose(pd_data['periodogram'].values.tolist()))))\n",
    "# pd_data['mfcc'] = list([np.mean(librosa.feature.mfcc(sample, sr=3000, n_fft=150, hop_length=75, n_mfcc=14), axis=1)\n",
    "#                            for sample in tpd_data['samples'].to_numpy()])\n",
    "# pd_ae_data = pd_data[(pd_data['sn'] == 'smrpiclient7')]\n",
    "pd_ae_data = pd_data\n",
    "# pd_ae_data = pd_data[(pd_data['sn'] == 1300001)\n",
    "#                         | (pd_data['sn'] == 1400001)\n",
    "#                         | (pd_data['sn'] == 1400002)]\n",
    "pd_ae_data = pd_ae_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train basic AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal as sig\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data as tdata\n",
    "\n",
    "# Divide data to test, validation and train\n",
    "train_stop_idx = int(pd_ae_data.shape[0]*90/100)\n",
    "\n",
    "pd_ae_data_train = pd_ae_data[:train_stop_idx]\n",
    "pd_ae_data_test = pd_ae_data[train_stop_idx:]\n",
    "\n",
    "print(f'Train data size: {pd_ae_data_train.shape[0]}')\n",
    "print(f'Test data size: {pd_ae_data_test.shape[0]}')\n",
    "\n",
    "tensor_train = torch.Tensor(pd_ae_data_train['periodogram'].values.tolist())\n",
    "tensor_test = torch.Tensor(pd_ae_data_test['periodogram'].values.tolist())\n",
    "\n",
    "train_dataset = tdata.TensorDataset(tensor_train)\n",
    "test_dataset = tdata.TensorDataset(tensor_test)\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = autoencoder_basic().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    for data in dataloader:\n",
    "        periodogram = data[0].to(device)\n",
    "        # ===================forward=====================\n",
    "        output = model(periodogram)\n",
    "        #train_loss = criterion(output, periodogram)\n",
    "        train_loss = F.binary_cross_entropy(output, periodogram)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += train_loss.item()\n",
    "    loss = loss / len(dataloader)\n",
    "    if (epoch+1) % (num_epochs/10) == 0:\n",
    "        print(f'epoch {epoch + 1}/{num_epochs}, loss:{loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CONV AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got dataset of size: 7141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# sc = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "spectrogram_ae_data = [hive_data[3][2] for hive_data in hives_data]\n",
    "# standarized_ae_data = [sc.fit_transform(spec.T).T for spec in conv_ae_data]\n",
    "scaled_ae_data = [mm.fit_transform(stan.T).T for stan in spectrogram_ae_data]\n",
    "\n",
    "print(f\"Got dataset of size: {len(scaled_ae_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: torch.Size([7141, 256, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.utils import data as tdata\n",
    "\n",
    "data_ratio = int(len(scaled_ae_data)*90/100)\n",
    "\n",
    "dataset_tensor = torch.Tensor(scaled_ae_data)\n",
    "print(f\"Dataset shape: {dataset_tensor.shape}\")\n",
    "# add one extra dimension as it is required for conv layer\n",
    "dataset_tensor = dataset_tensor[:, None, :, :] \n",
    "dataset = tdata.TensorDataset(dataset_tensor)\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [data_ratio, len(scaled_ae_data)-data_ratio])\n",
    "\n",
    "dataloader_train = tdata.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "dataloader_test = tdata.DataLoader(test_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvAutoencoder(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (t_conv1): ConvTranspose2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (t_conv2): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "modelConvAE = ConvAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(modelConvAE.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "print(modelConvAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.098176\n",
      "Epoch: 2 \tTraining Loss: 1.014356\n",
      "Epoch: 3 \tTraining Loss: 0.937788\n",
      "Epoch: 4 \tTraining Loss: 0.914127\n",
      "Epoch: 5 \tTraining Loss: 0.908649\n",
      "Epoch: 6 \tTraining Loss: 0.872004\n",
      "Epoch: 7 \tTraining Loss: 0.846211\n",
      "Epoch: 8 \tTraining Loss: 0.840525\n",
      "Epoch: 9 \tTraining Loss: 0.836914\n",
      "Epoch: 10 \tTraining Loss: 0.833600\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in dataloader_train:\n",
    "        # _ stands in for labels, here\n",
    "        # no need to flatten images\n",
    "        periodogram = data[0].to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = modelConvAE(periodogram)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, periodogram)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*periodogram.size(0)\n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(dataloader_train)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root='../MNIST-data', train=True, download=True, transform=transform)\n",
    "dataloader_train = torch.utils.data.DataLoader(train_data, batch_size=20, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, data in enumerate(train_loader):\n",
    "    print((data[0]).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    loss_test = 0\n",
    "    for data in dataloader_test:\n",
    "        periodograms_test = data[0].to(device)\n",
    "        output = model(periodograms_test)\n",
    "        for idx, i in enumerate(output):\n",
    "            #loss_test += nn.MSELoss()(periodograms_test[idx], i)\n",
    "            loss_test += F.binary_cross_entropy(periodograms_test[idx], i)\n",
    "\n",
    "loss_test = loss_test/len(pd_ae_data_test)\n",
    "print(f'Final test loss: {loss_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and visualize (MFCC vs AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    encode_data = pd_ae_data['periodogram'].values.tolist()\n",
    "    encode_data_tensor = torch.Tensor(encode_data).to(device)\n",
    "    output = [model.encoder(encode_data_tensor).cpu().numpy()][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx =708\n",
    "with torch.no_grad():\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(encode_data[idx], 'r')\n",
    "    plt.plot(model.decoder(torch.Tensor(output[idx]).to(device)).cpu().numpy(), 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension reduction - now we perform t-SNE and PCA to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "reduced_ae_tsne = TSNE(n_components=2, perplexity=100, learning_rate=500, verbose=1).fit_transform(output)\n",
    "reduced_mfcc_tsne = TSNE(n_components=2, perplexity=100, learning_rate=500, verbose=1).fit_transform(pd_ae_data['mfcc'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mfccs_standarized = StandardScaler().fit_transform(pd_ae_data['mfcc'].values.tolist())\n",
    "ae_standarized = StandardScaler().fit_transform(output)\n",
    "reduced_ae_pca = PCA(n_components = 2).fit_transform(ae_standarized)\n",
    "reduced_mfcc_pca = PCA(n_components = 2).fit_transform(mfccs_standarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA().fit(ae_standarized)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(.005), \\\n",
    "                        max_features=1.0, bootstrap=False, n_jobs=-1, random_state=43, verbose=0, behaviour=\"new\")\n",
    "\n",
    "clf.fit(reduced_ae_pca)\n",
    "clf.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_forest = clf.predict(reduced_ae_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,8))\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "idx_1300001 = pd_data[pd_data['sn'] == 1300001].index.values.tolist()\n",
    "idx_1300002 = pd_data[pd_data['sn'] == 1300002].index.values.tolist()\n",
    "idx_1400001 = pd_data[pd_data['sn'] == 1400001].index.values.tolist()\n",
    "idx_1400002 = pd_data[pd_data['sn'] == 1400002].index.values.tolist()\n",
    "\n",
    "l1_ae_tsne = axs[0][0].scatter([data[0] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1300001],\n",
    "                 [data[1] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1300001], c='r', alpha = 0.3)\n",
    "# l2_ae_tsne = axs[0][0].scatter([data[0] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1300002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1300002], c='b', alpha = 0.3)\n",
    "# l3_ae_tsne = axs[0][0].scatter([data[0] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1400001],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1400001], c='g', alpha = 0.3)\n",
    "# l4_ae_tsne = axs[0][0].scatter([data[0] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1400002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1400002], c='y', alpha = 0.3)\n",
    "\n",
    "l1_mfcc_tsne = axs[0][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1300001],\n",
    "                 [data[1] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1300001], c='r', alpha = 0.3)\n",
    "# l2_mfcc_tsne = axs[0][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1300002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1300002], c='b', alpha = 0.3)\n",
    "# l3_mfcc_tsne = axs[0][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1400001],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1400001], c='g', alpha = 0.3)\n",
    "# l4_mfcc_tsne = axs[0][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1400002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1400002], c='y', alpha = 0.3)\n",
    "\n",
    "# axs[0][0].legend((l1_ae_tsne, l2_ae_tsne, l3_ae_tsne, l4_ae_tsne), ('Sikorki', 'Not Hive', 'Sulmin 1', 'Sulmin 2'),\n",
    "#                  loc='upper right')\n",
    "axs[0][0].set_xlabel('DIM1')\n",
    "axs[0][0].set_ylabel('DIM2')\n",
    "axs[0][0].set_title('Hives - AE sounds with TSNE')\n",
    "\n",
    "# axs[0][1].legend((l1_mfcc_tsne, l2_mfcc_tsne, l3_mfcc_tsne, l4_mfcc_tsne), ('Sikorki', 'Not Hive', 'Sulmin 1', 'Sulmin 2'),\n",
    "#                  loc='upper right')\n",
    "axs[0][1].set_xlabel('DIM1')\n",
    "axs[0][1].set_ylabel('DIM2')\n",
    "axs[0][1].set_title('Hives - MFCC sounds with TSNE')\n",
    "\n",
    "l1_ae_pca = axs[1][0].scatter([data[0] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1300001],\n",
    "                 [data[1] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1300001], c='r', alpha = 0.3)\n",
    "markers = axs[1][0].scatter([data[0] for idx, data in enumerate(reduced_ae_pca) if output_forest[idx] == -1],\n",
    "                           [data[1] for idx, data in enumerate(reduced_ae_pca) if output_forest[idx] == -1], c='b', alpha = 0.9)\n",
    "# l2_ae_pca = axs[1][0].scatter([data[0] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1300002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1300002], c='b', alpha = 0.3)\n",
    "# l3_ae_pca = axs[1][0].scatter([data[0] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1400001],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1400001], c='g', alpha = 0.3)\n",
    "# l4_ae_pca = axs[1][0].scatter([data[0] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1400002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1400002], c='y', alpha = 0.3)\n",
    "\n",
    "l1_mfcc_pca = axs[1][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1300001],\n",
    "                 [data[1] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1300001], c='r', alpha = 0.3)\n",
    "# l2_mfcc_pca = axs[1][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1300002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1300002], c='b', alpha = 0.3)\n",
    "# l3_mfcc_pca = axs[1][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1400001],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1400001], c='g', alpha = 0.3)\n",
    "# l4_mfcc_pca = axs[1][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1400002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1400002], c='y', alpha = 0.3)\n",
    "\n",
    "# axs[1][0].legend((l1_ae_pca, l2_ae_pca, l3_ae_pca, l4_ae_pca), ('Sikorki', 'Not Hive', 'Sulmin 1', 'Sulmin 2'),\n",
    "#                  loc='upper right')\n",
    "axs[1][0].set_xlabel('PC1')\n",
    "axs[1][0].set_ylabel('PC2')\n",
    "axs[1][0].set_title('Hives - AE sounds with PCA')\n",
    "\n",
    "# axs[1][1].legend((l1_mfcc_pca, l2_mfcc_pca, l3_mfcc_pca, l4_mfcc_pca), ('Sikorki', 'Not Hive', 'Sulmin 1', 'Sulmin 2'),\n",
    "#                   loc='upper right')\n",
    "axs[1][1].set_xlabel('PC1')\n",
    "axs[1][1].set_ylabel('PC2')\n",
    "axs[1][1].set_title('Hives - MFCC sounds with PCA')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('output-ae/basic-2warstwy-128-32.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "\n",
    "RECORD_TIME = 2\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "data = hives_data[-10][2][:, 0]/(2.0**31)\n",
    "datetime = hives_data[-1][0]\n",
    "fft_data = abs(fft(data))\n",
    "freqs = fftfreq(int(len(fft_data)/2), 1/SAMPLE_RATE)\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.tight_layout(pad=3.0)\n",
    "axs[0].set_title(f\"Sound recording at {datetime} ({RECORD_TIME}s)\")\n",
    "axs[0].grid()\n",
    "axs[0].set_xlabel('Time [sec]')\n",
    "axs[0].plot(np.linspace(0, 2, len(data)), data)\n",
    "\n",
    "axs[1].set_title(\"Periodogram\")\n",
    "axs[1].set_xticks(np.arange(0, (freqs.size/2), step=100))\n",
    "axs[1].set_xticklabels(np.arange(0, (freqs.size/2), step=100, dtype=int), rotation=45)\n",
    "axs[1].grid()\n",
    "axs[1].set_xlabel('Frequency [Hz]')\n",
    "axs[1].plot(freqs[1:1500], fft_data[1:1500], 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
