{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'data.indice.compute_indice' from 'c:\\\\Users\\\\tymot\\\\projects\\\\001.smartula\\\\smartula-analysis\\\\smartula-analysis\\\\notebooks\\\\data\\\\indice\\\\compute_indice.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 308
    }
   ],
   "source": [
    "\n",
    "import utils.periodogram_dataset_keras as pdk\n",
    "import utils.contrastive_keras as ck\n",
    "import utils.data_utils as du\n",
    "import importlib\n",
    "import data.spectrogram_dataset as sd\n",
    "import data.melspectrogram_dataset as md\n",
    "import data.periodogram_dataset as pd\n",
    "import data.mfcc_dataset as mfcd\n",
    "import data.sound as sound\n",
    "import data.indice.sound_indicies_dataset as si\n",
    "import data.indice.compute_indice as compute_indice\n",
    "import data.double_feature_dataset as dfd\n",
    "import utils.pytorch_impl.cvae as ccc\n",
    "import utils.pytorch_impl.discriminator as discriminator\n",
    "\n",
    "importlib.reload(discriminator)\n",
    "importlib.reload(ccc)\n",
    "importlib.reload(pd)\n",
    "importlib.reload(md)\n",
    "importlib.reload(sound)\n",
    "importlib.reload(mfcd)\n",
    "importlib.reload(du)\n",
    "importlib.reload(si)\n",
    "importlib.reload(sd)\n",
    "importlib.reload(dfd)\n",
    "importlib.reload(compute_indice)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "got 39678 sound filenames read for 5 files/folders\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from utils.data_utils import create_valid_sounds_datalist, get_valid_sounds_datalist\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CHECK_FILES = False                             # should we perform sound checking (long)\n",
    "\n",
    "root_folder = \"..\\\\measurements\\\\smartulav2\"    # root folder with folder containing sounds\n",
    "valid_filename = \"valid-files.txt\"              # filename for file which will be contain\n",
    "                                                # filenames for valid recordings\n",
    "checked_folders = []\n",
    "if CHECK_FILES:\n",
    "    checked_folders = create_valid_sounds_datalist(root_folder, valid_filename, \"smrpiclient\")\n",
    "else:\n",
    "     checked_folders = [os.path.join(root_folder, \"smrpiclient0_10082020-19012021\"),       \n",
    "                        os.path.join(root_folder, \"smrpiclient3_10082020-19012021\"), \n",
    "                        os.path.join(root_folder, \"smrpiclient5_10082020-19012021\"),    \n",
    "                        os.path.join(root_folder, \"smrpiclient6_10082020-19012021\"),\n",
    "                        os.path.join(root_folder, \"smrpiclient7_10082020-19012021\")]\n",
    "\n",
    "sound_filenames = get_valid_sounds_datalist(checked_folders, valid_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoder\n",
    "\n",
    "1. Convolutional version - dedicated to mel spectrograms and spectrograms\n",
    "2. Simple Autoencoder - fetures extracted from stationary signals and periodograms itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.periodogram_dataset import PeriodogramDataset\n",
    "from data.spectrogram_dataset import SpectrogramDataset\n",
    "from data.melspectrogram_dataset import MelSpectrogramDataset\n",
    "from data.mfcc_dataset import MfccDataset\n",
    "\n",
    "from utils.contrastive_pytorch import VAE, train_vae_model\n",
    "from utils.data_utils import filter_strlist\n",
    "from utils.contrastive_pytorch import ConvolutionalVAE\n",
    "from utils.pytorch_impl.cvae import cVAE\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "PATH = 'models\\\\vae-apprirary-approach-10022021.pth'\n",
    "TRAIN = True\n",
    "BATCH_SCALE = True\n",
    "\n",
    "# model for \n",
    "modelVAE = VAE(encoder_layer_sizes=[2048, 512, 64], latent_size=16, decoder_layer_sizes=[64, 512, 2048]).to(device)\n",
    "# model for mfcc \n",
    "mfccVAE = VAE(encoder_layer_sizes=[64, 32], latent_size=4, decoder_layer_sizes=[32, 64]).to(device)\n",
    "# model for spectrogram\n",
    "convVAE = ConvolutionalVAE(encoder_conv_sizes=[128, 64, 32, 16], encoder_mlp_sizes=[1024, 512, 128],\n",
    "                            decoder_conv_sizes=[16, 32, 64, 128], decoder_mlp_sizes=[128, 512, 1024], latent=16).to(device)\n",
    "# model for melspectrogram\n",
    "melsConvVAE = ConvolutionalVAE(encoder_conv_sizes=[128, 64, 32, 16], encoder_mlp_sizes=[256, 128],\n",
    "                            decoder_conv_sizes=[16, 32, 64, 128], decoder_mlp_sizes=[128, 256], latent=16).to(device)\n",
    "\n",
    "if TRAIN:\n",
    "    # training hyperparameters\n",
    "    batch_size = 16\n",
    "    epochs = 10\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    # prepare train data for complete dataset\n",
    "    train_hives_soundfiles = filter_strlist(sound_filenames, \"smrpiclient6\", \"smrpiclient7\")\n",
    "    print(f\"preparing train dataset with len: {len(train_hives_soundfiles)}...\")\n",
    "\n",
    "    # --- HERE CHOOSE CORRECT DATASET AS FEATURE --- #\n",
    "    # dataset = PeriodogramDataset(train_hives_soundfiles, [\"smrpiclient6\", \"smrpiclient7\"], slice_freq=(0, 2048))    \n",
    "    # dataset = SpectrogramDataset(train_hives_soundfiles, [\"smrpiclient6\", \"smrpiclient7\"], nfft=4096, hop_len=(4096//3)+30, fmax=2750)\n",
    "    # dataset = MelSpectrogramDataset(train_hives_soundfiles, [\"smrpiclient6\", \"smrpiclient7\"], nfft=4096, hop_len=(4096//3)+30, mels=64)\n",
    "    # dataset = MfccDataset(train_hives_soundfiles, [\"smrpiclient6\", \"smrpiclient7\"], nfft=4096, hop_len=(4096//3)+30, mels=64)\n",
    "\n",
    "    # --- HERE COOSE CORRECT MODEL --- #\n",
    "    model = mfccVAE\n",
    "\n",
    "    val_amount = int(dataset.__len__() * 0.15)\n",
    "    train_set, val_set = random_split(dataset, [(dataset.__len__() - val_amount), val_amount])\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # train model\n",
    "    modelVAE = train_vae_model(model, learning_rate=lr, weight_decay=weight_decay, num_epochs=epochs, patience=100, \\\n",
    "                                  dataloader_train=train_loader, dataloader_val=val_loader, scale=BATCH_SCALE, checkpoint_name=PATH)\n",
    "else:\n",
    "    if os.path.isfile(PATH) and os.access(PATH, os.R_OK):\n",
    "        modelVAE.load_state_dict(torch.load(PATH))\n",
    "        print('model load success!')\n",
    "    else:\n",
    "        print(f'ERROR! There is no file: {PATH}')"
   ]
  },
  {
   "source": [
    "### - inference -\n",
    "here we perform inference to see how our vae models handle data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.spectrogram_dataset import SpectrogramDataset\n",
    "from data.melspectrogram_dataset import MelSpectrogramDataset\n",
    "\n",
    "inference_hives = filter_strlist(sound_filenames, \"smrpiclient6\")\n",
    "idx = random.randrange(len(inference_hives))\n",
    "\n",
    "spec_dataset = SpectrogramDataset(inference_hives, [\"smrpiclient6\"], nfft=4096, hop_len=(4096//3)+30, fmax=2750)\n",
    "melspec_dataset = MelSpectrogramDataset(inference_hives, [\"smrpiclient6\"], nfft=4096, hop_len=(4096//3)+30, mels=64)\n",
    "\n",
    "(spectrogram_db, freqs, times), _ = spec_dataset.sample(idx)\n",
    "melspectrogram_db, _ = melspec_dataset.sample(idx)\n",
    "\n",
    "with torch.no_grad():\n",
    "    modelVAE.eval()\n",
    "    \n",
    "    fig, axs = plt.subplots(2,2, figsize=(16, 8))\n",
    "\n",
    "    axs[0][0].set_title('orginal spectrogram')\n",
    "    axs[0][0].pcolormesh(times, freqs, spectrogram_db.squeeze())\n",
    "    axs[0][1].set_title('reconstructed spectrogram (convVAE)')\n",
    "    axs[0][1].pcolormesh(times, freqs, modelVAE(spectrogram_db)[0].cpu().squeeze())\n",
    "    \n",
    "    axs[1][0].set_title('original mel-spectrogram')\n",
    "    axs[1][0].imshow(melspectrogram_db.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Variational Autoencoder\n",
    "\n",
    "-- keras implementation --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random \n",
    "\n",
    "from utils.periodogram_dataset_keras import PeriodogramGenerator\n",
    "from utils.contrastive_keras import contrastive_keras_vae\n",
    "from utils.data_utils import filter_strlist\n",
    "from tensorflow import keras\n",
    "\n",
    "bsize = 64\n",
    "TRAIN = True\n",
    "MIXED = True\n",
    "\n",
    "# background_hives = [\"smrpiclient6\", \"smrpiclient5\"]\n",
    "# target_hives = [\"smrpiclient7\"]\n",
    "\n",
    "cvae_keras, _, z_encoder, s_encoder, sb_encoder, cvae_decoder = contrastive_keras_vae(input_dim=2048, intermediate_dim=[512, 64], latent_dim=16,\n",
    "                                                                                        disentangle=True, gamma=1, batch_size=bsize)\n",
    "\n",
    "if TRAIN:        \n",
    "    # prepare train data for complete dataset\n",
    "    target_soundfilenames = filter_strlist(sound_filenames, \"smrpiclient5\")\n",
    "    background_soundfilenames = random.sample(filter_strlist(sound_filenames, \"smrpiclient6\", \"smrpiclient7\"), len(target_soundfilenames))\n",
    "    print(f\"preparing target dataset with len: {len(target_soundfilenames)} and background with len: {len(background_soundfilenames)}\")\n",
    "            \n",
    "    # Prepare dataset and train\n",
    "    train_generator = PeriodogramGenerator(target_soundfilenames, background_soundfilenames, batch_size=bsize, labels=[\"smrpiclient5\"], slice_freq=(0, 2048))\n",
    "    history = cvae_keras.fit(train_generator, epochs=10, batch_size=bsize, verbose=1)\n",
    "\n",
    "    cvae_keras.save_weights('models/cvae_keras_weights_10022021.h5')\n",
    "    sb_encoder.save_weights('models/sb_encoder_weights_10022021.h5')\n",
    "    cvae_decoder.save_weights('models/cvae_decoder_weights_10022021.h5')\n",
    "else:\n",
    "    cvae_keras.load_weights('models/cvae_keras_weights_10022021.h5')\n",
    "    sb_encoder.load_weights('models/sb_encoder_weights_10022021.h5')\n",
    "    cvae_decoder.load_weights('models/cvae_decoder_weights_10022021.h5')\n",
    "    print(f'model loaded successfully')\n"
   ]
  },
  {
   "source": [
    "-- pytorch implementation --"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(86, 1024)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy import signal\n",
    "from data.sound import read_samples\n",
    "\n",
    "W = signal.get_window('hanning', 1024, fftbins=False)\n",
    "samples, sampling_rate = read_samples(target_soundfilenames[0], raw=True)\n",
    "\n",
    "time_shift = int(1024/2)\n",
    "times = range(time_shift, len(samples)+1-time_shift, 1024) # centered\n",
    "frames = [samples[i-time_shift:i+time_shift]*W for i in times] # centered frames\n",
    "\n",
    "windowed_signal = np.array([frame*W for frame in frames])\n",
    "print(windowed_signal.shape)\n"
   ]
  },
  {
   "source": [
    "-- PyTorch implementation --"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.5421, 0.9584, 0.5274],\n        [0.3611, 0.8751, 0.0157]])\ntensor([[1., 1., 1.],\n        [1., 1., 1.]])\ntensor(0.3046)\ntensor(0.3046)\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((2,3))\n",
    "b = torch.ones((2,3))\n",
    "\n",
    "print(a)\n",
    "print(b)\n",
    "loss = torch.nn.MSELoss(reduction='sum')\n",
    "loss2 = torch.nn.MSELoss(reduction='mean')\n",
    "print(loss(a,b)/6)\n",
    "print(loss2(a,b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_var = torch.rand((32, 1, 16))\n",
    "mu = torch.rand((32, 1, 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([-0.5751])"
      ]
     },
     "metadata": {},
     "execution_count": 212
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preparing target dataset with len: 10947 and background with len: 10947\n",
      "-> training at epoch 0\n",
      "100%|██████████| 291/291 [00:40<00:00,  7.14it/s]\n",
      "100%|██████████| 291/291 [00:40<00:00,  7.14it/s]-> validating at epoch 0\n",
      "\n",
      "100%|██████████| 52/52 [00:06<00:00,  7.72it/s]\n",
      "100%|██████████| 52/52 [00:06<00:00,  7.72it/s]Epoch [1/10], LOSS: 0.2439, VAL_LOSS: 0.3183 DISC_LOSS: 1.3867\n",
      "-> training at epoch 1\n",
      "\n",
      "100%|██████████| 291/291 [00:37<00:00,  7.84it/s]\n",
      "100%|██████████| 291/291 [00:37<00:00,  7.84it/s]-> validating at epoch 1\n",
      "\n",
      "100%|██████████| 52/52 [00:06<00:00,  7.85it/s]\n",
      "100%|██████████| 52/52 [00:06<00:00,  7.85it/s]\n",
      "Epoch [2/10], LOSS: 0.2279, VAL_LOSS: 0.2730 DISC_LOSS: 1.3862\n",
      "-> training at epoch 2\n",
      " 40%|████      | 117/291 [00:13<00:20,  8.37it/s]\n",
      " 40%|████      | 117/291 [00:13<00:20,  8.37it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-309-92e1ac200f2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mcvae_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1e-7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'epochs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mdiscriminator_parameters\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'alpha'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mcvae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_cvae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelcVae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcvae_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrastive_train_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrastive_val_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\tymot\\projects\\001.smartula\\smartula-analysis\\smartula-analysis\\notebooks\\utils\\pytorch_impl\\cvae.py\u001b[0m in \u001b[0;36mtrain_cvae\u001b[1;34m(model, model_params, dataloader_train, dataloader_val, disc, disc_params)\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m                 \u001b[1;31m# cvae\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1170\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1171\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1172\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\utils\\data\\dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 272\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tymot\\projects\\001.smartula\\smartula-analysis\\smartula-analysis\\notebooks\\data\\double_feature_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mtarget_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mbackground_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackground\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground_sample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tymot\\projects\\001.smartula\\smartula-analysis\\smartula-analysis\\notebooks\\data\\periodogram_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;34m\"\"\" Method for pytorch dataloader \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mperiodogram\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mperiodogram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tymot\\projects\\001.smartula\\smartula-analysis\\smartula-analysis\\notebooks\\data\\periodogram_dataset.py\u001b[0m in \u001b[0;36mget_item\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0msound_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSound\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mperiodogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msound_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampling_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_db\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m             \u001b[0mperiodogram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperiodogram\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msound_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mrfft\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\numpy\\fft\\_pocketfft.py\u001b[0m in \u001b[0;36mrfft\u001b[1;34m(a, n, axis, norm)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m     \u001b[0minv_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_forward_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_raw_fft\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minv_norm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\numpy\\fft\\_pocketfft.py\u001b[0m in \u001b[0;36m_raw_fft\u001b[1;34m(a, n, axis, is_real, is_forward, inv_norm)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpfi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_real\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_forward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from utils.pytorch_impl.cvae import cVAE, train_cvae\n",
    "from utils.pytorch_impl.discriminator import Discriminator\n",
    "from utils.data_utils import filter_strlist\n",
    "\n",
    "from data.periodogram_dataset import PeriodogramDataset\n",
    "from data.double_feature_dataset import DoubleFeatureDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.data_utils import permutate2d\n",
    "\n",
    "target_hives = [\"smrpiclient5\"]\n",
    "background_hives = [\"smrpiclient6\", \"smrpiclient7\"]\n",
    "\n",
    "target_soundfilenames = filter_strlist(sound_filenames, *target_hives)\n",
    "background_soundfilenames = random.sample(filter_strlist(sound_filenames, *background_hives), len(target_soundfilenames))\n",
    "print(f\"preparing target dataset with len: {len(target_soundfilenames)} and background with len: {len(background_soundfilenames)}\")\n",
    "\n",
    "contrastive_periodogram_dataset = DoubleFeatureDataset(target_soundfilenames, target_hives, background_soundfilenames, background_hives,\n",
    "                                                        PeriodogramDataset, scale_db=False, slice_freq=(0, 2048))\n",
    "\n",
    "val_amount = int(contrastive_periodogram_dataset.__len__() * 0.15)\n",
    "train_set, val_set = random_split(contrastive_periodogram_dataset, [(contrastive_periodogram_dataset.__len__() - val_amount), val_amount])\n",
    "contrastive_train_dataloader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "contrastive_val_dataloader = DataLoader(val_set, batch_size=32, shuffle=True)\n",
    "                                \n",
    "modelcVae = cVAE(encoder_layer_sizes=[2048, 512, 64], latent_size=16, decoder_layer_sizes=[64, 512, 2048])\n",
    "discriminator = Discriminator([64, 128, 16], input_size=2*modelcVae.latent_size)\n",
    "\n",
    "cvae_parameters = { 'learning_rate': 1e-7, 'weight_decay': 1e-3, 'epochs': 10 }\n",
    "discriminator_parameters  = { 'learning_rate': 0.001, 'weight_decay': 1e-6, 'alpha': 0.01}\n",
    "cvae = train_cvae(modelcVae, cvae_parameters, contrastive_train_dataloader, contrastive_val_dataloader, discriminator, discriminator_parameters)"
   ]
  },
  {
   "source": [
    "-> Here we test generalization ability by testing MSE loss for unseen hive. We start with VAE and then contrastive Autoencoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.periodogram_dataset import PeriodogramDataset\n",
    "from utils.data_utils import filter_strlist\n",
    "\n",
    "hut_filenames = filter_strlist(sound_filenames, \"smrpiclient3\")\n",
    "hut_dataset = PeriodogramDataset(hut_filenames, [\"smrpiclient3\"], slice_freq=(0, 2048))\n",
    "\n",
    "test_loss = []\n",
    "with torch.no_grad():\n",
    "    modelVAE.eval()\n",
    "    for input_data in tqdm(hut_dataset):\n",
    "        if input_data:\n",
    "            input_data = torch.Tensor(input_data[0]).to(device)\n",
    "            output, mean, var = modelVAE(input_data.to(device))\n",
    "            test_loss.append(F.mse_loss(output, input_data).cpu().numpy())\n",
    "\n",
    "test_loss_avg = np.mean(test_loss)\n",
    "\n",
    "print(f'\\r\\nVAE test loss: {test_loss_avg:.8f}')\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from operator import itemgetter \n",
    "from utils.periodogram_dataset_keras import PeriodogramGenerator, read_sound_file\n",
    "from tqdm import tqdm \n",
    "\n",
    "hut_sounds =  np.array([read_sound_file(f, (0, 2048)) for f in tqdm(hut_filenames)])\n",
    "mean, var, latent = sb_encoder.predict(hut_sounds)\n",
    "zeros = np.zeros_like(latent)\n",
    "decoder_input = np.concatenate((zeros, latent), axis=1)\n",
    "cvae_output = cvae_decoder.predict(decoder_input)\n",
    "loss = tf.keras.losses.mean_squared_error(hut_sounds, cvae_output)\n",
    "\n",
    "print(f'cVAE test loss:: {np.mean(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices = np.random.randint(0, len(background_soundfilenames), len(hut_sounds))\n",
    "\n",
    "background_train_sound_files = list(itemgetter(*subset_indices)(background_soundfilenames))\n",
    "bg_sounds = np.array([read_sound_file(f, (0, 2048)) for f in tqdm(background_train_sound_files)])\n",
    "mean, var, latent = sb_encoder.predict(bg_sounds)\n",
    "zeros = np.zeros_like(latent)\n",
    "decoder_input = np.concatenate((zeros, latent), axis=1)\n",
    "cvae_output = cvae_decoder.predict(decoder_input)\n",
    "loss = tf.keras.losses.mean_squared_error(bg_sounds, cvae_output)\n",
    "\n",
    "print(f'MSE loss for background test: {np.mean(loss)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD ANALYSIS BELOW\n",
    "##  |\n",
    "##  |\n",
    "## \\/\n",
    "# Data Preprocessing\n",
    "\n",
    "Here we load train data sound samples and prepare spectrogram, periodogram and mfcc features (along with some data to visualize this). We should provide data with **utc timestamps** as it will be shifted with `timezone_offset_hours` var. What we also do is remove those samples which has strange rms signal. Threshold 0.8 was chosen based on `plot_distribution` output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import librosa\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "\n",
    "sound_time_ms = 2000\n",
    "# ~93 ms for fft window\n",
    "nfft = 4096\n",
    "# ~34% overlapping\n",
    "hop_len = (nfft//3) + 30\n",
    "# This can be manipulated to adjust number of bins for conv layer\n",
    "fmax = 2750\n",
    "\n",
    "hives_data = []\n",
    "rmses = {}\n",
    "max_to_norm = 0\n",
    "\n",
    "if DATA_INIT:\n",
    "    for idx, hive_id in enumerate(hives_ids):\n",
    "        sound_files = [f for f in glob.glob(f\"..\\\\measurements\\\\smartulav2\\\\{hive_id}_*\\\\*.wav\")]\n",
    "        print(f\"Sound data preparation for hive: {hive_id} which has {len(sound_files)} recordings...\", end=' ', flush=True)\n",
    "        for file in tqdm(sound_files):\n",
    "            sample_rate, sound_samples = wavfile.read(file)\n",
    "            sound_samples = sound_samples.T[0]/(2.0**31)\n",
    "            rms = np.sqrt(sum(sound_samples**2)/len(sound_samples))\n",
    "            if rms < 0.7:    # that threshold was observed from plot_distribution() function\n",
    "                # calculate timestamp\n",
    "                filename = file.rsplit('\\\\', 1)[-1]\n",
    "                utc_timestamp = filename[filename.index('-')+1:].rsplit(\".wav\")[0]\n",
    "                sound_datetime = datetime.strptime(utc_timestamp, '%Y-%m-%dT%H-%M-%S') + timedelta(hours=timezone_offset_hours)\n",
    "                \n",
    "                # calculate mfcc feature\n",
    "                mfccs = librosa.feature.mfcc(y=sound_samples, sr=sample_rate, n_fft=nfft, hop_length=hop_len, n_mfcc=13)\n",
    "                np_mfcc_avg = np.mean(mfccs, axis=1)\n",
    "                \n",
    "                # calculate spectrogram\n",
    "                spectrogram = librosa.core.stft(sound_samples, n_fft=nfft, hop_length=hop_len)\n",
    "                spectrogram_magnitude = np.abs(spectrogram)\n",
    "                spectrogram_phase = np.angle(spectrogram)\n",
    "                spectrogram_db = librosa.amplitude_to_db(spectrogram_magnitude, ref=np.max)\n",
    "                frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "                times = (np.arange(0, spectrogram_magnitude.shape[1])*hop_len)/sample_rate\n",
    "                freq_slice = np.where((frequencies < fmax))\n",
    "                frequencies = frequencies[freq_slice]\n",
    "                spectrogram_db = spectrogram_db[freq_slice, :][0]\n",
    "                spectrogram_mean = np.mean(spectrogram_db, axis=1)\n",
    "                # decimate?\n",
    "                # spectrogram_db_decimated = decimate(spectrogram_db.T, 4).T\n",
    "                # frequencies_decimated = decimate(frequencies, 4)\n",
    "\n",
    "                #calculate periodogram\n",
    "                periodogram = fft(sound_samples, n=sample_rate)\n",
    "                periodogram = abs(periodogram[1:int(len(periodogram)/2)])\n",
    "                periodogram_freq = fftfreq(len(sound_samples)//(sound_time_ms//1000), 1/sample_rate)\n",
    "                periodogram_freq = periodogram_freq[:(len(periodogram_freq)//2)-1]\n",
    "                \n",
    "                hives_data.append(\n",
    "                    {\n",
    "                        'datetime': sound_datetime,\n",
    "                        'id': hive_id,\n",
    "                        'samples': sound_samples,\n",
    "                        'freq':\n",
    "                            {\n",
    "                                'spectrogram':\n",
    "                                    {\n",
    "                                        'frequencies': frequencies,\n",
    "                                        'time': times,\n",
    "                                        'spectrogram_full_db': spectrogram_db,\n",
    "                                        'spectrogram_mean': spectrogram_mean\n",
    "                                    },\n",
    "                                'periodogram':\n",
    "                                    {\n",
    "                                        'frequencies': periodogram_freq,\n",
    "                                        'samples': periodogram\n",
    "                                    }\n",
    "                            },\n",
    "                        'features':\n",
    "                            {\n",
    "                                'mfcc_avg': np_mfcc_avg\n",
    "                            }\n",
    "                    }\n",
    "                )\n",
    "        print(\" done.\")\n",
    "        \n",
    "    print(\"saving data on disc...\", end=' ')\n",
    "    np.save('data/raw_hives_data.npy', hives_data, allow_pickle=True)\n",
    "    print(\"done.\")\n",
    "else:\n",
    "    hives_data = np.load('data/raw_hives_data.npy', allow_pickle=True)\n",
    "    \n",
    "print(f\"got full dataset of {len(hives_data)} sound samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data preprocessing \n",
    "\n",
    "Here we perform scaling standarization etc.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "sc = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "# extract every spectrogram from hives data list and standarize every periodogram from stft \n",
    "# so data for spectrogram will have zero mean and unit variance + scale every standarized periodogram\n",
    "print('preparing spectrograms...', end=' ', flush=True)\n",
    "spectrograms = [hive_data['freq']['spectrogram']['spectrogram_full_db'] for hive_data in hives_data]\n",
    "# standarized_spectrograms = [sc.fit_transform(spectrogram.T).T for spectrogram in tqdm(spectrograms)]\n",
    "scaled_spectrogram = [mm.fit_transform(spectrogram.T).T for spectrogram in tqdm(spectrograms, position=0, leave=True)]\n",
    "print('done.')\n",
    "\n",
    "# get datatime, names and mfcc\n",
    "print('getting mfccs', end=' ')\n",
    "datetimes = [hive_data['datetime'] for hive_data in hives_data]\n",
    "names = [hive_data['id'] for hive_data in hives_data]\n",
    "mfccs = [hive_data['features']['mfcc_avg'] for hive_data in hives_data]\n",
    "print('done.')\n",
    "\n",
    "# standarize and scale mean spectrogram for sounds\n",
    "print('preparing spectrogram means', end=' ')\n",
    "spectrograms_mean = [hive_data['freq']['spectrogram']['spectrogram_mean'] for hive_data in hives_data]\n",
    "standarized_spectrograms_mean = StandardScaler().fit_transform(spectrograms_mean)\n",
    "scaled_spectrograms_means = MinMaxScaler().fit_transform(standarized_spectrograms_mean)\n",
    "print('done.')\n",
    "\n",
    "# prepare truncated periodogram\n",
    "end_frequency = 2048\n",
    "print(f'preparing truncated periodograms ({end_frequency})', end=' ', flush=True)\n",
    "periodograms = [hive_data['freq']['periodogram']['samples'][:end_frequency] for hive_data in hives_data]\n",
    "periodograms = [mm.fit_transform(perio.reshape(-1, 1)).T for perio in tqdm(periodograms, position=0, leave=True)]\n",
    "print('done.')\n",
    "\n",
    "sounds = list(zip(scaled_spectrogram, mfccs, scaled_spectrograms_means, periodograms, datetimes, names))\n",
    "\n",
    "sounds_data = pd.DataFrame(sounds, columns=['spectrogram', 'mfccs', 'spectrogram_mean', 'periodogram', 'datetime', 'name'])\n",
    "sounds_data['datetime'] = pd.to_datetime(sounds_data['datetime'])\n",
    "sounds_hive_data = sounds_data[sounds_data['name'] == hive_under_analysis]\n",
    "\n",
    "print(f\"Got dataset of size: {len(sounds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CONTRASTIVE AE - keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sounds_data[sounds_data['name'] == 'smrpiclient7']['periodogram'].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.contrastive_keras import contrastive_keras_vae\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "target_keras = np.stack(sounds_data[sounds_data['name'] == 'smrpiclient7']['periodogram'].to_numpy()[:3000]).squeeze()\n",
    "background_keras = np.stack(sounds_data[sounds_data['name'] == 'smrpiclient6']['periodogram'].to_numpy()[:3000]).squeeze()\n",
    "\n",
    "print(f'training cvae (keras) with target of shape: {target_keras.shape} '\n",
    "    f'and background of shape: {background_keras.shape}')\n",
    "\n",
    "assert(target_keras.shape == background_keras.shape)\n",
    "\n",
    "cvae_keras, cvae_fg, z_encoder_keras, s_encoder_keras, cvae_keras_decoder = contrastive_keras_vae(\n",
    "    input_dim=2048, intermediate_dim=512, latent_dim=16, disentangle=True, gamma=1)\n",
    "history = cvae_keras.fit([target_keras, background_keras], epochs=50, batch_size=100, \n",
    "                         validation_data=([target_keras, background_keras], None), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train BASIC AE\n",
    "Here we train basic fully connected autoencoder on data from particular hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "\n",
    "from torch.utils import data as tdata\n",
    "from utils.data_utils import prepare_dataset1d\n",
    "from utils.autoencoder_utils import BasicAutoencoder, train_model\n",
    "\n",
    "TRAIN_MODEL = False\n",
    "PATH = 'basic_ae.pth'\n",
    "\n",
    "modelBasicAE = BasicAutoencoder().to(device)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    train_dataloader, val_dataloader = prepare_dataset1d(sounds_hive_data['spectrogram_mean'], train_ratio=0.8, batch_size=64)\n",
    "    modelBasicAE = train_model(modelBasicAE,\n",
    "                           learning_rate=1e-3, weight_decay=1e-5, num_epochs=10, patience=20,\n",
    "                           dataloader_train=train_dataloader, dataloader_val=val_dataloader,\n",
    "                           checkpoint_name=PATH)\n",
    "else:\n",
    "    if os.path.isfile(PATH) and os.access(PATH, os.R_OK):\n",
    "        modelBasicAE.load_state_dict(torch.load(PATH))\n",
    "        print('model load success!')\n",
    "    else:\n",
    "        print(f'ERROR! There is no file: {PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CONV AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we train convolutional autoencoder on data from particular hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils import data as tdata\n",
    "from utils.data_utils import prepare_dataset2d\n",
    "from utils.autoencoder_utils import ConvAutoencoder, train_model\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "PATH = 'conv_ae.pth'\n",
    "\n",
    "modelConvAE = ConvAutoencoder().to(device)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    train_set, val_set = prepare_dataset2d(sounds_hive_data['spectrogram'], train_ratio=0.8)\n",
    "    \n",
    "    dataloader_train = tdata.DataLoader(train_set, batch_size=6, shuffle=True)\n",
    "    dataloader_val = tdata.DataLoader(val_set, batch_size=6, shuffle=True)\n",
    "    \n",
    "    modelConvAE = train_model(modelConvAE,\n",
    "                               learning_rate=1e-3, weight_decay=1e-6, num_epochs=100, patience=20,\n",
    "                               dataloader_train=dataloader_train, dataloader_val=dataloader_val,\n",
    "                               checkpoint_name=PATH)\n",
    "else:\n",
    "    if os.path.isfile(PATH) and os.access(PATH, os.R_OK):\n",
    "        modelConvAE.load_state_dict(torch.load(PATH))\n",
    "        print('model load success!')\n",
    "    else:\n",
    "        print(f'ERROR! There is no such file: {PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train vae autoencoder on data from particular hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils import data as tdata\n",
    "from torchvision import transforms\n",
    "from utils.data_utils import prepare_dataset1d\n",
    "from utils.contrastive_pytorch import VAE, train_vae_model\n",
    "from utils.customdataset import CustomDataset\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "PATH = 'vae.pth'\n",
    "\n",
    "modelVAE = VAE(encoder_layer_sizes=[2048, 512, 64], latent_size=16, decoder_layer_sizes=[64, 512, 2048])\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    data = torch.Tensor(sounds_hive_data['periodogram'])\n",
    "    data = data[:, None, :]\n",
    "    idx_split = data.size(0)*80//100\n",
    "    train_dataset = CustomDataset(data[:idx_split], should_scale=True)\n",
    "    val_dataset = CustomDataset(data[idx_split:], should_scale=True)\n",
    "    train_dataloader = tdata.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_dataloader = tdata.DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    modelVAE = train_vae_model(modelVAE, learning_rate=1e-3, weight_decay=1e-5, num_epochs=100, patience=100,\n",
    "                              dataloader_train=train_dataloader, dataloader_val=val_dataloader, checkpoint_name=PATH)\n",
    "else:\n",
    "    if os.path.isfile(PATH) and os.access(PATH, os.R_OK):\n",
    "        modelVAE.load_state_dict(torch.load(PATH))\n",
    "        print('model load success!')\n",
    "    else:\n",
    "        print(f'ERROR! There is no such file: {PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# read just sample file as probably all has the same sampling rate\n",
    "sample_rate, samples = wavfile.read('C:\\\\Users\\\\tymot\\\\projects\\\\001.smartula\\\\smartula-analysis\\\\smartula-analysis'\n",
    "                           '\\\\measurements\\\\smartulav2\\\\smrpiclient6_10082020-15092020\\\\DEADBEEF94-2020-08-10T03-11-50.wav')\n",
    "idx = random.randint(0, sounds_hive_data.shape[0] - 1)\n",
    "idx = 2240\n",
    "with torch.no_grad():   \n",
    "    modelConvAE.eval()\n",
    "    modelBasicAE.eval()\n",
    "    modelVAE.eval()\n",
    "    \n",
    "    # Get sound from sounds hive data (with respect to hive under analysis)\n",
    "    sound = sounds_hive_data.iloc[idx]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12,8))\n",
    "    fig.suptitle(f'Convolutional Autoencoder real vs output for sample id: {idx}')\n",
    "    fig.tight_layout(pad = 3.0)\n",
    "\n",
    "    frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "    freq_slice = np.where((frequencies < fmax))\n",
    "    frequencies = frequencies[freq_slice]\n",
    "    times = (np.arange(0, sound['spectrogram'].shape[1])*hop_len)/sample_rate\n",
    "    elem = sound['spectrogram']\n",
    "    elem = elem[None, None, :, :]\n",
    "    elem = torch.Tensor(elem).to(device)\n",
    "    \n",
    "    elem_mean = sound['spectrogram_mean']\n",
    "    elem_mean = elem_mean[None, :]\n",
    "    elem_mean = torch.Tensor(elem_mean).to(device)\n",
    "    \n",
    "    axs[0][0].set_title('real spectrogram')\n",
    "    axs[0][0].pcolormesh(times, frequencies, sound['spectrogram'])\n",
    "    axs[0][1].set_title('encoded conv2d')\n",
    "    axs[0][1].pcolormesh(times, frequencies, modelConvAE(elem.to(device)).cpu().numpy().squeeze())\n",
    "    \n",
    "    # prepare data for vae prediction\n",
    "    freqs = np.arange(sound['periodogram'].shape[1])\n",
    "    input_data = torch.Tensor(sound['periodogram']).to(device)\n",
    "    vae_output, _, _ = modelVAE(input_data)\n",
    "    \n",
    "    # prepare data for cvae prediction\n",
    "    s_mean, s_log_var, s = s_encoder_keras.predict(sound['periodogram'])\n",
    "    latent = np.concatenate((np.zeros_like(s), s), axis = 1)\n",
    "    cvae_output = cvae_keras_decoder.predict(latent)\n",
    "\n",
    "    axs[1][0].set_title('spectrogram mean')\n",
    "    axs[1][0].plot(sound['spectrogram_mean'], label='real', color='blue')\n",
    "    axs[1][0].plot(modelBasicAE(elem_mean).cpu().numpy().squeeze(), label='encoded', color='red')\n",
    "    axs[1][0].legend()\n",
    "    \n",
    "    axs[1][1].set_title('periodogram')\n",
    "    axs[1][1].plot(freqs, periodogram_df, label='real', color='blue')\n",
    "    axs[1][1].plot(freqs, vae_output.cpu().numpy().squeeze(), label='vae encoded', color='red')\n",
    "    axs[1][1].plot(freqs, cvae_output.squeeze(), label='cvae encoded', color='green')\n",
    "    axs[1][1].legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add temperature/humidity/gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import read_sensor_data\n",
    "\n",
    "start_time = '2020-08-10 00:00:00'\n",
    "end_time = '2020-09-16 00:00:00'\n",
    "print(f\"extracting data for hive under analysis: {hive_under_analysis} from {start_time} to {end_time}...\")\n",
    "\n",
    "df_hives_sound = pd.DataFrame(sounds_data)\n",
    "df_hive_sound_ua = df_hives_sound[(df_hives_sound['name'] == hive_under_analysis)\n",
    "                                 & (df_hives_sound['datetime'] > start_time)\n",
    "                                 & (df_hives_sound['datetime'] < end_time)]\n",
    "df_hive_sound_ua.set_index('datetime', inplace=True)\n",
    "print(f\"-> prepared base of {df_hive_sound_ua.count()['spectrogram']} number of sound spectrums <-\")\n",
    "\n",
    "df_hive_temperature_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-temperature.csv',\n",
    "                                          hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'temperature')\n",
    "df_hive_humidity_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-humidity.csv',\n",
    "                                       hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'humidity')\n",
    "df_hive_alcohol_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-alcohol.csv',\n",
    "                                      hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'alcohol')\n",
    "df_hive_aceton_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-aceton.csv',\n",
    "                                     hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'aceton')\n",
    "df_hive_amon_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-jon-amonowy.csv',\n",
    "                                   hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'jon-amonowy')\n",
    "df_hive_toluen_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-toluen.csv',\n",
    "                                     hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'toluen')\n",
    "df_hive_co2_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-co2.csv',\n",
    "                                    hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'co2')\n",
    "df_hive_siarkowodor_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-siarkowodor.csv',\n",
    "                                          hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'siarkowodor')\n",
    "df_hive_metanotiol_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-metanotiol.csv',\n",
    "                                         hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'metanotiol')\n",
    "df_hive_trimetyloamina_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-trimetyloamina.csv',\n",
    "                                         hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'trimetyloamina')\n",
    "df_hive_wodor_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-wodor.csv',\n",
    "                                    hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'wodor')\n",
    "df_hive_co_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-co.csv',\n",
    "                                 hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'co')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check autocorrelation for specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import merge_dataframes_ontimestamp, merge_columns\n",
    "from utils.autoencoder_utils import conv2d_encode, basic_ae_encode\n",
    "from utils.contrastive_pytorch import vae_encode\n",
    "from utils.contrastive_keras import cvae_encode\n",
    "\n",
    "df_hive_data = merge_dataframes_ontimestamp(df_hive_sound_ua,\n",
    "                                            df_hive_temperature_ua, df_hive_humidity_ua,\n",
    "                                            df_hive_alcohol_ua, df_hive_aceton_ua, df_hive_amon_ua, df_hive_toluen_ua, df_hive_co2_ua,\n",
    "                                            df_hive_siarkowodor_ua, df_hive_metanotiol_ua, df_hive_trimetyloamina_ua, df_hive_wodor_ua,\n",
    "                                            df_hive_co_ua)\n",
    "\n",
    "df_hive_data['conv_ae'] = conv2d_encode(modelConvAE, df_hive_data['spectrogram'].to_list())\n",
    "df_hive_data['basic_ae'] = basic_ae_encode(modelBasicAE, df_hive_data['spectrogram_mean'].to_list())\n",
    "df_hive_data['vae'] = vae_encode(modelVAE, df_hive_data['periodogram'].to_list()).tolist()\n",
    "df_hive_data['cvae_s'] = cvae_encode(s_encoder_keras, df_hive_data['periodogram']).tolist()\n",
    "df_hive_data['cvae_z'] = cvae_encode(z_encoder_keras, df_hive_data['periodogram']).tolist()\n",
    "\n",
    "df_hive_data['bae_feature_vector'] = merge_columns(df_hive_data, ['basic_ae', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['conv_feature_vector'] = merge_columns(df_hive_data, ['conv_ae', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['mfcc_feature_vector'] = merge_columns(df_hive_data, ['mfccs', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['vae_feature_vector'] = merge_columns(df_hive_data, ['vae', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['cvae_s_feature_vector'] = merge_columns(df_hive_data, ['cvae_s', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['cvae_z_feature_vector'] = merge_columns(df_hive_data, ['cvae_z', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.data_utils import search_best_night_day\n",
    "\n",
    "start_hours = [20, 21, 22, 23, 0, 1, 2, 3, 4]\n",
    "\n",
    "df_hive_data_scaled = pd.DataFrame(df_hive_data)\n",
    "\n",
    "# data for convolutional autoencoder\n",
    "df_hive_data_scaled['conv_feature_vector'] = StandardScaler().fit_transform(df_hive_data['conv_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for basic autoencoder\n",
    "df_hive_data_scaled['bae_feature_vector'] = StandardScaler().fit_transform(df_hive_data['bae_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for mfcc features\n",
    "df_hive_data_scaled['mfcc_feature_vector'] = StandardScaler().fit_transform(df_hive_data['mfcc_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for vae features\n",
    "df_hive_data_scaled['vae_feature_vector'] = StandardScaler().fit_transform(df_hive_data['vae_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for cvae s features\n",
    "df_hive_data_scaled['cvae_s_feature_vector'] = StandardScaler().fit_transform(df_hive_data['cvae_s_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for cvae s features\n",
    "df_hive_data_scaled['cvae_z_feature_vector'] = StandardScaler().fit_transform(df_hive_data['cvae_z_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for plain mfcc \n",
    "mfccs = [hive_data['features']['mfcc_avg'] for hive_data in hives_data if hive_data['id'] == hive_under_analysis]\n",
    "mfccs = StandardScaler().fit_transform(mfccs)\n",
    "datetimes = [hive_data['datetime'] for hive_data in hives_data if hive_data['id'] == hive_under_analysis]\n",
    "mfccs_data = list(zip(datetimes, mfccs))\n",
    "pd_mfcc_data = pd.DataFrame(mfccs_data, columns=['datetime', 'mfcc'])\n",
    "pd_mfcc_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# calculate one class SVM match\n",
    "print('calculating mfccs match...', end=' ', flush=True)\n",
    "mfcc_accs = search_best_night_day(pd_mfcc_data, 'mfcc', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(mfcc_accs)}/{len(mfcc_accs[0])}')\n",
    "print('calculating conv ae feature vector match...', end=' ', flush=True)\n",
    "conv_ae_accs = search_best_night_day(df_hive_data_scaled, 'conv_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(conv_ae_accs)}/{len(conv_ae_accs[0])}')\n",
    "print('calculating basic ae feature vector match...', end=' ', flush=True)\n",
    "bae_accs = search_best_night_day(df_hive_data_scaled, 'bae_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(bae_accs)}/{len(bae_accs[0])}')\n",
    "print('calculating mfccs extended feature vector match...', end=' ', flush=True)\n",
    "mffce_accs = search_best_night_day(df_hive_data_scaled, 'mfcc_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(mffce_accs)}/{len(mffce_accs[0])}')\n",
    "print('calculating vae feature vector match...', end=' ', flush=True)\n",
    "vae_accs = search_best_night_day(df_hive_data_scaled, 'vae_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(vae_accs)}/{len(vae_accs[0])}')\n",
    "print('calculating cvae s feature vector match...', end=' ', flush=True)\n",
    "cvae_accs_s = search_best_night_day(df_hive_data_scaled, 'cvae_s_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(cvae_accs_s)}/{len(cvae_accs_s[0])}')\n",
    "print('calculating cvae z feature vector match...', end=' ', flush=True)\n",
    "cvae_accs_z = search_best_night_day(df_hive_data_scaled, 'cvae_z_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(cvae_accs_z)}/{len(cvae_accs_z[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import plot_hour_shift\n",
    "\n",
    "plot_hour_shift(mfcc_accs, conv_ae_accs, bae_accs, mffce_accs, vae_accs, cvae_accs_s, cvae_accs_z,\n",
    "                labels_list=['mfcc', 'conv', 'bae', 'mfcce', 'vae', 'cvae_s', 'cvae_z'],\n",
    "                xticklabels=[str(start_hour) for start_hour in start_hours],\n",
    "                save_path = 'data\\\\outputs\\\\zs_encoder_0_s.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize on 2D map, we basically perform TSNE and PCA dimension reduction in order to visualize night and day. Probably this will be not efficent but it is worth to give a shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "start_hour = 23\n",
    "end_hour = 2\n",
    "\n",
    "reduce_df = pd.DataFrame(df_hive_data)\n",
    "reduce_df['feature_vector'] = StandardScaler().fit_transform(df_hive_data['bae_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "reduced_ae_pca = PCA(n_components=2).fit_transform(reduce_df['feature_vector'].values.tolist())\n",
    "reduced_ae_tsne =  TSNE(n_components=2, perplexity=100, learning_rate=500).fit_transform(reduce_df['feature_vector'].values.tolist())\n",
    "is_night_list = (reduce_df.index.hour >= start_hour) | (reduce_df.index.hour <= end_hour)\n",
    "                \n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "labels = ['day', 'night']\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(10,10))\n",
    "\n",
    "axs[0].scatter(x=[data[0] for data in reduced_ae_pca],\n",
    "               y=[data[1] for data in reduced_ae_pca],\n",
    "               c=[colors[night] for night in is_night_list],\n",
    "              alpha=0.3)\n",
    "axs[0].set_title('PCA')\n",
    "\n",
    "axs[1].scatter(x=[data[0] for data in reduced_ae_tsne],\n",
    "               y=[data[1] for data in reduced_ae_tsne],\n",
    "               c=[colors[night] for night in is_night_list],\n",
    "              alpha=0.3)\n",
    "axs[1].set_title('TSNE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(reduce_df['feature_vector'].values.tolist())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def plot_distribution(distribution_dict, bin_size):\n",
    "    \"\"\" Plotting distribiution for dictionary elements\"\"\"\n",
    "    colors = ['blue', 'green', 'red', 'yellow', 'black', 'pink', 'purple']\n",
    "    rms_max = 0\n",
    "    rms_min = 65535\n",
    "    for k, v in rmses.items():\n",
    "        if np.max(v) > rms_max:\n",
    "            rms_max = np.max(v)\n",
    "        if np.min(v) < rms_min:\n",
    "            rms_min = np.min(v)\n",
    "        \n",
    "    plt.figure()\n",
    "    for idx, (k, v) in enumerate(distribution_dict.items()):\n",
    "        plt.hist(v, color=colors[idx%len(colors)], bins=int(np.abs(rms_max-rms_min)/bin_size))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of code for calculating autocorrelaction for specific feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "features = ['conv_ae', 'humidity', 'temperature',\n",
    "            'alcohol', 'aceton', 'jon-amonowy',\n",
    "            'toluen', 'co2', 'siarkowodor',\n",
    "            'metanotiol', 'trimetyloamina', 'wodor', 'co']\n",
    "\n",
    "feature = features[12]\n",
    "data_to_autocorr = df_hive_co_ua\n",
    "\n",
    "roll_len = 3\n",
    "interval = (data_to_autocorr.index[2] - data_to_autocorr.index[1]).seconds//60%60\n",
    "\n",
    "y2 = data_to_autocorr[feature].rolling(window=roll_len).mean().values\n",
    "y_corr = y2[roll_len:]\n",
    "x_corelation = np.arange(start=0, step=2, stop=150)\n",
    "\n",
    "fig, axes = plt.subplots(1, figsize=(8,5))\n",
    "x = plot_acf(y_corr, lags=x_corelation, zero=False, ax=axes)\n",
    "x_raw = acf(y_corr, nlags=150)\n",
    "axes.set_title(f'{feature} autocorrelaction')\n",
    "axes.set_xlabel(f'Lag (1 lag = {interval} minutes)') \n",
    "axes.set_ylabel('Correlation')\n",
    "axes.set_xticks(np.arange(0, 151, step=10))\n",
    "\n",
    "print(f'{feature} with max {max(x_raw[60:]):.2f} at {60 + np.argmax(x_raw[60:])}')\n",
    "\n",
    "# temperature with max 0.74 at 93 (15 mint)\n",
    "# humidity with max 0.58 at 92 (15 min)\n",
    "# alcohol with max 0.53 at 134 (10 min)\n",
    "# aceton with max 0.52 at 133 (10 min)\n",
    "# jon-amonowy with max 0.57 at 133 (10 min)\n",
    "# toluen with max 0.52 at 134 (10 min)\n",
    "# co2 with max 0.54 at 133 (10 min)\n",
    "# siarkowodor with max 0.16 at 142 (10 min)\n",
    "# metanotiol with max 0.34 at 140 (10 min)\n",
    "# trimetyloamina with max 0.56 at 138 (10 min)\n",
    "# wodor with max 0.14 at 142 (10 min)\n",
    "# co with max 0.62 at 134 (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.autoencoder_utils as ae\n",
    "import utils.data_utils as du\n",
    "import utils.customdataset as cd\n",
    "import utils.contrastive_pytorch as cp\n",
    "import utils.contrastive_keras as ck\n",
    "import utils.periodogram_dataset as pd\n",
    "\n",
    "importlib.reload(pd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}