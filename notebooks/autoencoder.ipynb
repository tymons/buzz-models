{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "hives_ids = [\"smrpiclient7\", \"smrpiclient6\", \"smrpiclient3\"]\n",
    "\n",
    "hive_under_analysis = hives_ids[0]\n",
    "timezone_offset_hours = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load train data sound samples and prepare spectrogram, periodogram and mfcc features (along with some data to visualize this). We should provide data with **utc timestamps** as it will be shifted with `timezone_offset_hours` var. What we also do is remove those samples which has strange rms signal. Threshold 0.8 was chosen based on `plot_distribution` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound data preparation for hive: smrpiclient7 which has 3367 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3367/3367 [01:52<00:00, 30.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "Sound data preparation for hive: smrpiclient6 which has 3172 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3172/3172 [01:48<00:00, 29.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "Sound data preparation for hive: smrpiclient3 which has 602 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 602/602 [00:17<00:00, 33.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "got full dataset of 7033 sound samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft\n",
    "from scipy.signal import decimate\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "sound_time_ms = 2000\n",
    "# ~93 ms for fft window\n",
    "nfft = 4096\n",
    "# ~34% overlapping\n",
    "hop_len = (nfft//3) + 30\n",
    "# This can be manipulated to adjust number of bins for conv layer\n",
    "fmax = 2750\n",
    "\n",
    "hives_data = []\n",
    "rmses = {}\n",
    "max_to_norm = 0\n",
    "for idx, hive_id in enumerate(hives_ids):\n",
    "    sound_files = [f for f in glob.glob(f\"..\\\\measurements\\\\smartulav2\\\\{hive_id}_*\\\\*.wav\")]\n",
    "    print(f\"Sound data preparation for hive: {hive_id} which has {len(sound_files)} recordings...\", end=' ', flush=True)\n",
    "    rmses[hive_id] = []\n",
    "    for file in tqdm(sound_files):\n",
    "        sample_rate, sound_samples = wavfile.read(file)\n",
    "        sound_samples = sound_samples.T[0]/(2.0**31)\n",
    "        rms = np.sqrt(sum(sound_samples**2)/len(sound_samples))\n",
    "        if(rms < 0.7):    # that threshold was observed from plot_distribution() function\n",
    "            rmses[hive_id].append(rms)\n",
    "            \n",
    "            mfccs = librosa.feature.mfcc(y=sound_samples, sr=sample_rate, n_fft=nfft, hop_length=hop_len, n_mfcc=13)\n",
    "            np_mfcc_avg = np.mean(mfccs, axis=1)\n",
    "\n",
    "            spectrogram = librosa.core.stft(sound_samples, n_fft=nfft, hop_length=hop_len)\n",
    "            spectrogram_magnitude = np.abs(spectrogram)\n",
    "            spectrogram_phase = np.angle(spectrogram)\n",
    "            spectrogram_db = librosa.amplitude_to_db(spectrogram_magnitude, ref=np.max)\n",
    "            frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "            times = (np.arange(0, spectrogram_magnitude.shape[1])*hop_len)/sample_rate\n",
    "\n",
    "            freq_slice = np.where((frequencies < fmax))\n",
    "            frequencies = frequencies[freq_slice]\n",
    "            spectrogram_db = spectrogram_db[freq_slice, :][0]    \n",
    "            spectrogram_mean = np.mean(spectrogram_db, axis=1)\n",
    "\n",
    "#             spectrogram_db_decimated = decimate(spectrogram_db.T, 4).T\n",
    "#             frequencies_decimated = decimate(frequencies, 4)\n",
    "            \n",
    "            filename = file.rsplit('\\\\', 1)[-1]\n",
    "            utc_timestamp = filename[filename.index('-')+1:].rsplit(\".wav\")[0]\n",
    "            sound_datetime = datetime.strptime(utc_timestamp, '%Y-%m-%dT%H-%M-%S') + timedelta(hours=timezone_offset_hours)\n",
    "            hives_data.append([sound_datetime, hive_id, sound_samples, [frequencies, times, spectrogram_db], np_mfcc_avg, spectrogram_mean])\n",
    "    print(\" done.\")\n",
    "\n",
    "print(f\"got full dataset of {len(hives_data)} sound samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got dataset of size: 7033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "# extract every spectrogram from hives data list\n",
    "spectrograms = [hive_data[3][2] for hive_data in hives_data]\n",
    "# standarize every periodogram from stft so data for spectrogram will have zero mean and unit variance\n",
    "standarized_spectrograms = [sc.fit_transform(spectrogram.T).T for spectrogram in spectrograms]\n",
    "# scale every standarized periodogram \n",
    "scaled_spectrogram = [mm.fit_transform(spectrogram_stan.T).T for spectrogram_stan in standarized_spectrograms]\n",
    "# get datatime, names and mfcc\n",
    "datetimes = [hive_data[0] for hive_data in hives_data]\n",
    "names = [hive_data[1] for hive_data in hives_data]\n",
    "mfccs = [hive_data[4] for hive_data in hives_data]\n",
    "# standarize and scale periodogram for sounds \n",
    "periodograms_mean = [data[5] for data in hives_data]\n",
    "standarized_periodograms = StandardScaler().fit_transform(periodograms_mean)\n",
    "scaled_periodograms = MinMaxScaler().fit_transform(standarized_periodograms)\n",
    "\n",
    "sounds = list(zip(scaled_spectrogram, mfccs, scaled_periodograms, datetimes, names))\n",
    "\n",
    "sounds_data = pd.DataFrame(sounds, columns=['spectrogram', 'mfccs', 'periodogram', 'datetime', 'name'])\n",
    "sounds_data['datetime'] = pd.to_datetime(sounds_data['datetime'])\n",
    "sounds_hive_data = sounds_data[sounds_data['name'] == hive_under_analysis]\n",
    "\n",
    "print(f\"Got dataset of size: {len(sounds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BASIC AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train basic fully connected autoencoder on data from particular hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_basic_train, ae_basic_val = prepare_dataset1d(sounds_hive_data['periodogram'], train_ratio=0.8)\n",
    "\n",
    "dl_aebasic_train = tdata.DataLoader(ae_basic_train, batch_size=32, shuffle=True)\n",
    "dl_aebasic_val = tdata.DataLoader(ae_basic_val, batch_size=32, shuffle=True)\n",
    "\n",
    "modelBasicAE = BasicAutoencoder()\n",
    "modelBasicAE = train_model(modelBasicAE, learning_rate=1e-3, weight_decay=1e-5, num_epochs=100, patience=20, dataloader_train=dl_aebasic_train, dataloader_val=dl_aebasic_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CONV AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train convolutional autoencoder on data from particular hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: torch.Size([3367, 256, 64])\n",
      "Train set size: 3030\n",
      "Validation set size: 337\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.utils import data as tdata\n",
    "\n",
    "train_data_size = (sounds_hive_data.shape[0]*90)//100\n",
    "val_data_size = sounds_hive_data.shape[0] - train_data_size\n",
    "\n",
    "dataset_tensor = torch.Tensor(sounds_hive_data['spectrogram'].values.tolist())\n",
    "print(f\"Dataset shape: {dataset_tensor.shape}\")\n",
    "print(f\"Train set size: {train_data_size}\")\n",
    "print(f\"Validation set size: {val_data_size}\")\n",
    "\n",
    "# add one extra dimension as it is required for conv layer\n",
    "dataset_tensor = dataset_tensor[:, None, :, :] \n",
    "dataset = tdata.TensorDataset(dataset_tensor)\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_data_size, val_data_size])\n",
    "\n",
    "dataloader_train = tdata.DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "dataloader_val = tdata.DataLoader(val_set, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del modelConvAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1/100] train_loss: 0.06261 valid_loss: 0.04426 checkpoint!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# del modelConvAE\n",
    "\n",
    "modelConvAE = ConvAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(modelConvAE.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "# monitor training loss per batch\n",
    "train_loss = []\n",
    "# monitor validation loss per batch\n",
    "val_loss = []\n",
    "# save avg train losses for early stopping visualization\n",
    "avg_train_loss = []\n",
    "# save avg train losses for early stopping visualization\n",
    "avg_val_loss = [] \n",
    "# patience when stop training\n",
    "patience = 20\n",
    "# counter for patience in early sotpping\n",
    "patience_counter = 0\n",
    "# best validation score\n",
    "best_val_loss = -1\n",
    "# model checkpoint filename\n",
    "checkpoint_filename = 'checkpoint.pth'\n",
    "# early stopping epoch\n",
    "win_epoch = 0\n",
    "    \n",
    "for epoch in range(1, num_epochs+1):    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    modelConvAE.train()\n",
    "    for data in dataloader_train:\n",
    "        # transfer data to device\n",
    "        spectrogram = data[0].to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = modelConvAE(spectrogram)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, spectrogram)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss.append(float(loss.item()))\n",
    "        \n",
    "    ###################\n",
    "    # val the model   #\n",
    "    ###################\n",
    "    modelConvAE.eval()\n",
    "    for val_data in dataloader_val:\n",
    "        # transfer data to device\n",
    "        spectrogram_val = val_data[0].to(device)\n",
    "        # forward pass\n",
    "        voutputs = modelConvAE(spectrogram_val)\n",
    "        # calculate the loss\n",
    "        vloss = criterion(voutputs, spectrogram_val)\n",
    "        # update running val loss\n",
    "        val_loss.append(float(vloss.item()))\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = np.average(train_loss)\n",
    "    val_loss = np.average(val_loss)\n",
    "    avg_train_loss.append(train_loss)\n",
    "    avg_val_loss.append(val_loss)\n",
    "    \n",
    "    epoch_len = len(str(num_epochs))\n",
    "    # print avg training statistics \n",
    "    print(f'[{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] train_loss: {train_loss:.5f} valid_loss: {val_loss:.5f}', end=' ', flush=True)\n",
    "    \n",
    "    if val_loss < best_val_loss or best_val_loss == -1:\n",
    "        # new checkpoint\n",
    "        print(\"checkpoint!\")\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(modelConvAE.state_dict(), checkpoint_filename)\n",
    "        win_epoch = epoch\n",
    "    elif patience_counter >= patience:\n",
    "        print(\"early stopping.\")\n",
    "        print(f\"=> loading checkpoint {checkpoint_filename}\")\n",
    "        device = torch.device(\"cuda\")\n",
    "        modelConvAE.load_state_dict(torch.load(checkpoint_filename))\n",
    "        break\n",
    "    else:\n",
    "        print(\".\")\n",
    "        patience_counter = patience_counter + 1\n",
    "        \n",
    "    # clear batch losses\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(1, epoch + 1), avg_train_loss, 'r', label=\"train loss\")\n",
    "plt.plot(np.arange(1, epoch + 1), avg_val_loss, 'b', label=\"validation loss\")\n",
    "plt.axvline(win_epoch, linestyle='--', color='g',label='Early Stopping Checkpoint')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = conv2d_encode(modelConvAE, scaled_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "idx = random.randint(0, len(hives_data) - 1)\n",
    "with torch.no_grad():\n",
    "    fig, axs = plt.subplots(2, 1)\n",
    "    frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "    freq_slice = np.where((frequencies < fmax))\n",
    "    frequencies = frequencies[freq_slice]\n",
    "    times = (np.arange(0, spectrogram_magnitude.shape[1])*hop_len)/sample_rate    \n",
    "    elem = scaled_spectrogram[idx]\n",
    "    elem = elem[None, None,: ,:]\n",
    "    elem = torch.Tensor(elem)\n",
    "\n",
    "    axs[0].pcolormesh(times, frequencies, scaled_spectrogram[idx])\n",
    "    axs[1].pcolormesh(times, frequencies, modelConvAE(elem.to(device)).cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add temperature/humidity/gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '2020-08-10 00:00:00'\n",
    "end_time = '2020-09-16 00:00:00'\n",
    "print(f\"extracting data for hive under analysis: {hive_under_analysis} from {start_time} to {end_time}...\")\n",
    "\n",
    "df_hives_sound = pd.DataFrame(sounds_data)\n",
    "df_hive_sound_ua = df_hives_sound[(df_hives_sound['name'] == hive_under_analysis)\n",
    "                                 & (df_hives_sound['datetime'] > start_time)\n",
    "                                 & (df_hives_sound['datetime'] < end_time)]\n",
    "df_hive_sound_ua.set_index('datetime', inplace=True)\n",
    "print(f\"-> prepared base of {df_hive_sound_ua.count()['spectrogram']} numer of sound spectrum <-\")\n",
    "\n",
    "df_hive_temperature_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-temperature.csv', hive_under_analysis, start_time, end_time, 'temperature')\n",
    "df_hive_humidity_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-humidity.csv', hive_under_analysis, start_time, end_time, 'humidity')\n",
    "df_hive_alcohol_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-alcohol.csv', hive_under_analysis, start_time, end_time, 'alcohol')\n",
    "df_hive_aceton_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-aceton.csv', hive_under_analysis, start_time, end_time, 'aceton')\n",
    "df_hive_amon_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-jon-amonowy.csv', hive_under_analysis, start_time, end_time, 'jon-amonowy')\n",
    "df_hive_toluen_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-toluen.csv', hive_under_analysis, start_time, end_time, 'toluen')\n",
    "df_hive_co2_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-co2.csv', hive_under_analysis, start_time, end_time, 'co2')\n",
    "df_hive_siarkowodor_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-siarkowodor.csv', hive_under_analysis, start_time, end_time, 'siarkowodor')\n",
    "df_hive_metanotiol_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-metanotiol.csv', hive_under_analysis, start_time, end_time, 'metanotiol')\n",
    "df_hive_trimetyloamina_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-trimetyloamina.csv', hive_under_analysis, start_time, end_time, 'trimetyloamina')\n",
    "df_hive_wodor_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-wodor.csv', hive_under_analysis, start_time, end_time, 'wodor')\n",
    "df_hive_co_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-co.csv', hive_under_analysis, start_time, end_time, 'co')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check autocorrelation for specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change sound base (mfccs dataframe or conv ae dataframe)\n",
    "sound_base_feature = 'conv_ae' \n",
    "\n",
    "df_hive_data = merge_dataframes_ontimestamp(df_hive_sound_ua,\n",
    "                                            df_hive_temperature_ua, df_hive_humidity_ua,\n",
    "                                            df_hive_alcohol_ua, df_hive_aceton_ua, df_hive_amon_ua, df_hive_toluen_ua, df_hive_co2_ua,\n",
    "                                            df_hive_siarkowodor_ua, df_hive_metanotiol_ua, df_hive_trimetyloamina_ua, df_hive_wodor_ua,\n",
    "                                            df_hive_co_ua)\n",
    "\n",
    "if sound_base_feature == 'conv_ae':\n",
    "    print(f\"encoding sound samples with autoencoder...\", end=' ')\n",
    "    df_hive_data['conv_ae'] = conv2d_encode(modelConvAE, df_hive_data['spectrogram'].to_list())\n",
    "    print(f\"finish!\")\n",
    "\n",
    "df_hive_data['feature_vector'] = merge_columns(df_hive_data, [sound_base_feature, 'humidity', 'temperature',\n",
    "                                                'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2',\n",
    "                                                'trimetyloamina',\n",
    "#                                                 'siarkowodor', 'metanotiol', 'trimetyloamina', 'wodor',\n",
    "                                                'co'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "start_hours = [20, 21, 22, 23, 0, 1, 2, 3, 4]\n",
    "\n",
    "# data for convolutional autoencoder\n",
    "pd_convae_data = pd.DataFrame(df_hive_data)\n",
    "pd_convae_data['feature_vector'] = StandardScaler().fit_transform(df_hive_data['feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for mfcc \n",
    "mfccs = [hive_data[4] for hive_data in hives_data if hive_data[1] == hive_under_analysis]\n",
    "mfccs = StandardScaler().fit_transform(mfccs)\n",
    "datetimes = [hive_data[0] for hive_data in hives_data if hive_data[1] == hive_under_analysis]\n",
    "mfccs_data = list(zip(datetimes, mfccs))\n",
    "pd_mfcc_data = pd.DataFrame(mfccs_data, columns=['datetime', 'mfcc'])\n",
    "pd_mfcc_data.set_index('datetime', inplace=True)\n",
    "\n",
    "print('calculating mfccs match...', end=' ', flush=True)\n",
    "mfcc_accs = search_best_night_day(pd_mfcc_data, 'mfcc', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(mfcc_accs)}/{len(mfcc_accs[0])}')\n",
    "print('calculating feature vector match...', end=' ', flush=True)\n",
    "ae_accs = search_best_night_day(pd_convae_data, 'feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(ae_accs)}/{len(ae_accs[0])}')\n",
    "\n",
    "plot_hour_shift(mfcc_accs, ae_accs, xticklabels=[str(start_hour) for start_hour in start_hours])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize on 2D map, we basically perform TSNE and PCA dimension reduction in order to visualize night and day. Probably this will be not efficent but it is worth to hive a shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "start_hour = 23\n",
    "end_hour = 2\n",
    "\n",
    "reduce_df = pd.DataFrame(df_hive_data)\n",
    "reduce_df['feature_vector'] = StandardScaler().fit_transform(df_hive_data['feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "reduced_ae_pca = PCA(n_components=2).fit_transform(reduce_df['feature_vector'].values.tolist())\n",
    "reduced_ae_tsne =  TSNE(n_components=2, perplexity=100, learning_rate=500).fit_transform(reduce_df['feature_vector'].values.tolist())\n",
    "is_night_list = (reduce_df.index.hour >= start_hour) | (reduce_df.index.hour <= end_hour)\n",
    "                \n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "labels = ['day', 'night']\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(10,10))\n",
    "\n",
    "axs[0].scatter(x=[data[0] for data in reduced_ae_pca],\n",
    "               y=[data[1] for data in reduced_ae_pca],\n",
    "               c=[colors[night] for night in is_night_list],\n",
    "              alpha=0.3)\n",
    "axs[0].set_title('PCA')\n",
    "\n",
    "axs[1].scatter(x=[data[0] for data in reduced_ae_tsne],\n",
    "               y=[data[1] for data in reduced_ae_tsne],\n",
    "               c=[colors[night] for night in is_night_list],\n",
    "              alpha=0.3)\n",
    "axs[1].set_title('TSNE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(reduce_df['feature_vector'].values.tolist())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- Functions/Classes -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting sepctrogram by function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "def plot_spectrogram(frequency, time_x, spectrocgram, title):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.title(title)\n",
    "    plt.pcolormesh(time_x, frequency, spectrocgram)\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for convolutional autoencoder and some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class View(nn.Module):\n",
    "    \"\"\" Function for nn.Sequentional to reshape data \"\"\"\n",
    "    def __init__(self, shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "def conv2d_block(in_f, out_f, *args, **kwargs):\n",
    "    \"\"\" Function for building convolutional block\n",
    "\n",
    "        Attributes\n",
    "            in_f - number of input features\n",
    "            out_f - number of output features\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p=0.2)\n",
    "    )\n",
    "\n",
    "def conv2d_transpose_block(in_f, out_f, *args, **kwargs):\n",
    "    \"\"\" Function for building transpose convolutional block\n",
    "        \n",
    "        Attributes\n",
    "            in_f - number of input features\n",
    "            out_f - number of output features\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p=0.2)\n",
    "    )\n",
    "\n",
    "######################################\n",
    "#                                    #\n",
    "#   Main convolutional autoencoder   #\n",
    "#                                    #\n",
    "######################################\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        ## encoder layers ##\n",
    "        self.encoder = nn.Sequential(\n",
    "            # [1x256x64] => [64x256x64]\n",
    "            conv2d_block(1, 128, kernel_size=3, padding=1),\n",
    "            # [64x256x64] => [64x128x32]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [64x128x32] => [32x128x32]\n",
    "            conv2d_block(128, 64, kernel_size=3, padding=1),\n",
    "            # [32x128x32] => [32x64x16]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [32x64x16] => [16x64x16]\n",
    "            conv2d_block(64, 32, kernel_size=3, padding=1),\n",
    "            # [16x64x16] => [16x32x8]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [16x32x8] => [4x32x8]\n",
    "            conv2d_block(32, 16, kernel_size=3, padding=1),\n",
    "            # [4x32x8] => [4x16x4]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [4x16x4] => [1x256]\n",
    "            nn.Flatten(),\n",
    "            # [1x256] => [1x64]\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        self.decoder = nn.Sequential(\n",
    "            # [1x64] => [1x256]\n",
    "            nn.Linear(128, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            # [1x256] => [4x16x4]\n",
    "            View([-1, 16, 16, 4]),\n",
    "            # [4x16x4] => [16x32x8]\n",
    "            conv2d_transpose_block(16, 32, kernel_size=2, stride=2),\n",
    "            # [16x32x8] => [32x64x16]\n",
    "            conv2d_transpose_block(32, 64, kernel_size=2, stride=2),\n",
    "            # [32x64x16] => [64x128x32]\n",
    "            conv2d_transpose_block(64, 128, kernel_size=2, stride=2),\n",
    "            # [64x128x32] => [1x256x64]\n",
    "            nn.ConvTranspose2d(128, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic fully connected autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BasicAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=32, out_features=64),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for extracting ecoded data from trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_encode(model, data_intput):\n",
    "    \"\"\" Function for encoding data and returning encoded \"\"\"\n",
    "    dataset_tensor = torch.Tensor(data_intput)\n",
    "    dataset_tensor = dataset_tensor[:, None, :, :]\n",
    "    dataset_tensor = tdata.TensorDataset(dataset_tensor)\n",
    "    dataset = tdata.DataLoader(dataset_tensor, batch_size=32, shuffle=True)\n",
    "    encoded_data = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            periodograms = data[0].to(device)\n",
    "            output = modelConvAE.encoder(periodograms).cpu().numpy().squeeze()\n",
    "            encoded_data.extend(output)\n",
    "    \n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for preparing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.utils import data as tdata\n",
    "\n",
    "def prepare_dataset1d(data_df, train_ratio):\n",
    "    \"\"\" Function for preparing dataset for autoencoder \n",
    "    \n",
    "        attributes: data_df - pandas dataframe column\n",
    "        attributes: train_ratio - radio of train set size\n",
    "        return train_dataset, test_dataset\n",
    "    \"\"\"\n",
    "    train_data_size = int(data_df.shape[0]*train_ratio)\n",
    "    val_data_size = data_df.shape[0] - train_data_size\n",
    "\n",
    "    dataset_tensor = torch.Tensor(data_df.values.tolist())\n",
    "    print(f\"Dataset shape: {dataset_tensor.shape}\")\n",
    "    print(f\"Train set size: {train_data_size}\")\n",
    "    print(f\"Validation set size: {val_data_size}\")\n",
    "\n",
    "    # add one extra dimension as it is required for conv layer\n",
    "    # dataset_tensor = dataset_tensor[:, None, :] \n",
    "    dataset = tdata.TensorDataset(dataset_tensor)\n",
    "    train_set, val_set = tdata.random_split(dataset, [train_data_size, val_data_size])\n",
    "    \n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for model learning with early stopping and plotting tran graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, learning_rate, weight_decay, num_epochs, patience,\n",
    "                dataloader_train, dataloader_val, checkpoint_name='checkpoint.pth'):\n",
    "    \"\"\" \n",
    "    Function for training model \n",
    "    \n",
    "    attribute: model - model which should be trained\n",
    "    attribute: learning_rate - learning rate for the model\n",
    "    attribute: weight_decay - weight decay for learning\n",
    "    attribute: num_epochs - number epochs \n",
    "    attribute: patience - patience for early stopping\n",
    "    attribute: dataloader_train - train data loader\n",
    "    attribute: dataloader_val - validation data loader\n",
    "    attribute: checkpoint_name - checkpoint name for early stopping\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # monitor training loss per batch\n",
    "    train_loss = []\n",
    "    # monitor validation loss per batch\n",
    "    val_loss = []\n",
    "    # save avg train losses for early stopping visualization\n",
    "    avg_train_loss = []\n",
    "    # save avg train losses for early stopping visualization\n",
    "    avg_val_loss = [] \n",
    "    # counter for patience in early sotpping\n",
    "    patience_counter = 0\n",
    "    # best validation score\n",
    "    best_val_loss = -1\n",
    "    # model checkpoint filename\n",
    "    checkpoint_filename = checkpoint_name\n",
    "    # early stopping epoch\n",
    "    win_epoch = 0\n",
    "    \n",
    "    # pass model to gpu if is available\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):    \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for data in dataloader_train:\n",
    "            # transfer data to device\n",
    "            input_data = data[0].to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            outputs = model(input_data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, input_data)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        ###################\n",
    "        # val the model   #\n",
    "        ###################\n",
    "        model.eval()\n",
    "        for val_data in dataloader_val:\n",
    "            # transfer data to device\n",
    "            input_data_val = val_data[0].to(device)\n",
    "            # forward pass\n",
    "            val_outputs = model(input_data_val)\n",
    "            # calculate the loss\n",
    "            vloss = criterion(val_outputs, input_data_val)\n",
    "            # update running val loss\n",
    "            val_loss.append(vloss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_loss)\n",
    "        val_loss = np.average(val_loss)\n",
    "        avg_train_loss.append(train_loss)\n",
    "        avg_val_loss.append(val_loss)\n",
    "\n",
    "        epoch_len = len(str(num_epochs))\n",
    "        # print avg training statistics \n",
    "        print(f'[{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] train_loss: {train_loss:.5f} valid_loss: {val_loss:.5f}', end=' ', flush=True)\n",
    "\n",
    "        if val_loss < best_val_loss or best_val_loss == -1:\n",
    "            # new checkpoint\n",
    "            print(\"checkpoint!\")\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), checkpoint_filename)\n",
    "            win_epoch = epoch\n",
    "        elif patience_counter >= patience:\n",
    "            print(\"early stopping.\")\n",
    "            print(f\"=> loading checkpoint {checkpoint_filename}\")\n",
    "            device = torch.device(\"cuda\")\n",
    "            modelConvAE.load_state_dict(torch.load(checkpoint_filename))\n",
    "            break\n",
    "        else:\n",
    "            print(\".\")\n",
    "            patience_counter = patience_counter + 1\n",
    "\n",
    "        # clear batch losses\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.arange(1, epoch + 1), avg_train_loss, 'r', label=\"train loss\")\n",
    "    plt.plot(np.arange(1, epoch + 1), avg_val_loss, 'b', label=\"validation loss\")\n",
    "    plt.axvline(win_epoch, linestyle='--', color='g',label='Early Stopping Checkpoint')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for reading sensor data from file and some helper function for merging data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def read_sensor_data(filename, hive_sn, start_time, end_time, sensor_column_name):\n",
    "    \"\"\" Function for reading smartula sensor file (from grafana) and build pandas dataframe \"\"\"\n",
    "    df_sensor_data = pd.read_csv(filename, skiprows=1, sep=\";\")\n",
    "    \n",
    "    if hive_sn not in hives_ids:\n",
    "        print(f\"Hive {hive_sn} is not in hives_ids set! Returning empty dataframe\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # change series column to be coherent with sounds\n",
    "    for hive in hives_ids:\n",
    "        df_sensor_data.loc[df_sensor_data['Series'].str.contains(hive[2:]), 'Series'] = hive\n",
    "\n",
    "    # change column names to match sound\n",
    "    df_sensor_data.columns = ['name', 'datetime', sensor_column_name]\n",
    "    # convert timestamp to pandas timestamp\n",
    "    df_sensor_data['datetime'] = [(datetime.strptime(date_pd[:-6], '%Y-%m-%dT%H:%M:%S') +\n",
    "                                   timedelta(hours=timezone_offset_hours)) for date_pd in df_sensor_data['datetime'].values.tolist()]\n",
    "    \n",
    "    df_sensor_data = df_sensor_data[(df_sensor_data['name'] == hive_sn) & (df_sensor_data['datetime'] > start_time) & (df_sensor_data['datetime'] < end_time)]\n",
    "    df_sensor_data.set_index('datetime', inplace=True)\n",
    "    print(f\"got {df_sensor_data[sensor_column_name].count()} of {sensor_column_name} samples\")\n",
    "    \n",
    "    return df_sensor_data\n",
    "\n",
    "def merge_dataframes_ontimestamp(df_merge_to, *args):\n",
    "    \"\"\" Merging dataframes to df_merge_to \"\"\"\n",
    "    df_hive_data_ua = df_merge_to\n",
    "    for dataframe in args:\n",
    "        df_hive_data_ua = pd.merge(df_hive_data_ua, dataframe.reindex(df_hive_data_ua.index, method='nearest'), on=['datetime', 'name'])\n",
    "        \n",
    "    return df_hive_data_ua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def flatten(x):\n",
    "    if isinstance(x, collections.abc.Iterable):\n",
    "        return [a for i in x for a in flatten(i)]\n",
    "    else:\n",
    "        return [x]\n",
    "    \n",
    "def merge_columns(dataframe, column_names):\n",
    "    \"\"\" Function for merging columns with irregular size \"\"\"\n",
    "    return [flatten(val) for val in  dataframe[column_names].values.tolist()]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for performing grid search on best OneClasSVM on day/night classification and visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "\n",
    "  \n",
    "def plot_hour_shift(*args, xticklabels):\n",
    "    \"\"\" Function for plotting n-hour shift \"\"\"\n",
    "    fig, axs  = plt.subplots(len(args[0])//2, 2, figsize=(10,8))\n",
    "    fig.subplots_adjust(hspace=0.7)\n",
    "    \n",
    "    colors = ['ro', 'bx', 'go', 'yx', 'ko']\n",
    "    \n",
    "    if len(args) > len(colors):\n",
    "        print('warning your accuracies are bigger than colors for plot!')\n",
    "    \n",
    "    for feature_idx, accuracy in enumerate(args):\n",
    "        for acc_idx, acc_in_shift in enumerate(accuracy):\n",
    "            axs[acc_idx//2][acc_idx%2].plot(acc_in_shift, colors[feature_idx])\n",
    "            axs[acc_idx//2][acc_idx%2].grid()\n",
    "            axs[acc_idx//2][acc_idx%2].set_xticks(np.arange(0, len(xticklabels), 1))\n",
    "            axs[acc_idx//2][acc_idx%2].tick_params(axis='x', rotation=270)\n",
    "            axs[acc_idx//2][acc_idx%2].set_xticklabels(xticklabels)\n",
    "            axs[acc_idx//2][acc_idx%2].set_title(f'{acc_idx+1} hour long bee-night')\n",
    "            axs[acc_idx//2][acc_idx%2].set_ylabel('SVM accuracy')\n",
    "            axs[acc_idx//2][acc_idx%2].set_xlabel('Hour')\n",
    "    fig.show()\n",
    "\n",
    "def search_best_night_day(input_data, feature_name, days_as_test, start_hours, max_shift, verbose=0):\n",
    "    \"\"\" Function performing One-class SVM\n",
    "    \n",
    "        attribute: train_data - pandas series dataframe\n",
    "        attribute: feature_name - name of column from dataframe which will be used as feature\n",
    "        attribute: days_test - number of last days which will be used to create train data\n",
    "        attribute: start_hours - list with start hours\n",
    "        attribute: max_shift - max shift in hours\n",
    "    \"\"\"\n",
    "    max_accuracy = 0\n",
    "    \n",
    "    accs_per_shift = []\n",
    "    final_accs = []\n",
    "\n",
    "    for shift in range(1, max_shift+1):\n",
    "        for start_hour in start_hours:\n",
    "            data_to_svm = pd.DataFrame(input_data)\n",
    "            data_to_svm.sort_index(inplace=True)\n",
    "            \n",
    "            end_hour = (start_hour + shift) % 24\n",
    "            if end_hour > 12 or start_hour < max_shift:\n",
    "                data_to_svm['is_night'] = (data_to_svm.index.hour >= start_hour) & (data_to_svm.index.hour <= end_hour)\n",
    "            else:\n",
    "                data_to_svm['is_night'] = (data_to_svm.index.hour >= start_hour) | (data_to_svm.index.hour <= end_hour)\n",
    "                \n",
    "            samples_in_day = data_to_svm[data_to_svm.index < (data_to_svm.index[0] + timedelta(days=1))].count()\n",
    "            data_test = data_to_svm.tail(samples_in_day[0]*days_as_test)\n",
    "            data_train = data_to_svm[~data_to_svm.isin(data_test)].dropna(how='all')\n",
    "            \n",
    "            train_data = data_train[feature_name].values.tolist()\n",
    "            train_labels = data_train['is_night'].values.tolist()\n",
    "            test_data = data_test[feature_name].values.tolist()\n",
    "            test_labels = data_test['is_night'].values.tolist()\n",
    "            \n",
    "            if verbose > 0:\n",
    "                print(f'learning with train data size: {len(train_data)} and test data size: {len(test_data)}')\n",
    "                print(f'number of nights in train/test data: {sum(train_labels)}/{sum(test_labels)}')\n",
    "            svc = SVC(kernel='rbf', class_weight='balanced', gamma='auto')\n",
    "            svc.fit(train_data, train_labels)\n",
    "            predicted = svc.predict(test_data)\n",
    "            \n",
    "            sum_correct = 0\n",
    "            for idx, label_predicted in enumerate(predicted):\n",
    "                if(label_predicted == int(test_labels[idx])):\n",
    "                    sum_correct += 1\n",
    "\n",
    "            accuracy = (sum_correct/len(test_labels)*100)\n",
    "            if accuracy > max_accuracy:\n",
    "                if verbose > 0:\n",
    "                    print(f'new max acuuracy for {start_hour} to {end_hour}, accuracy: {accuracy:.2f}')\n",
    "                max_accuracy = accuracy\n",
    "            \n",
    "            if verbose > 0:\n",
    "                print(f'for night start at {start_hour} and end at {end_hour} got accuracy: {accuracy:.2f}')\n",
    "                print('==============================================================================')\n",
    "            \n",
    "            accs_per_shift.append(accuracy)\n",
    "        final_accs.append(accs_per_shift)\n",
    "        accs_per_shift = []\n",
    "        \n",
    "    return final_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for calculating distribution for specific feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def plot_distribution(distribution_dict, bin_size):\n",
    "    \"\"\" Plotting distribiution for dictionary elements\"\"\"\n",
    "    colors = ['blue', 'green', 'red', 'yellow', 'black', 'pink', 'purple']\n",
    "    rms_max = 0\n",
    "    rms_min = 65535\n",
    "    for k, v in rmses.items():\n",
    "        if np.max(v) > rms_max:\n",
    "            rms_max = np.max(v)\n",
    "        if np.min(v) < rms_min:\n",
    "            rms_min = np.min(v)\n",
    "        \n",
    "    plt.figure()\n",
    "    for idx, (k, v) in enumerate(distribution_dict.items()):\n",
    "        plt.hist(v, color=colors[idx%len(colors)], bins=int(np.abs(rms_max-rms_min)/bin_size))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of code for calculating autocorrelaction for specific feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "features = ['conv_ae', 'humidity', 'temperature',\n",
    "            'alcohol', 'aceton', 'jon-amonowy',\n",
    "            'toluen', 'co2', 'siarkowodor',\n",
    "            'metanotiol', 'trimetyloamina', 'wodor', 'co']\n",
    "\n",
    "feature = features[12]\n",
    "data_to_autocorr = df_hive_co_ua\n",
    "\n",
    "roll_len = 3\n",
    "interval = (data_to_autocorr.index[2] - data_to_autocorr.index[1]).seconds//60%60\n",
    "\n",
    "y2 = data_to_autocorr[feature].rolling(window=roll_len).mean().values\n",
    "y_corr = y2[roll_len:]\n",
    "x_corelation = np.arange(start=0, step=2, stop=150)\n",
    "\n",
    "fig, axes = plt.subplots(1, figsize=(8,5))\n",
    "x = plot_acf(y_corr, lags=x_corelation, zero=False, ax=axes)\n",
    "x_raw = acf(y_corr, nlags=150)\n",
    "axes.set_title(f'{feature} autocorrelaction')\n",
    "axes.set_xlabel(f'Lag (1 lag = {interval} minutes)') \n",
    "axes.set_ylabel('Correlation')\n",
    "axes.set_xticks(np.arange(0, 151, step=10))\n",
    "\n",
    "print(f'{feature} with max {max(x_raw[60:]):.2f} at {60 + np.argmax(x_raw[60:])}')\n",
    "\n",
    "# temperature with max 0.74 at 93 (15 mint)\n",
    "# humidity with max 0.58 at 92 (15 min)\n",
    "# alcohol with max 0.53 at 134 (10 min)\n",
    "# aceton with max 0.52 at 133 (10 min)\n",
    "# jon-amonowy with max 0.57 at 133 (10 min)\n",
    "# toluen with max 0.52 at 134 (10 min)\n",
    "# co2 with max 0.54 at 133 (10 min)\n",
    "# siarkowodor with max 0.16 at 142 (10 min)\n",
    "# metanotiol with max 0.34 at 140 (10 min)\n",
    "# trimetyloamina with max 0.56 at 138 (10 min)\n",
    "# wodor with max 0.14 at 142 (10 min)\n",
    "# co with max 0.62 at 134 (10 min)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
