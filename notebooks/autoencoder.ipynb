{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "hives_ids = [\"smrpiclient7\", \"smrpiclient6\", \"smrpiclient3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "def plot_spectrogram(frequency, time_x, spectrocgram, title):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.title(title)\n",
    "    plt.pcolormesh(time_x, frequency, spectrocgram)\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 1 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        # output layer (with sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class autoencoder_basic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder_basic, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1499, 512),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 1499),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load train data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound data preparation for hive: smrpiclient7 which has 3367 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3367/3367 [00:40<00:00, 82.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "Sound data preparation for hive: smrpiclient6 which has 3172 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3172/3172 [00:37<00:00, 85.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "Sound data preparation for hive: smrpiclient3 which has 602 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 602/602 [00:06<00:00, 86.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "Got 7141 sound samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "sound_time_ms = 2000\n",
    "# ~93 ms for fft window\n",
    "nfft = 4096\n",
    "# ~34% overlapping\n",
    "hop_len = (nfft//3) + 30\n",
    "# This can be manipulated to adjust number of bins for conv layer\n",
    "fmax = 2750\n",
    "\n",
    "hives_data = []\n",
    "max_to_norm = 0\n",
    "for idx, hive_id in enumerate(hives_ids):\n",
    "    sound_files = [f for f in glob.glob(f\"..\\\\measurements\\\\smartulav2\\\\{hive_id}_*\\\\*.wav\")]\n",
    "    print(f\"Sound data preparation for hive: {hive_id} which has {len(sound_files)} recordings...\", end=' ', flush=True)\n",
    "    for file in tqdm(sound_files):\n",
    "        sample_rate, sound_samples = wavfile.read(file)\n",
    "        sound_samples = sound_samples.T[0]/(2.0**31)\n",
    "    \n",
    "        spectrogram = librosa.core.stft(sound_samples, n_fft=nfft, hop_length=hop_len)\n",
    "        spectrogram_magnitude = np.abs(spectrogram)\n",
    "        spectrogram_phase = np.angle(spectrogram)\n",
    "        spectrogram_db = librosa.amplitude_to_db(spectrogram_magnitude, ref=np.max)\n",
    "        frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "        times = (np.arange(0, spectrogram_magnitude.shape[1])*hop_len)/sample_rate\n",
    "        \n",
    "        freq_slice = np.where((frequencies < fmax))\n",
    "        frequencies = frequencies[freq_slice]\n",
    "        spectrogram_db = spectrogram_db[freq_slice, :][0]    \n",
    "    \n",
    "        filename = file.rsplit('\\\\', 1)[-1]\n",
    "        timestamp = filename[filename.index('-')+1:].rsplit(\".wav\")[0]\n",
    "        datetime = datetime.strptime(timestamp, '%Y-%m-%dT%H-%M-%S')\n",
    "        hives_data.append([datetime, hive_id, sound_samples, [frequencies, times, spectrogram_db]])\n",
    "    print(\" done.\")\n",
    "\n",
    "print(f\"Got {len(hives_data)} sound samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for autoencoder train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random_idx = random.randint(0, len(hives_data) - 1)\n",
    "\n",
    "plot_spectrogram(hives_data[random_idx][3][0],\n",
    "                 hives_data[random_idx][3][1],\n",
    "                 hives_data[random_idx][3][2],\n",
    "                 f\"hive: {hives_data[random_idx][1]}, time: {hives_data[random_idx][0]}, idx: {random_idx}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train basic AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal as sig\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data as tdata\n",
    "\n",
    "# Divide data to test, validation and train\n",
    "train_stop_idx = int(pd_ae_data.shape[0]*90/100)\n",
    "\n",
    "pd_ae_data_train = pd_ae_data[:train_stop_idx]\n",
    "pd_ae_data_test = pd_ae_data[train_stop_idx:]\n",
    "\n",
    "print(f'Train data size: {pd_ae_data_train.shape[0]}')\n",
    "print(f'Test data size: {pd_ae_data_test.shape[0]}')\n",
    "\n",
    "tensor_train = torch.Tensor(pd_ae_data_train['periodogram'].values.tolist())\n",
    "tensor_test = torch.Tensor(pd_ae_data_test['periodogram'].values.tolist())\n",
    "\n",
    "train_dataset = tdata.TensorDataset(tensor_train)\n",
    "test_dataset = tdata.TensorDataset(tensor_test)\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = autoencoder_basic().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    for data in dataloader:\n",
    "        periodogram = data[0].to(device)\n",
    "        # ===================forward=====================\n",
    "        output = model(periodogram)\n",
    "        #train_loss = criterion(output, periodogram)\n",
    "        train_loss = F.binary_cross_entropy(output, periodogram)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += train_loss.item()\n",
    "    loss = loss / len(dataloader)\n",
    "    if (epoch+1) % (num_epochs/10) == 0:\n",
    "        print(f'epoch {epoch + 1}/{num_epochs}, loss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    loss_test = 0\n",
    "    for data in dataloader_test:\n",
    "        periodograms_test = data[0].to(device)\n",
    "        output = model(periodograms_test)\n",
    "        for idx, i in enumerate(output):\n",
    "            #loss_test += nn.MSELoss()(periodograms_test[idx], i)\n",
    "            loss_test += F.binary_cross_entropy(periodograms_test[idx], i)\n",
    "\n",
    "loss_test = loss_test/len(pd_ae_data_test)\n",
    "print(f'Final test loss: {loss_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CONV AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got dataset of size: 7141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "spectrogram_ae_data = [hive_data[3][2] for hive_data in hives_data]\n",
    "standarized_ae_data = [sc.fit_transform(spec.T).T for spec in spectrogram_ae_data]\n",
    "scaled_ae_data = [mm.fit_transform(stan.T).T for stan in standarized_ae_data]\n",
    "\n",
    "print(f\"Got dataset of size: {len(scaled_ae_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: torch.Size([7141, 256, 64])\n",
      "Train set size: 5712\n",
      "Test set size: 714\n",
      "Validation set size: 715\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.utils import data as tdata\n",
    "\n",
    "train_data_size = len(scaled_ae_data)*80//100\n",
    "test_data_size = (len(scaled_ae_data) - train_data_size) // 2\n",
    "val_data_size = len(scaled_ae_data) - train_data_size - test_data_size\n",
    "\n",
    "dataset_tensor = torch.Tensor(scaled_ae_data)\n",
    "print(f\"Dataset shape: {dataset_tensor.shape}\")\n",
    "print(f\"Train set size: {train_data_size}\")\n",
    "print(f\"Test set size: {test_data_size}\")\n",
    "print(f\"Validation set size: {val_data_size}\")\n",
    "\n",
    "# add one extra dimension as it is required for conv layer\n",
    "dataset_tensor = dataset_tensor[:, None, :, :] \n",
    "dataset = tdata.TensorDataset(dataset_tensor)\n",
    "train_set, test_set, val_set = torch.utils.data.random_split(dataset, [train_data_size, test_data_size, val_data_size])\n",
    "\n",
    "dataloader_train = tdata.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "dataloader_test = tdata.DataLoader(test_set, batch_size=32, shuffle=True)\n",
    "dataloader_val = tdata.DataLoader(val_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "modelConvAE = ConvAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(modelConvAE.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/50] train_loss: 0.10458 valid_loss: 0.03559\n",
      "[ 2/50] train_loss: 0.03472 valid_loss: 0.03722\n",
      "[ 3/50] train_loss: 0.03350 valid_loss: 0.03620\n",
      "[ 4/50] train_loss: 0.03220 valid_loss: 0.03345\n",
      "[ 5/50] train_loss: 0.03072 valid_loss: 0.03188\n",
      "[ 6/50] train_loss: 0.02984 valid_loss: 0.02684\n",
      "[ 7/50] train_loss: 0.02934 valid_loss: 0.02951\n",
      "[ 8/50] train_loss: 0.02898 valid_loss: 0.02875\n",
      "[ 9/50] train_loss: 0.02872 valid_loss: 0.02679\n",
      "[10/50] train_loss: 0.02822 valid_loss: 0.02948\n",
      "[11/50] train_loss: 0.02701 valid_loss: 0.02636\n",
      "[12/50] train_loss: 0.02668 valid_loss: 0.02404\n",
      "[13/50] train_loss: 0.02650 valid_loss: 0.02652\n",
      "[14/50] train_loss: 0.02634 valid_loss: 0.02483\n",
      "[15/50] train_loss: 0.02621 valid_loss: 0.02849\n",
      "[16/50] train_loss: 0.02606 valid_loss: 0.02200\n",
      "[17/50] train_loss: 0.02594 valid_loss: 0.02921\n",
      "[18/50] train_loss: 0.02581 valid_loss: 0.02512\n",
      "[19/50] train_loss: 0.02572 valid_loss: 0.02785\n",
      "[20/50] train_loss: 0.02561 valid_loss: 0.02215\n",
      "[21/50] train_loss: 0.02554 valid_loss: 0.02776\n",
      "[22/50] train_loss: 0.02544 valid_loss: 0.02279\n",
      "[23/50] train_loss: 0.02538 valid_loss: 0.02629\n",
      "[24/50] train_loss: 0.02531 valid_loss: 0.02207\n",
      "[25/50] train_loss: 0.02526 valid_loss: 0.02508\n",
      "[26/50] train_loss: 0.02520 valid_loss: 0.01878\n",
      "[27/50] train_loss: 0.02518 valid_loss: 0.02849\n",
      "[28/50] train_loss: 0.02514 valid_loss: 0.02598\n",
      "[29/50] train_loss: 0.02512 valid_loss: 0.02826\n",
      "[30/50] train_loss: 0.02508 valid_loss: 0.02417\n",
      "[31/50] train_loss: 0.02505 valid_loss: 0.02288\n",
      "[32/50] train_loss: 0.02503 valid_loss: 0.02506\n",
      "[33/50] train_loss: 0.02501 valid_loss: 0.02428\n",
      "[34/50] train_loss: 0.02499 valid_loss: 0.02581\n",
      "[35/50] train_loss: 0.02497 valid_loss: 0.02315\n",
      "[36/50] train_loss: 0.02496 valid_loss: 0.02486\n",
      "[37/50] train_loss: 0.02495 valid_loss: 0.02499\n",
      "[38/50] train_loss: 0.02492 valid_loss: 0.02426\n",
      "[39/50] train_loss: 0.02492 valid_loss: 0.02741\n",
      "[40/50] train_loss: 0.02491 valid_loss: 0.02732\n",
      "[41/50] train_loss: 0.02488 valid_loss: 0.02428\n",
      "[42/50] train_loss: 0.02488 valid_loss: 0.02660\n",
      "[43/50] train_loss: 0.02485 valid_loss: 0.02240\n",
      "[44/50] train_loss: 0.02484 valid_loss: 0.02378\n",
      "[45/50] train_loss: 0.02484 valid_loss: 0.02643\n",
      "[46/50] train_loss: 0.02482 valid_loss: 0.02506\n",
      "[47/50] train_loss: 0.02481 valid_loss: 0.02364\n",
      "[48/50] train_loss: 0.02478 valid_loss: 0.01895\n",
      "[49/50] train_loss: 0.02479 valid_loss: 0.02647\n",
      "[50/50] train_loss: 0.02477 valid_loss: 0.02073\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd2665411ee4feaa133bae286136190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# monitor training loss per batch\n",
    "train_loss = []\n",
    "# monitor validation loss per batch\n",
    "val_loss = []\n",
    "# save avg train losses for early stopping visualization\n",
    "avg_train_loss = []\n",
    "# save avg train losses for early stopping visualization\n",
    "avg_val_loss = [] \n",
    "    \n",
    "for epoch in range(1, num_epochs+1):    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    modelConvAE.train()\n",
    "    for data in dataloader_train:\n",
    "        # transfer data to device\n",
    "        periodogram = data[0].to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = modelConvAE(periodogram)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, periodogram)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "    ###################\n",
    "    # val the model   #\n",
    "    ###################\n",
    "    modelConvAE.eval()\n",
    "    for val_data in dataloader_val:\n",
    "        # transfer data to device\n",
    "        periodogram = data[0].to(device)\n",
    "        # forward pass\n",
    "        outputs = modelConvAE(periodogram)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, periodogram)\n",
    "        # update running val loss\n",
    "        val_loss.append(loss.item())\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = np.average(train_loss)\n",
    "    val_loss = np.average(val_loss)\n",
    "    avg_train_loss.append(train_loss)\n",
    "    avg_val_loss.append(val_loss)\n",
    "    \n",
    "    epoch_len = len(str(num_epochs))\n",
    "    # print avg training statistics \n",
    "    print(f'[{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] train_loss: {train_loss:.5f} valid_loss: {val_loss:.5f}')\n",
    "    \n",
    "    # clear batch losses\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(6,8))\n",
    "plt.plot(avg_train_loss, 'r', label=\"train loss\")\n",
    "plt.plot(avg_val_loss, 'b', label=\"validation loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    loss_test = 0\n",
    "    for data in dataloader_test:\n",
    "        periodograms_test = data[0].to(device)\n",
    "        output = modelConvAE(periodograms_test)\n",
    "        for idx, output_elem in enumerate(output):\n",
    "            loss_test += criterion(periodograms_test[idx], output_elem)\n",
    "\n",
    "loss_test = loss_test/len(scaled_ae_data)\n",
    "print(f'Final test loss: {loss_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot and visualize (MFCC vs AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    encode_data = pd_ae_data['periodogram'].values.tolist()\n",
    "    encode_data_tensor = torch.Tensor(encode_data).to(device)\n",
    "    output = [model.encoder(encode_data_tensor).cpu().numpy()][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "idx = random.randint(0, len(hives_data) - 1)\n",
    "with torch.no_grad():\n",
    "    fig, axs = plt.subplots(2, 1)\n",
    "    frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "    freq_slice = np.where((frequencies < fmax))\n",
    "    frequencies = frequencies[freq_slice]\n",
    "    times = (np.arange(0, spectrogram_magnitude.shape[1])*hop_len)/sample_rate    \n",
    "    \n",
    "    elem = scaled_ae_data[idx]\n",
    "    elem = elem[None, None,: ,:]\n",
    "    elem = torch.Tensor(elem)\n",
    "\n",
    "    axs[0].pcolormesh(times, frequencies, scaled_ae_data[idx])\n",
    "    axs[1].pcolormesh(times, frequencies, modelConvAE(elem.to(device)).cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction - now we perform t-SNE and PCA to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "reduced_ae_tsne = TSNE(n_components=2, perplexity=100, learning_rate=500, verbose=1).fit_transform(output)\n",
    "reduced_mfcc_tsne = TSNE(n_components=2, perplexity=100, learning_rate=500, verbose=1).fit_transform(pd_ae_data['mfcc'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mfccs_standarized = StandardScaler().fit_transform(pd_ae_data['mfcc'].values.tolist())\n",
    "ae_standarized = StandardScaler().fit_transform(output)\n",
    "reduced_ae_pca = PCA(n_components = 2).fit_transform(ae_standarized)\n",
    "reduced_mfcc_pca = PCA(n_components = 2).fit_transform(mfccs_standarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA().fit(ae_standarized)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "\n",
    "RECORD_TIME = 2\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "data = hives_data[-10][2][:, 0]/(2.0**31)\n",
    "datetime = hives_data[-1][0]\n",
    "fft_data = abs(fft(data))\n",
    "freqs = fftfreq(int(len(fft_data)/2), 1/SAMPLE_RATE)\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.tight_layout(pad=3.0)\n",
    "axs[0].set_title(f\"Sound recording at {datetime} ({RECORD_TIME}s)\")\n",
    "axs[0].grid()\n",
    "axs[0].set_xlabel('Time [sec]')\n",
    "axs[0].plot(np.linspace(0, 2, len(data)), data)\n",
    "\n",
    "axs[1].set_title(\"Periodogram\")\n",
    "axs[1].set_xticks(np.arange(0, (freqs.size/2), step=100))\n",
    "axs[1].set_xticklabels(np.arange(0, (freqs.size/2), step=100, dtype=int), rotation=45)\n",
    "axs[1].grid()\n",
    "axs[1].set_xlabel('Frequency [Hz]')\n",
    "axs[1].plot(freqs[1:1500], fft_data[1:1500], 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
