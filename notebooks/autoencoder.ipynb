{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<module 'data.indice.compute_indice' from 'c:\\\\Users\\\\tymot\\\\projects\\\\001.smartula\\\\smartula-analysis\\\\smartula-analysis\\\\notebooks\\\\data\\\\indice\\\\compute_indice.py'>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "\n",
    "import utils.periodogram_dataset_keras as pdk\n",
    "import utils.contrastive_keras as ck\n",
    "import utils.data_utils as du\n",
    "import importlib\n",
    "import data.spectrogram_dataset as sd\n",
    "import data.melspectrogram_dataset as md\n",
    "import data.periodogram_dataset as pd\n",
    "import data.mfcc_dataset as mfcd\n",
    "import data.sound as sound\n",
    "import data.indice.sound_indicies_dataset as si\n",
    "import data.indice.compute_indice as compute_indice\n",
    "import data.double_feature_dataset as dfd\n",
    "import utils.pytorch_impl.cvae as ccc\n",
    "import utils.pytorch_impl.conv_cvae as conv_ccc\n",
    "import utils.pytorch_impl.vae as vae\n",
    "import utils.pytorch_impl.conv_vae as conv_vae\n",
    "import utils.pytorch_impl.discriminator as discriminator\n",
    "\n",
    "importlib.reload(vae)\n",
    "importlib.reload(discriminator)\n",
    "importlib.reload(conv_vae)\n",
    "importlib.reload(ccc)\n",
    "importlib.reload(conv_ccc)\n",
    "importlib.reload(pd)\n",
    "importlib.reload(md)\n",
    "importlib.reload(sound)\n",
    "importlib.reload(mfcd)\n",
    "importlib.reload(du)\n",
    "importlib.reload(si)\n",
    "importlib.reload(sd)\n",
    "importlib.reload(dfd)\n",
    "importlib.reload(compute_indice)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "got 39678 sound filenames read for 5 files/folders\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from utils.data_utils import create_valid_sounds_datalist, get_valid_sounds_datalist\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CHECK_FILES = False                             # should we perform sound checking (long)\n",
    "\n",
    "root_folder = \"..\\\\measurements\\\\smartulav2\"    # root folder with folder containing sounds\n",
    "valid_filename = \"valid-files.txt\"              # filename for file which will be contain\n",
    "                                                # filenames for valid recordings\n",
    "checked_folders = []\n",
    "if CHECK_FILES:\n",
    "    checked_folders = create_valid_sounds_datalist(root_folder, valid_filename, \"smrpiclient\")\n",
    "else:\n",
    "     checked_folders = [os.path.join(root_folder, \"smrpiclient0_10082020-19012021\"),       \n",
    "                        os.path.join(root_folder, \"smrpiclient3_10082020-19012021\"), \n",
    "                        os.path.join(root_folder, \"smrpiclient5_10082020-19012021\"),    \n",
    "                        os.path.join(root_folder, \"smrpiclient6_10082020-19012021\"),\n",
    "                        os.path.join(root_folder, \"smrpiclient7_10082020-19012021\")]\n",
    "\n",
    "sound_filenames = get_valid_sounds_datalist(checked_folders, valid_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoder\n",
    "\n",
    "1. Convolutional version - dedicated to mel spectrograms and spectrograms\n",
    "2. Simple Autoencoder - fetures extracted from stationary signals and periodograms itself?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preparing train dataset with len: 24952...\n",
      "VAE model training performed on cuda\n",
      "-> training at epoch 1\n",
      "  0%|          | 0/1326 [00:08<?, ?it/s]\n",
      "  0%|          | 0/1326 [00:08<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-06b224289f3d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# train model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     modelVAE = train_vae_model(model, learning_rate=lr, weight_decay=weight_decay, num_epochs=epochs, patience=100, \\\n\u001b[0m\u001b[0;32m     59\u001b[0m                                   dataloader_train=train_loader, dataloader_val=val_loader, scale=BATCH_SCALE, checkpoint_name=PATH)\n\u001b[0;32m     60\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tymot\\projects\\001.smartula\\smartula-analysis\\smartula-analysis\\notebooks\\utils\\pytorch_impl\\vae.py\u001b[0m in \u001b[0;36mtrain_vae_model\u001b[1;34m(model, learning_rate, weight_decay, num_epochs, patience, dataloader_train, dataloader_val, scale, checkpoint_name)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m                     \u001b[1;31m# scale data accross batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.periodogram_dataset import PeriodogramDataset\n",
    "from data.spectrogram_dataset import SpectrogramDataset\n",
    "from data.melspectrogram_dataset import MelSpectrogramDataset\n",
    "from data.mfcc_dataset import MfccDataset\n",
    "\n",
    "from utils.pytorch_impl.vae import VAE, train_vae_model\n",
    "from utils.data_utils import filter_strlist\n",
    "from utils.pytorch_impl.conv_vae import ConvolutionalVAE\n",
    "from utils.pytorch_impl.cvae import cVAE\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "PATH = 'models\\\\vae-apprirary-approach-10022021.pth'\n",
    "TRAIN = True\n",
    "BATCH_SCALE = True\n",
    "\n",
    "# model for \n",
    "modelVAE = VAE(encoder_layer_sizes=[2048, 512, 64], latent_size=16, decoder_layer_sizes=[64, 512, 2048]).to(device)\n",
    "# model for mfcc \n",
    "mfccVAE = VAE(encoder_layer_sizes=[64, 32], latent_size=4, decoder_layer_sizes=[32, 64]).to(device)\n",
    "# model for spectrogram\n",
    "convVAE = ConvolutionalVAE(encoder_conv_sizes=[128, 64, 32, 16], encoder_mlp_sizes=[1024, 512, 128],\n",
    "                            decoder_conv_sizes=[16, 32, 64, 128], decoder_mlp_sizes=[128, 512, 1024], latent_size=16).to(device)\n",
    "# model for melspectrogram\n",
    "melsConvVAE = ConvolutionalVAE(encoder_conv_sizes=[128, 64, 32, 16], encoder_mlp_sizes=[256, 128],\n",
    "                            decoder_conv_sizes=[16, 32, 64, 128], decoder_mlp_sizes=[128, 256], latent_size=16).to(device)\n",
    "\n",
    "if TRAIN:\n",
    "    # training hyperparameters\n",
    "    batch_size = 16\n",
    "    epochs = 10\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-5\n",
    "\n",
    "    # prepare train data for complete dataset\n",
    "    train_hives_soundfiles = filter_strlist(sound_filenames, \"smrpiclient6\", \"smrpiclient7\")\n",
    "    print(f\"preparing train dataset with len: {len(train_hives_soundfiles)}...\")\n",
    "\n",
    "    # --- HERE CHOOSE CORRECT DATASET AS FEATURE --- #\n",
    "    # dataset = PeriodogramDataset(train_hives_soundfiles, [\"smrpiclient6\", \"smrpiclient7\"], slice_freq=(0, 2048))    \n",
    "    dataset = SpectrogramDataset(train_hives_soundfiles, [\"smrpiclient6\", \"smrpiclient7\"], nfft=4096, hop_len=(4096//3)+30, fmax=2750)\n",
    "    # dataset = MelSpectrogramDataset(train_hives_soundfiles, [\"smrpiclient6\", \"smrpiclient7\"], nfft=4096, hop_len=(4096//3)+30, mels=64)\n",
    "    # dataset = MfccDataset(train_hives_soundfiles, [\"smrpiclient6\", \"smrpiclient7\"], nfft=4096, hop_len=(4096//3)+30, mels=64)\n",
    "\n",
    "    # --- HERE COOSE CORRECT MODEL --- #\n",
    "    model = convVAE\n",
    "\n",
    "    val_amount = int(dataset.__len__() * 0.15)\n",
    "    train_set, val_set = random_split(dataset, [(dataset.__len__() - val_amount), val_amount])\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    # train model\n",
    "    modelVAE = train_vae_model(model, learning_rate=lr, weight_decay=weight_decay, num_epochs=epochs, patience=100, \\\n",
    "                                  dataloader_train=train_loader, dataloader_val=val_loader, scale=BATCH_SCALE, checkpoint_name=PATH)\n",
    "else:\n",
    "    if os.path.isfile(PATH) and os.access(PATH, os.R_OK):\n",
    "        modelVAE.load_state_dict(torch.load(PATH))\n",
    "        print('model load success!')\n",
    "    else:\n",
    "        print(f'ERROR! There is no file: {PATH}')"
   ]
  },
  {
   "source": [
    "### - inference -\n",
    "here we perform inference to see how our vae models handle data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.spectrogram_dataset import SpectrogramDataset\n",
    "from data.melspectrogram_dataset import MelSpectrogramDataset\n",
    "\n",
    "inference_hives = filter_strlist(sound_filenames, \"smrpiclient6\")\n",
    "idx = random.randrange(len(inference_hives))\n",
    "\n",
    "spec_dataset = SpectrogramDataset(inference_hives, [\"smrpiclient6\"], nfft=4096, hop_len=(4096//3)+30, fmax=2750)\n",
    "melspec_dataset = MelSpectrogramDataset(inference_hives, [\"smrpiclient6\"], nfft=4096, hop_len=(4096//3)+30, mels=64)\n",
    "\n",
    "(spectrogram_db, freqs, times), _ = spec_dataset.sample(idx)\n",
    "melspectrogram_db, _ = melspec_dataset.sample(idx)\n",
    "\n",
    "with torch.no_grad():\n",
    "    modelVAE.eval()\n",
    "    \n",
    "    fig, axs = plt.subplots(2,2, figsize=(16, 8))\n",
    "\n",
    "    axs[0][0].set_title('orginal spectrogram')\n",
    "    axs[0][0].pcolormesh(times, freqs, spectrogram_db.squeeze())\n",
    "    axs[0][1].set_title('reconstructed spectrogram (convVAE)')\n",
    "    axs[0][1].pcolormesh(times, freqs, modelVAE(spectrogram_db)[0].cpu().squeeze())\n",
    "    \n",
    "    axs[1][0].set_title('original mel-spectrogram')\n",
    "    axs[1][0].imshow(melspectrogram_db.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Variational Autoencoder\n",
    "\n",
    "-- keras implementation --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random \n",
    "\n",
    "from utils.periodogram_dataset_keras import PeriodogramGenerator\n",
    "from utils.contrastive_keras import contrastive_keras_vae\n",
    "from utils.data_utils import filter_strlist\n",
    "from tensorflow import keras\n",
    "\n",
    "bsize = 64\n",
    "TRAIN = True\n",
    "MIXED = True\n",
    "\n",
    "# background_hives = [\"smrpiclient6\", \"smrpiclient5\"]\n",
    "# target_hives = [\"smrpiclient7\"]\n",
    "\n",
    "cvae_keras, _, z_encoder, s_encoder, sb_encoder, cvae_decoder = contrastive_keras_vae(input_dim=2048, intermediate_dim=[512, 64], latent_dim=16,\n",
    "                                                                                        disentangle=True, gamma=1, batch_size=bsize)\n",
    "\n",
    "if TRAIN:        \n",
    "    # prepare train data for complete dataset\n",
    "    target_soundfilenames = filter_strlist(sound_filenames, \"smrpiclient5\")\n",
    "    background_soundfilenames = random.sample(filter_strlist(sound_filenames, \"smrpiclient6\", \"smrpiclient7\"), len(target_soundfilenames))\n",
    "    print(f\"preparing target dataset with len: {len(target_soundfilenames)} and background with len: {len(background_soundfilenames)}\")\n",
    "            \n",
    "    # Prepare dataset and train\n",
    "    train_generator = PeriodogramGenerator(target_soundfilenames, background_soundfilenames, batch_size=bsize, labels=[\"smrpiclient5\"], slice_freq=(0, 2048))\n",
    "    history = cvae_keras.fit(train_generator, epochs=10, batch_size=bsize, verbose=1)\n",
    "\n",
    "    cvae_keras.save_weights('models/cvae_keras_weights_10022021.h5')\n",
    "    sb_encoder.save_weights('models/sb_encoder_weights_10022021.h5')\n",
    "    cvae_decoder.save_weights('models/cvae_decoder_weights_10022021.h5')\n",
    "else:\n",
    "    cvae_keras.load_weights('models/cvae_keras_weights_10022021.h5')\n",
    "    sb_encoder.load_weights('models/sb_encoder_weights_10022021.h5')\n",
    "    cvae_decoder.load_weights('models/cvae_decoder_weights_10022021.h5')\n",
    "    print(f'model loaded successfully')\n"
   ]
  },
  {
   "source": [
    "-- pytorch implementation --"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy import signal\n",
    "from data.sound import read_samples\n",
    "\n",
    "W = signal.get_window('hanning', 1024, fftbins=False)\n",
    "samples, sampling_rate = read_samples(target_soundfilenames[0], raw=True)\n",
    "\n",
    "time_shift = int(1024/2)\n",
    "times = range(time_shift, len(samples)+1-time_shift, 1024) # centered\n",
    "frames = [samples[i-time_shift:i+time_shift]*W for i in times] # centered frames\n",
    "\n",
    "windowed_signal = np.array([frame*W for frame in frames])\n",
    "print(windowed_signal.shape)\n"
   ]
  },
  {
   "source": [
    "-- PyTorch implementation --"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [128, 1, 3, 3], but got 5-dimensional input of size [2, 4, 1, 256, 64] instead",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3add455f0356>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m conv_vae_model = ConvolutionalVAE(encoder_conv_sizes=[128, 64, 32, 16], encoder_mlp_sizes=[1024, 512, 128],\n\u001b[0;32m      8\u001b[0m                                             decoder_conv_sizes=[16, 32, 64, 128], decoder_mlp_sizes=[128, 512, 1024], latent_size=16)\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv_vae_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torchsummary\\torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m# make a forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m# print(x.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m# remove these hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tymot\\projects\\001.smartula\\smartula-analysis\\smartula-analysis\\notebooks\\utils\\pytorch_impl\\conv_vae.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mlatent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreparameterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_var\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mrecon_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlatent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tymot\\projects\\001.smartula\\smartula-analysis\\smartula-analysis\\notebooks\\utils\\pytorch_impl\\conv_vae.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight)\u001b[0m\n\u001b[0;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 419\u001b[1;33m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[0;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [128, 1, 3, 3], but got 5-dimensional input of size [2, 4, 1, 256, 64] instead"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "from utils.pytorch_impl.conv_vae import ConvolutionalVAE\n",
    "from utils.pytorch_impl.conv_cvae import ConvolutionalCVAE\n",
    "cvae_model = ConvolutionalCVAE(encoder_conv_sizes=[128, 64, 32, 16], encoder_mlp_sizes=[1024, 512, 128],\n",
    "                                            decoder_conv_sizes=[16, 32, 64, 128], decoder_mlp_sizes=[128, 512, 1024], latent_size=16)\n",
    "\n",
    "conv_vae_model = ConvolutionalVAE(encoder_conv_sizes=[128, 64, 32, 16], encoder_mlp_sizes=[1024, 512, 128],\n",
    "                                            decoder_conv_sizes=[16, 32, 64, 128], decoder_mlp_sizes=[128, 512, 1024], latent_size=16)\n",
    "summary(conv_vae_model, (4,1,256,64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "preparing target dataset with len: 10947 and background with len: 10947\n",
      "-> training at epoch 0\n",
      "  0%|          | 0/2327 [00:00<?, ?it/s]\n",
      "  0%|          | 0/2327 [00:00<?, ?it/s]torch.Size([4, 1, 256, 64])\n",
      "torch.Size([4, 1, 128, 128])/torch.Size([4, 1, 256, 64])\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 3",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-063a9cafbb4d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mcvae_parameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1e-7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1e-3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'epochs'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[0mdiscriminator_parameters\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1e-6\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'alpha'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mcvae\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_cvae\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcvae_parameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrastive_train_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontrastive_val_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator_parameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\tymot\\projects\\001.smartula\\smartula-analysis\\smartula-analysis\\notebooks\\utils\\pytorch_impl\\cvae.py\u001b[0m in \u001b[0;36mtrain_cvae\u001b[1;34m(model, model_params, dataloader_train, dataloader_val, disc, disc_params)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[0moutput_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcvae_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\tymot\\projects\\001.smartula\\smartula-analysis\\smartula-analysis\\notebooks\\utils\\pytorch_impl\\cvae.py\u001b[0m in \u001b[0;36mcvae_loss\u001b[1;34m(cvae_output, input_target, input_background, kld_weight, discriminator, discriminator_aplha)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[1;31m# MSE target\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcvae_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m     \u001b[1;31m# MSE background\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcvae_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'background'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_background\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   2657\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m     \u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\programs\\anaconda\\envs\\uni-bees\\lib\\site-packages\\torch\\functional.py\u001b[0m in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (128) must match the size of tensor b (64) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from utils.pytorch_impl.cvae import cVAE, train_cvae\n",
    "from utils.pytorch_impl.conv_cvae import ConvolutionalCVAE\n",
    "from utils.pytorch_impl.discriminator import Discriminator\n",
    "from utils.data_utils import filter_strlist\n",
    "\n",
    "from data.spectrogram_dataset import SpectrogramDataset\n",
    "from data.periodogram_dataset import PeriodogramDataset\n",
    "from data.double_feature_dataset import DoubleFeatureDataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "target_hives = [\"smrpiclient5\"]\n",
    "background_hives = [\"smrpiclient6\", \"smrpiclient7\"]\n",
    "\n",
    "target_soundfilenames = filter_strlist(sound_filenames, *target_hives)\n",
    "background_soundfilenames = random.sample(filter_strlist(sound_filenames, *background_hives), len(target_soundfilenames))\n",
    "print(f\"preparing target dataset with len: {len(target_soundfilenames)} and background with len: {len(background_soundfilenames)}\")\n",
    "\n",
    "# prepare data for contrastive FC variational autoencoder\n",
    "# contrastive_dataset = DoubleFeatureDataset(target_soundfilenames, target_hives, background_soundfilenames, background_hives,\n",
    "#                                                         PeriodogramDataset, scale_db=False, slice_freq=(0, 2048))\n",
    "# prepare data for contrastive FC variational autoencoder\n",
    "contrastive_dataset = DoubleFeatureDataset(target_soundfilenames, target_hives, background_soundfilenames, background_hives,\n",
    "                                                        SpectrogramDataset, nfft=4096, hop_len=(4096//3)+30, fmax=2750)\n",
    "\n",
    "val_amount = int(contrastive_dataset.__len__() * 0.15)\n",
    "train_set, val_set = random_split(contrastive_dataset, [(contrastive_dataset.__len__() - val_amount), val_amount])\n",
    "contrastive_train_dataloader = DataLoader(train_set, batch_size=4, shuffle=True)\n",
    "contrastive_val_dataloader = DataLoader(val_set, batch_size=4, shuffle=True)\n",
    "                                \n",
    "# model = cVAE(encoder_layer_sizes=[2048, 512, 64], latent_size=16, decoder_layer_sizes=[64, 512, 2048])\n",
    "model = ConvolutionalCVAE(encoder_conv_sizes=[128, 64, 32, 16], encoder_mlp_sizes=[1024, 512, 128],\n",
    "                                            decoder_conv_sizes=[16, 32, 64, 128], decoder_mlp_sizes=[128, 512, 1024], latent_size=16)\n",
    "discriminator = Discriminator([64, 128, 16], input_size=2*model.latent_size)\n",
    "\n",
    "cvae_parameters = { 'learning_rate': 1e-7, 'weight_decay': 1e-3, 'epochs': 10 }\n",
    "discriminator_parameters  = { 'learning_rate': 0.001, 'weight_decay': 1e-6, 'alpha': 0.01}\n",
    "cvae = train_cvae(model, cvae_parameters, contrastive_train_dataloader, contrastive_val_dataloader, discriminator, discriminator_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([8, 16])\ntorch.Size([8, 16])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([8, 32])"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "a = torch.rand(8, 16)\n",
    "b = torch.ones_like(a)\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "\n",
    "torch.cat((a, b), axis=1).shape"
   ]
  },
  {
   "source": [
    "-> Here we test generalization ability by testing MSE loss for unseen hive. We start with VAE and then contrastive Autoencoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils.periodogram_dataset import PeriodogramDataset\n",
    "from utils.data_utils import filter_strlist\n",
    "\n",
    "hut_filenames = filter_strlist(sound_filenames, \"smrpiclient3\")\n",
    "hut_dataset = PeriodogramDataset(hut_filenames, [\"smrpiclient3\"], slice_freq=(0, 2048))\n",
    "\n",
    "test_loss = []\n",
    "with torch.no_grad():\n",
    "    modelVAE.eval()\n",
    "    for input_data in tqdm(hut_dataset):\n",
    "        if input_data:\n",
    "            input_data = torch.Tensor(input_data[0]).to(device)\n",
    "            output, mean, var = modelVAE(input_data.to(device))\n",
    "            test_loss.append(F.mse_loss(output, input_data).cpu().numpy())\n",
    "\n",
    "test_loss_avg = np.mean(test_loss)\n",
    "\n",
    "print(f'\\r\\nVAE test loss: {test_loss_avg:.8f}')\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from operator import itemgetter \n",
    "from utils.periodogram_dataset_keras import PeriodogramGenerator, read_sound_file\n",
    "from tqdm import tqdm \n",
    "\n",
    "hut_sounds =  np.array([read_sound_file(f, (0, 2048)) for f in tqdm(hut_filenames)])\n",
    "mean, var, latent = sb_encoder.predict(hut_sounds)\n",
    "zeros = np.zeros_like(latent)\n",
    "decoder_input = np.concatenate((zeros, latent), axis=1)\n",
    "cvae_output = cvae_decoder.predict(decoder_input)\n",
    "loss = tf.keras.losses.mean_squared_error(hut_sounds, cvae_output)\n",
    "\n",
    "print(f'cVAE test loss:: {np.mean(loss)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_indices = np.random.randint(0, len(background_soundfilenames), len(hut_sounds))\n",
    "\n",
    "background_train_sound_files = list(itemgetter(*subset_indices)(background_soundfilenames))\n",
    "bg_sounds = np.array([read_sound_file(f, (0, 2048)) for f in tqdm(background_train_sound_files)])\n",
    "mean, var, latent = sb_encoder.predict(bg_sounds)\n",
    "zeros = np.zeros_like(latent)\n",
    "decoder_input = np.concatenate((zeros, latent), axis=1)\n",
    "cvae_output = cvae_decoder.predict(decoder_input)\n",
    "loss = tf.keras.losses.mean_squared_error(bg_sounds, cvae_output)\n",
    "\n",
    "print(f'MSE loss for background test: {np.mean(loss)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD ANALYSIS BELOW\n",
    "##  |\n",
    "##  |\n",
    "## \\/\n",
    "# Data Preprocessing\n",
    "\n",
    "Here we load train data sound samples and prepare spectrogram, periodogram and mfcc features (along with some data to visualize this). We should provide data with **utc timestamps** as it will be shifted with `timezone_offset_hours` var. What we also do is remove those samples which has strange rms signal. Threshold 0.8 was chosen based on `plot_distribution` output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import librosa\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "\n",
    "sound_time_ms = 2000\n",
    "# ~93 ms for fft window\n",
    "nfft = 4096\n",
    "# ~34% overlapping\n",
    "hop_len = (nfft//3) + 30\n",
    "# This can be manipulated to adjust number of bins for conv layer\n",
    "fmax = 2750\n",
    "\n",
    "hives_data = []\n",
    "rmses = {}\n",
    "max_to_norm = 0\n",
    "\n",
    "if DATA_INIT:\n",
    "    for idx, hive_id in enumerate(hives_ids):\n",
    "        sound_files = [f for f in glob.glob(f\"..\\\\measurements\\\\smartulav2\\\\{hive_id}_*\\\\*.wav\")]\n",
    "        print(f\"Sound data preparation for hive: {hive_id} which has {len(sound_files)} recordings...\", end=' ', flush=True)\n",
    "        for file in tqdm(sound_files):\n",
    "            sample_rate, sound_samples = wavfile.read(file)\n",
    "            sound_samples = sound_samples.T[0]/(2.0**31)\n",
    "            rms = np.sqrt(sum(sound_samples**2)/len(sound_samples))\n",
    "            if rms < 0.7:    # that threshold was observed from plot_distribution() function\n",
    "                # calculate timestamp\n",
    "                filename = file.rsplit('\\\\', 1)[-1]\n",
    "                utc_timestamp = filename[filename.index('-')+1:].rsplit(\".wav\")[0]\n",
    "                sound_datetime = datetime.strptime(utc_timestamp, '%Y-%m-%dT%H-%M-%S') + timedelta(hours=timezone_offset_hours)\n",
    "                \n",
    "                # calculate mfcc feature\n",
    "                mfccs = librosa.feature.mfcc(y=sound_samples, sr=sample_rate, n_fft=nfft, hop_length=hop_len, n_mfcc=13)\n",
    "                np_mfcc_avg = np.mean(mfccs, axis=1)\n",
    "                \n",
    "                # calculate spectrogram\n",
    "                spectrogram = librosa.core.stft(sound_samples, n_fft=nfft, hop_length=hop_len)\n",
    "                spectrogram_magnitude = np.abs(spectrogram)\n",
    "                spectrogram_phase = np.angle(spectrogram)\n",
    "                spectrogram_db = librosa.amplitude_to_db(spectrogram_magnitude, ref=np.max)\n",
    "                frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "                times = (np.arange(0, spectrogram_magnitude.shape[1])*hop_len)/sample_rate\n",
    "                freq_slice = np.where((frequencies < fmax))\n",
    "                frequencies = frequencies[freq_slice]\n",
    "                spectrogram_db = spectrogram_db[freq_slice, :][0]\n",
    "                spectrogram_mean = np.mean(spectrogram_db, axis=1)\n",
    "                # decimate?\n",
    "                # spectrogram_db_decimated = decimate(spectrogram_db.T, 4).T\n",
    "                # frequencies_decimated = decimate(frequencies, 4)\n",
    "\n",
    "                #calculate periodogram\n",
    "                periodogram = fft(sound_samples, n=sample_rate)\n",
    "                periodogram = abs(periodogram[1:int(len(periodogram)/2)])\n",
    "                periodogram_freq = fftfreq(len(sound_samples)//(sound_time_ms//1000), 1/sample_rate)\n",
    "                periodogram_freq = periodogram_freq[:(len(periodogram_freq)//2)-1]\n",
    "                \n",
    "                hives_data.append(\n",
    "                    {\n",
    "                        'datetime': sound_datetime,\n",
    "                        'id': hive_id,\n",
    "                        'samples': sound_samples,\n",
    "                        'freq':\n",
    "                            {\n",
    "                                'spectrogram':\n",
    "                                    {\n",
    "                                        'frequencies': frequencies,\n",
    "                                        'time': times,\n",
    "                                        'spectrogram_full_db': spectrogram_db,\n",
    "                                        'spectrogram_mean': spectrogram_mean\n",
    "                                    },\n",
    "                                'periodogram':\n",
    "                                    {\n",
    "                                        'frequencies': periodogram_freq,\n",
    "                                        'samples': periodogram\n",
    "                                    }\n",
    "                            },\n",
    "                        'features':\n",
    "                            {\n",
    "                                'mfcc_avg': np_mfcc_avg\n",
    "                            }\n",
    "                    }\n",
    "                )\n",
    "        print(\" done.\")\n",
    "        \n",
    "    print(\"saving data on disc...\", end=' ')\n",
    "    np.save('data/raw_hives_data.npy', hives_data, allow_pickle=True)\n",
    "    print(\"done.\")\n",
    "else:\n",
    "    hives_data = np.load('data/raw_hives_data.npy', allow_pickle=True)\n",
    "    \n",
    "print(f\"got full dataset of {len(hives_data)} sound samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data preprocessing \n",
    "\n",
    "Here we perform scaling standarization etc.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "sc = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "# extract every spectrogram from hives data list and standarize every periodogram from stft \n",
    "# so data for spectrogram will have zero mean and unit variance + scale every standarized periodogram\n",
    "print('preparing spectrograms...', end=' ', flush=True)\n",
    "spectrograms = [hive_data['freq']['spectrogram']['spectrogram_full_db'] for hive_data in hives_data]\n",
    "# standarized_spectrograms = [sc.fit_transform(spectrogram.T).T for spectrogram in tqdm(spectrograms)]\n",
    "scaled_spectrogram = [mm.fit_transform(spectrogram.T).T for spectrogram in tqdm(spectrograms, position=0, leave=True)]\n",
    "print('done.')\n",
    "\n",
    "# get datatime, names and mfcc\n",
    "print('getting mfccs', end=' ')\n",
    "datetimes = [hive_data['datetime'] for hive_data in hives_data]\n",
    "names = [hive_data['id'] for hive_data in hives_data]\n",
    "mfccs = [hive_data['features']['mfcc_avg'] for hive_data in hives_data]\n",
    "print('done.')\n",
    "\n",
    "# standarize and scale mean spectrogram for sounds\n",
    "print('preparing spectrogram means', end=' ')\n",
    "spectrograms_mean = [hive_data['freq']['spectrogram']['spectrogram_mean'] for hive_data in hives_data]\n",
    "standarized_spectrograms_mean = StandardScaler().fit_transform(spectrograms_mean)\n",
    "scaled_spectrograms_means = MinMaxScaler().fit_transform(standarized_spectrograms_mean)\n",
    "print('done.')\n",
    "\n",
    "# prepare truncated periodogram\n",
    "end_frequency = 2048\n",
    "print(f'preparing truncated periodograms ({end_frequency})', end=' ', flush=True)\n",
    "periodograms = [hive_data['freq']['periodogram']['samples'][:end_frequency] for hive_data in hives_data]\n",
    "periodograms = [mm.fit_transform(perio.reshape(-1, 1)).T for perio in tqdm(periodograms, position=0, leave=True)]\n",
    "print('done.')\n",
    "\n",
    "sounds = list(zip(scaled_spectrogram, mfccs, scaled_spectrograms_means, periodograms, datetimes, names))\n",
    "\n",
    "sounds_data = pd.DataFrame(sounds, columns=['spectrogram', 'mfccs', 'spectrogram_mean', 'periodogram', 'datetime', 'name'])\n",
    "sounds_data['datetime'] = pd.to_datetime(sounds_data['datetime'])\n",
    "sounds_hive_data = sounds_data[sounds_data['name'] == hive_under_analysis]\n",
    "\n",
    "print(f\"Got dataset of size: {len(sounds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CONTRASTIVE AE - keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sounds_data[sounds_data['name'] == 'smrpiclient7']['periodogram'].to_numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.contrastive_keras import contrastive_keras_vae\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "target_keras = np.stack(sounds_data[sounds_data['name'] == 'smrpiclient7']['periodogram'].to_numpy()[:3000]).squeeze()\n",
    "background_keras = np.stack(sounds_data[sounds_data['name'] == 'smrpiclient6']['periodogram'].to_numpy()[:3000]).squeeze()\n",
    "\n",
    "print(f'training cvae (keras) with target of shape: {target_keras.shape} '\n",
    "    f'and background of shape: {background_keras.shape}')\n",
    "\n",
    "assert(target_keras.shape == background_keras.shape)\n",
    "\n",
    "cvae_keras, cvae_fg, z_encoder_keras, s_encoder_keras, cvae_keras_decoder = contrastive_keras_vae(\n",
    "    input_dim=2048, intermediate_dim=512, latent_dim=16, disentangle=True, gamma=1)\n",
    "history = cvae_keras.fit([target_keras, background_keras], epochs=50, batch_size=100, \n",
    "                         validation_data=([target_keras, background_keras], None), verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Train BASIC AE\n",
    "Here we train basic fully connected autoencoder on data from particular hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch \n",
    "import os\n",
    "\n",
    "from torch.utils import data as tdata\n",
    "from utils.data_utils import prepare_dataset1d\n",
    "from utils.autoencoder_utils import BasicAutoencoder, train_model\n",
    "\n",
    "TRAIN_MODEL = False\n",
    "PATH = 'basic_ae.pth'\n",
    "\n",
    "modelBasicAE = BasicAutoencoder().to(device)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    train_dataloader, val_dataloader = prepare_dataset1d(sounds_hive_data['spectrogram_mean'], train_ratio=0.8, batch_size=64)\n",
    "    modelBasicAE = train_model(modelBasicAE,\n",
    "                           learning_rate=1e-3, weight_decay=1e-5, num_epochs=10, patience=20,\n",
    "                           dataloader_train=train_dataloader, dataloader_val=val_dataloader,\n",
    "                           checkpoint_name=PATH)\n",
    "else:\n",
    "    if os.path.isfile(PATH) and os.access(PATH, os.R_OK):\n",
    "        modelBasicAE.load_state_dict(torch.load(PATH))\n",
    "        print('model load success!')\n",
    "    else:\n",
    "        print(f'ERROR! There is no file: {PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CONV AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we train convolutional autoencoder on data from particular hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils import data as tdata\n",
    "from utils.data_utils import prepare_dataset2d\n",
    "from utils.autoencoder_utils import ConvAutoencoder, train_model\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "PATH = 'conv_ae.pth'\n",
    "\n",
    "modelConvAE = ConvAutoencoder().to(device)\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    train_set, val_set = prepare_dataset2d(sounds_hive_data['spectrogram'], train_ratio=0.8)\n",
    "    \n",
    "    dataloader_train = tdata.DataLoader(train_set, batch_size=6, shuffle=True)\n",
    "    dataloader_val = tdata.DataLoader(val_set, batch_size=6, shuffle=True)\n",
    "    \n",
    "    modelConvAE = train_model(modelConvAE,\n",
    "                               learning_rate=1e-3, weight_decay=1e-6, num_epochs=100, patience=20,\n",
    "                               dataloader_train=dataloader_train, dataloader_val=dataloader_val,\n",
    "                               checkpoint_name=PATH)\n",
    "else:\n",
    "    if os.path.isfile(PATH) and os.access(PATH, os.R_OK):\n",
    "        modelConvAE.load_state_dict(torch.load(PATH))\n",
    "        print('model load success!')\n",
    "    else:\n",
    "        print(f'ERROR! There is no such file: {PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train vae autoencoder on data from particular hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.utils import data as tdata\n",
    "from torchvision import transforms\n",
    "from utils.data_utils import prepare_dataset1d\n",
    "from utils.contrastive_pytorch import VAE, train_vae_model\n",
    "from utils.customdataset import CustomDataset\n",
    "\n",
    "TRAIN_MODEL = True\n",
    "PATH = 'vae.pth'\n",
    "\n",
    "modelVAE = VAE(encoder_layer_sizes=[2048, 512, 64], latent_size=16, decoder_layer_sizes=[64, 512, 2048])\n",
    "\n",
    "if TRAIN_MODEL:\n",
    "    data = torch.Tensor(sounds_hive_data['periodogram'])\n",
    "    data = data[:, None, :]\n",
    "    idx_split = data.size(0)*80//100\n",
    "    train_dataset = CustomDataset(data[:idx_split], should_scale=True)\n",
    "    val_dataset = CustomDataset(data[idx_split:], should_scale=True)\n",
    "    train_dataloader = tdata.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    val_dataloader = tdata.DataLoader(val_dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    modelVAE = train_vae_model(modelVAE, learning_rate=1e-3, weight_decay=1e-5, num_epochs=100, patience=100,\n",
    "                              dataloader_train=train_dataloader, dataloader_val=val_dataloader, checkpoint_name=PATH)\n",
    "else:\n",
    "    if os.path.isfile(PATH) and os.access(PATH, os.R_OK):\n",
    "        modelVAE.load_state_dict(torch.load(PATH))\n",
    "        print('model load success!')\n",
    "    else:\n",
    "        print(f'ERROR! There is no such file: {PATH}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "from scipy.io import wavfile\n",
    "\n",
    "# read just sample file as probably all has the same sampling rate\n",
    "sample_rate, samples = wavfile.read('C:\\\\Users\\\\tymot\\\\projects\\\\001.smartula\\\\smartula-analysis\\\\smartula-analysis'\n",
    "                           '\\\\measurements\\\\smartulav2\\\\smrpiclient6_10082020-15092020\\\\DEADBEEF94-2020-08-10T03-11-50.wav')\n",
    "idx = random.randint(0, sounds_hive_data.shape[0] - 1)\n",
    "idx = 2240\n",
    "with torch.no_grad():   \n",
    "    modelConvAE.eval()\n",
    "    modelBasicAE.eval()\n",
    "    modelVAE.eval()\n",
    "    \n",
    "    # Get sound from sounds hive data (with respect to hive under analysis)\n",
    "    sound = sounds_hive_data.iloc[idx]\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12,8))\n",
    "    fig.suptitle(f'Convolutional Autoencoder real vs output for sample id: {idx}')\n",
    "    fig.tight_layout(pad = 3.0)\n",
    "\n",
    "    frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "    freq_slice = np.where((frequencies < fmax))\n",
    "    frequencies = frequencies[freq_slice]\n",
    "    times = (np.arange(0, sound['spectrogram'].shape[1])*hop_len)/sample_rate\n",
    "    elem = sound['spectrogram']\n",
    "    elem = elem[None, None, :, :]\n",
    "    elem = torch.Tensor(elem).to(device)\n",
    "    \n",
    "    elem_mean = sound['spectrogram_mean']\n",
    "    elem_mean = elem_mean[None, :]\n",
    "    elem_mean = torch.Tensor(elem_mean).to(device)\n",
    "    \n",
    "    axs[0][0].set_title('real spectrogram')\n",
    "    axs[0][0].pcolormesh(times, frequencies, sound['spectrogram'])\n",
    "    axs[0][1].set_title('encoded conv2d')\n",
    "    axs[0][1].pcolormesh(times, frequencies, modelConvAE(elem.to(device)).cpu().numpy().squeeze())\n",
    "    \n",
    "    # prepare data for vae prediction\n",
    "    freqs = np.arange(sound['periodogram'].shape[1])\n",
    "    input_data = torch.Tensor(sound['periodogram']).to(device)\n",
    "    vae_output, _, _ = modelVAE(input_data)\n",
    "    \n",
    "    # prepare data for cvae prediction\n",
    "    s_mean, s_log_var, s = s_encoder_keras.predict(sound['periodogram'])\n",
    "    latent = np.concatenate((np.zeros_like(s), s), axis = 1)\n",
    "    cvae_output = cvae_keras_decoder.predict(latent)\n",
    "\n",
    "    axs[1][0].set_title('spectrogram mean')\n",
    "    axs[1][0].plot(sound['spectrogram_mean'], label='real', color='blue')\n",
    "    axs[1][0].plot(modelBasicAE(elem_mean).cpu().numpy().squeeze(), label='encoded', color='red')\n",
    "    axs[1][0].legend()\n",
    "    \n",
    "    axs[1][1].set_title('periodogram')\n",
    "    axs[1][1].plot(freqs, periodogram_df, label='real', color='blue')\n",
    "    axs[1][1].plot(freqs, vae_output.cpu().numpy().squeeze(), label='vae encoded', color='red')\n",
    "    axs[1][1].plot(freqs, cvae_output.squeeze(), label='cvae encoded', color='green')\n",
    "    axs[1][1].legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add temperature/humidity/gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import read_sensor_data\n",
    "\n",
    "start_time = '2020-08-10 00:00:00'\n",
    "end_time = '2020-09-16 00:00:00'\n",
    "print(f\"extracting data for hive under analysis: {hive_under_analysis} from {start_time} to {end_time}...\")\n",
    "\n",
    "df_hives_sound = pd.DataFrame(sounds_data)\n",
    "df_hive_sound_ua = df_hives_sound[(df_hives_sound['name'] == hive_under_analysis)\n",
    "                                 & (df_hives_sound['datetime'] > start_time)\n",
    "                                 & (df_hives_sound['datetime'] < end_time)]\n",
    "df_hive_sound_ua.set_index('datetime', inplace=True)\n",
    "print(f\"-> prepared base of {df_hive_sound_ua.count()['spectrogram']} number of sound spectrums <-\")\n",
    "\n",
    "df_hive_temperature_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-temperature.csv',\n",
    "                                          hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'temperature')\n",
    "df_hive_humidity_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-humidity.csv',\n",
    "                                       hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'humidity')\n",
    "df_hive_alcohol_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-alcohol.csv',\n",
    "                                      hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'alcohol')\n",
    "df_hive_aceton_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-aceton.csv',\n",
    "                                     hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'aceton')\n",
    "df_hive_amon_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-jon-amonowy.csv',\n",
    "                                   hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'jon-amonowy')\n",
    "df_hive_toluen_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-toluen.csv',\n",
    "                                     hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'toluen')\n",
    "df_hive_co2_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-co2.csv',\n",
    "                                    hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'co2')\n",
    "df_hive_siarkowodor_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-siarkowodor.csv',\n",
    "                                          hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'siarkowodor')\n",
    "df_hive_metanotiol_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-metanotiol.csv',\n",
    "                                         hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'metanotiol')\n",
    "df_hive_trimetyloamina_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-trimetyloamina.csv',\n",
    "                                         hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'trimetyloamina')\n",
    "df_hive_wodor_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-wodor.csv',\n",
    "                                    hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'wodor')\n",
    "df_hive_co_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-co.csv',\n",
    "                                 hive_under_analysis, hives_ids, start_time, end_time, timezone_offset_hours, 'co')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check autocorrelation for specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import merge_dataframes_ontimestamp, merge_columns\n",
    "from utils.autoencoder_utils import conv2d_encode, basic_ae_encode\n",
    "from utils.contrastive_pytorch import vae_encode\n",
    "from utils.contrastive_keras import cvae_encode\n",
    "\n",
    "df_hive_data = merge_dataframes_ontimestamp(df_hive_sound_ua,\n",
    "                                            df_hive_temperature_ua, df_hive_humidity_ua,\n",
    "                                            df_hive_alcohol_ua, df_hive_aceton_ua, df_hive_amon_ua, df_hive_toluen_ua, df_hive_co2_ua,\n",
    "                                            df_hive_siarkowodor_ua, df_hive_metanotiol_ua, df_hive_trimetyloamina_ua, df_hive_wodor_ua,\n",
    "                                            df_hive_co_ua)\n",
    "\n",
    "df_hive_data['conv_ae'] = conv2d_encode(modelConvAE, df_hive_data['spectrogram'].to_list())\n",
    "df_hive_data['basic_ae'] = basic_ae_encode(modelBasicAE, df_hive_data['spectrogram_mean'].to_list())\n",
    "df_hive_data['vae'] = vae_encode(modelVAE, df_hive_data['periodogram'].to_list()).tolist()\n",
    "df_hive_data['cvae_s'] = cvae_encode(s_encoder_keras, df_hive_data['periodogram']).tolist()\n",
    "df_hive_data['cvae_z'] = cvae_encode(z_encoder_keras, df_hive_data['periodogram']).tolist()\n",
    "\n",
    "df_hive_data['bae_feature_vector'] = merge_columns(df_hive_data, ['basic_ae', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['conv_feature_vector'] = merge_columns(df_hive_data, ['conv_ae', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['mfcc_feature_vector'] = merge_columns(df_hive_data, ['mfccs', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['vae_feature_vector'] = merge_columns(df_hive_data, ['vae', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['cvae_s_feature_vector'] = merge_columns(df_hive_data, ['cvae_s', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['cvae_z_feature_vector'] = merge_columns(df_hive_data, ['cvae_z', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from utils.data_utils import search_best_night_day\n",
    "\n",
    "start_hours = [20, 21, 22, 23, 0, 1, 2, 3, 4]\n",
    "\n",
    "df_hive_data_scaled = pd.DataFrame(df_hive_data)\n",
    "\n",
    "# data for convolutional autoencoder\n",
    "df_hive_data_scaled['conv_feature_vector'] = StandardScaler().fit_transform(df_hive_data['conv_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for basic autoencoder\n",
    "df_hive_data_scaled['bae_feature_vector'] = StandardScaler().fit_transform(df_hive_data['bae_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for mfcc features\n",
    "df_hive_data_scaled['mfcc_feature_vector'] = StandardScaler().fit_transform(df_hive_data['mfcc_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for vae features\n",
    "df_hive_data_scaled['vae_feature_vector'] = StandardScaler().fit_transform(df_hive_data['vae_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for cvae s features\n",
    "df_hive_data_scaled['cvae_s_feature_vector'] = StandardScaler().fit_transform(df_hive_data['cvae_s_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for cvae s features\n",
    "df_hive_data_scaled['cvae_z_feature_vector'] = StandardScaler().fit_transform(df_hive_data['cvae_z_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for plain mfcc \n",
    "mfccs = [hive_data['features']['mfcc_avg'] for hive_data in hives_data if hive_data['id'] == hive_under_analysis]\n",
    "mfccs = StandardScaler().fit_transform(mfccs)\n",
    "datetimes = [hive_data['datetime'] for hive_data in hives_data if hive_data['id'] == hive_under_analysis]\n",
    "mfccs_data = list(zip(datetimes, mfccs))\n",
    "pd_mfcc_data = pd.DataFrame(mfccs_data, columns=['datetime', 'mfcc'])\n",
    "pd_mfcc_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# calculate one class SVM match\n",
    "print('calculating mfccs match...', end=' ', flush=True)\n",
    "mfcc_accs = search_best_night_day(pd_mfcc_data, 'mfcc', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(mfcc_accs)}/{len(mfcc_accs[0])}')\n",
    "print('calculating conv ae feature vector match...', end=' ', flush=True)\n",
    "conv_ae_accs = search_best_night_day(df_hive_data_scaled, 'conv_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(conv_ae_accs)}/{len(conv_ae_accs[0])}')\n",
    "print('calculating basic ae feature vector match...', end=' ', flush=True)\n",
    "bae_accs = search_best_night_day(df_hive_data_scaled, 'bae_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(bae_accs)}/{len(bae_accs[0])}')\n",
    "print('calculating mfccs extended feature vector match...', end=' ', flush=True)\n",
    "mffce_accs = search_best_night_day(df_hive_data_scaled, 'mfcc_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(mffce_accs)}/{len(mffce_accs[0])}')\n",
    "print('calculating vae feature vector match...', end=' ', flush=True)\n",
    "vae_accs = search_best_night_day(df_hive_data_scaled, 'vae_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(vae_accs)}/{len(vae_accs[0])}')\n",
    "print('calculating cvae s feature vector match...', end=' ', flush=True)\n",
    "cvae_accs_s = search_best_night_day(df_hive_data_scaled, 'cvae_s_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(cvae_accs_s)}/{len(cvae_accs_s[0])}')\n",
    "print('calculating cvae z feature vector match...', end=' ', flush=True)\n",
    "cvae_accs_z = search_best_night_day(df_hive_data_scaled, 'cvae_z_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(cvae_accs_z)}/{len(cvae_accs_z[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.data_utils import plot_hour_shift\n",
    "\n",
    "plot_hour_shift(mfcc_accs, conv_ae_accs, bae_accs, mffce_accs, vae_accs, cvae_accs_s, cvae_accs_z,\n",
    "                labels_list=['mfcc', 'conv', 'bae', 'mfcce', 'vae', 'cvae_s', 'cvae_z'],\n",
    "                xticklabels=[str(start_hour) for start_hour in start_hours],\n",
    "                save_path = 'data\\\\outputs\\\\zs_encoder_0_s.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize on 2D map, we basically perform TSNE and PCA dimension reduction in order to visualize night and day. Probably this will be not efficent but it is worth to give a shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "start_hour = 23\n",
    "end_hour = 2\n",
    "\n",
    "reduce_df = pd.DataFrame(df_hive_data)\n",
    "reduce_df['feature_vector'] = StandardScaler().fit_transform(df_hive_data['bae_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "reduced_ae_pca = PCA(n_components=2).fit_transform(reduce_df['feature_vector'].values.tolist())\n",
    "reduced_ae_tsne =  TSNE(n_components=2, perplexity=100, learning_rate=500).fit_transform(reduce_df['feature_vector'].values.tolist())\n",
    "is_night_list = (reduce_df.index.hour >= start_hour) | (reduce_df.index.hour <= end_hour)\n",
    "                \n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "labels = ['day', 'night']\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(10,10))\n",
    "\n",
    "axs[0].scatter(x=[data[0] for data in reduced_ae_pca],\n",
    "               y=[data[1] for data in reduced_ae_pca],\n",
    "               c=[colors[night] for night in is_night_list],\n",
    "              alpha=0.3)\n",
    "axs[0].set_title('PCA')\n",
    "\n",
    "axs[1].scatter(x=[data[0] for data in reduced_ae_tsne],\n",
    "               y=[data[1] for data in reduced_ae_tsne],\n",
    "               c=[colors[night] for night in is_night_list],\n",
    "              alpha=0.3)\n",
    "axs[1].set_title('TSNE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(reduce_df['feature_vector'].values.tolist())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def plot_distribution(distribution_dict, bin_size):\n",
    "    \"\"\" Plotting distribiution for dictionary elements\"\"\"\n",
    "    colors = ['blue', 'green', 'red', 'yellow', 'black', 'pink', 'purple']\n",
    "    rms_max = 0\n",
    "    rms_min = 65535\n",
    "    for k, v in rmses.items():\n",
    "        if np.max(v) > rms_max:\n",
    "            rms_max = np.max(v)\n",
    "        if np.min(v) < rms_min:\n",
    "            rms_min = np.min(v)\n",
    "        \n",
    "    plt.figure()\n",
    "    for idx, (k, v) in enumerate(distribution_dict.items()):\n",
    "        plt.hist(v, color=colors[idx%len(colors)], bins=int(np.abs(rms_max-rms_min)/bin_size))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of code for calculating autocorrelaction for specific feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "features = ['conv_ae', 'humidity', 'temperature',\n",
    "            'alcohol', 'aceton', 'jon-amonowy',\n",
    "            'toluen', 'co2', 'siarkowodor',\n",
    "            'metanotiol', 'trimetyloamina', 'wodor', 'co']\n",
    "\n",
    "feature = features[12]\n",
    "data_to_autocorr = df_hive_co_ua\n",
    "\n",
    "roll_len = 3\n",
    "interval = (data_to_autocorr.index[2] - data_to_autocorr.index[1]).seconds//60%60\n",
    "\n",
    "y2 = data_to_autocorr[feature].rolling(window=roll_len).mean().values\n",
    "y_corr = y2[roll_len:]\n",
    "x_corelation = np.arange(start=0, step=2, stop=150)\n",
    "\n",
    "fig, axes = plt.subplots(1, figsize=(8,5))\n",
    "x = plot_acf(y_corr, lags=x_corelation, zero=False, ax=axes)\n",
    "x_raw = acf(y_corr, nlags=150)\n",
    "axes.set_title(f'{feature} autocorrelaction')\n",
    "axes.set_xlabel(f'Lag (1 lag = {interval} minutes)') \n",
    "axes.set_ylabel('Correlation')\n",
    "axes.set_xticks(np.arange(0, 151, step=10))\n",
    "\n",
    "print(f'{feature} with max {max(x_raw[60:]):.2f} at {60 + np.argmax(x_raw[60:])}')\n",
    "\n",
    "# temperature with max 0.74 at 93 (15 mint)\n",
    "# humidity with max 0.58 at 92 (15 min)\n",
    "# alcohol with max 0.53 at 134 (10 min)\n",
    "# aceton with max 0.52 at 133 (10 min)\n",
    "# jon-amonowy with max 0.57 at 133 (10 min)\n",
    "# toluen with max 0.52 at 134 (10 min)\n",
    "# co2 with max 0.54 at 133 (10 min)\n",
    "# siarkowodor with max 0.16 at 142 (10 min)\n",
    "# metanotiol with max 0.34 at 140 (10 min)\n",
    "# trimetyloamina with max 0.56 at 138 (10 min)\n",
    "# wodor with max 0.14 at 142 (10 min)\n",
    "# co with max 0.62 at 134 (10 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.autoencoder_utils as ae\n",
    "import utils.data_utils as du\n",
    "import utils.customdataset as cd\n",
    "import utils.contrastive_pytorch as cp\n",
    "import utils.contrastive_keras as ck\n",
    "import utils.periodogram_dataset as pd\n",
    "\n",
    "importlib.reload(pd)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}