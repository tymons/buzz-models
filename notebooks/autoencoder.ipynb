{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "hives_ids = [\"smrpiclient7\", \"smrpiclient6\", \"smrpiclient3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "def plot_spectrogram(frequency, time_x, spectrocgram, title):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.title(title)\n",
    "    plt.pcolormesh(time_x, frequency, spectrocgram)\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class View(nn.Module):\n",
    "    \"\"\" Function for nn.Sequentional to reshape data \"\"\"\n",
    "    def __init__(self, shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "def conv2d_block(in_f, out_f, *args, **kwargs):\n",
    "    \"\"\" Function for building convolutional block\n",
    "\n",
    "        Attributes\n",
    "            in_f - number of input features\n",
    "            out_f - number of output features\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p=0.2)\n",
    "    )\n",
    "\n",
    "def conv2d_transpose_block(in_f, out_f, *args, **kwargs):\n",
    "    \"\"\" Function for building transpose convolutional block\n",
    "        \n",
    "        Attributes\n",
    "            in_f - number of input features\n",
    "            out_f - number of output features\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p=0.2)\n",
    "    )\n",
    "\n",
    "######################################\n",
    "#                                    #\n",
    "#   Main convolutional autoencoder   #\n",
    "#                                    #\n",
    "######################################\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        ## encoder layers ##\n",
    "        self.encoder = nn.Sequential(\n",
    "            # [1x256x64] => [64x256x64]\n",
    "            conv2d_block(1, 64, kernel_size=3, padding=1),\n",
    "            # [64x256x64] => [64x128x32]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [64x128x32] => [32x128x32]\n",
    "            conv2d_block(64, 32, kernel_size=3, padding=1),\n",
    "            # [32x128x32] => [32x64x16]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [32x64x16] => [16x64x16]\n",
    "            conv2d_block(32, 16, kernel_size=3, padding=1),\n",
    "            # [16x64x16] => [16x32x8]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [16x32x8] => [4x32x8]\n",
    "            conv2d_block(16, 4, kernel_size=3, padding=1),\n",
    "            # [4x32x8] => [4x16x4]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [4x16x4] => [1x256]\n",
    "            nn.Flatten(),\n",
    "            # [1x256] => [1x64]\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        self.decoder = nn.Sequential(\n",
    "            # [1x64] => [1x256]\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            # [1x256] => [4x16x4]\n",
    "            View([-1, 4, 16, 4]),\n",
    "            # [4x16x4] => [16x32x8]\n",
    "            conv2d_transpose_block(4, 16, kernel_size=2, stride=2),\n",
    "            # [16x32x8] => [32x64x16]\n",
    "            conv2d_transpose_block(16, 32, kernel_size=2, stride=2),\n",
    "            # [32x64x16] => [64x128x32]\n",
    "            conv2d_transpose_block(32, 64, kernel_size=2, stride=2),\n",
    "            # [64x128x32] => [1x256x64]\n",
    "            nn.ConvTranspose2d(64, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class autoencoder_basic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder_basic, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1499, 512),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 1499),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load train data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound data preparation for hive: smrpiclient7 which has 3367 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3367/3367 [00:57<00:00, 58.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "Sound data preparation for hive: smrpiclient6 which has 3172 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3172/3172 [00:53<00:00, 58.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "Sound data preparation for hive: smrpiclient3 which has 602 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 602/602 [00:10<00:00, 58.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " done.\n",
      "Got 7141 sound samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import math\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "sound_time_ms = 2000\n",
    "# ~93 ms for fft window\n",
    "nfft = 4096\n",
    "# ~34% overlapping\n",
    "hop_len = (nfft//3) + 30\n",
    "# This can be manipulated to adjust number of bins for conv layer\n",
    "fmax = 2750\n",
    "\n",
    "hives_data = []\n",
    "max_to_norm = 0\n",
    "for idx, hive_id in enumerate(hives_ids):\n",
    "    sound_files = [f for f in glob.glob(f\"..\\\\measurements\\\\smartulav2\\\\{hive_id}_*\\\\*.wav\")]\n",
    "    print(f\"Sound data preparation for hive: {hive_id} which has {len(sound_files)} recordings...\", end=' ', flush=True)\n",
    "    for file in tqdm(sound_files):\n",
    "        sample_rate, sound_samples = wavfile.read(file)\n",
    "        sound_samples = sound_samples.T[0]/(2.0**31)\n",
    "    \n",
    "        spectrogram = librosa.core.stft(sound_samples, n_fft=nfft, hop_length=hop_len)\n",
    "        spectrogram_magnitude = np.abs(spectrogram)\n",
    "        spectrogram_phase = np.angle(spectrogram)\n",
    "        spectrogram_db = librosa.amplitude_to_db(spectrogram_magnitude, ref=np.max)\n",
    "        frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "        times = (np.arange(0, spectrogram_magnitude.shape[1])*hop_len)/sample_rate\n",
    "        \n",
    "        freq_slice = np.where((frequencies < fmax))\n",
    "        frequencies = frequencies[freq_slice]\n",
    "        spectrogram_db = spectrogram_db[freq_slice, :][0]    \n",
    "    \n",
    "        filename = file.rsplit('\\\\', 1)[-1]\n",
    "        timestamp = filename[filename.index('-')+1:].rsplit(\".wav\")[0]\n",
    "        datetime = datetime.strptime(timestamp, '%Y-%m-%dT%H-%M-%S')\n",
    "        hives_data.append([datetime, hive_id, sound_samples, [frequencies, times, spectrogram_db]])\n",
    "    print(\" done.\")\n",
    "\n",
    "print(f\"Got {len(hives_data)} sound samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for autoencoder train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "random_idx = random.randint(0, len(hives_data) - 1)\n",
    "\n",
    "plot_spectrogram(hives_data[random_idx][3][0],\n",
    "                 hives_data[random_idx][3][1],\n",
    "                 hives_data[random_idx][3][2],\n",
    "                 f\"hive: {hives_data[random_idx][1]}, time: {hives_data[random_idx][0]}, idx: {random_idx}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train basic AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal as sig\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data as tdata\n",
    "\n",
    "# Divide data to test, validation and train\n",
    "train_stop_idx = int(pd_ae_data.shape[0]*90/100)\n",
    "\n",
    "pd_ae_data_train = pd_ae_data[:train_stop_idx]\n",
    "pd_ae_data_test = pd_ae_data[train_stop_idx:]\n",
    "\n",
    "print(f'Train data size: {pd_ae_data_train.shape[0]}')\n",
    "print(f'Test data size: {pd_ae_data_test.shape[0]}')\n",
    "\n",
    "tensor_train = torch.Tensor(pd_ae_data_train['periodogram'].values.tolist())\n",
    "tensor_test = torch.Tensor(pd_ae_data_test['periodogram'].values.tolist())\n",
    "\n",
    "train_dataset = tdata.TensorDataset(tensor_train)\n",
    "test_dataset = tdata.TensorDataset(tensor_test)\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = autoencoder_basic().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    for data in dataloader:\n",
    "        periodogram = data[0].to(device)\n",
    "        # ===================forward=====================\n",
    "        output = model(periodogram)\n",
    "        #train_loss = criterion(output, periodogram)\n",
    "        train_loss = F.binary_cross_entropy(output, periodogram)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += train_loss.item()\n",
    "    loss = loss / len(dataloader)\n",
    "    if (epoch+1) % (num_epochs/10) == 0:\n",
    "        print(f'epoch {epoch + 1}/{num_epochs}, loss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    loss_test = 0\n",
    "    for data in dataloader_test:\n",
    "        periodograms_test = data[0].to(device)\n",
    "        output = model(periodograms_test)\n",
    "        for idx, i in enumerate(output):\n",
    "            #loss_test += nn.MSELoss()(periodograms_test[idx], i)\n",
    "            loss_test += F.binary_cross_entropy(periodograms_test[idx], i)\n",
    "\n",
    "loss_test = loss_test/len(pd_ae_data_test)\n",
    "print(f'Final test loss: {loss_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CONV AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got dataset of size: 7141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "spectrogram_ae_data = [hive_data[3][2] for hive_data in hives_data]\n",
    "standarized_ae_data = [sc.fit_transform(spec.T).T for spec in spectrogram_ae_data]\n",
    "scaled_ae_data = [mm.fit_transform(stan.T).T for stan in standarized_ae_data]\n",
    "\n",
    "print(f\"Got dataset of size: {len(scaled_ae_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: torch.Size([7141, 256, 64])\n",
      "Train set size: 5712\n",
      "Test set size: 28\n",
      "Validation set size: 1401\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from torch.utils import data as tdata\n",
    "\n",
    "train_data_size = len(scaled_ae_data)*80//100\n",
    "test_data_size = (len(scaled_ae_data) - train_data_size) // 50\n",
    "val_data_size = len(scaled_ae_data) - train_data_size - test_data_size\n",
    "\n",
    "dataset_tensor = torch.Tensor(scaled_ae_data)\n",
    "print(f\"Dataset shape: {dataset_tensor.shape}\")\n",
    "print(f\"Train set size: {train_data_size}\")\n",
    "print(f\"Test set size: {test_data_size}\")\n",
    "print(f\"Validation set size: {val_data_size}\")\n",
    "\n",
    "# add one extra dimension as it is required for conv layer\n",
    "dataset_tensor = dataset_tensor[:, None, :, :] \n",
    "dataset = tdata.TensorDataset(dataset_tensor)\n",
    "train_set, test_set, val_set = torch.utils.data.random_split(dataset, [train_data_size, test_data_size, val_data_size])\n",
    "\n",
    "dataloader_train = tdata.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "dataloader_test = tdata.DataLoader(test_set, batch_size=32, shuffle=True)\n",
    "dataloader_val = tdata.DataLoader(val_set, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1/50] train_loss: 0.07586 valid_loss: 0.05323 checkpoint!\n",
      "[ 2/50] train_loss: 0.04580 valid_loss: 0.04042 checkpoint!\n",
      "[ 3/50] train_loss: 0.04318 valid_loss: 0.04921 .\n",
      "[ 4/50] train_loss: 0.04239 valid_loss: 0.04637 .\n",
      "[ 5/50] train_loss: 0.04156 valid_loss: 0.03884 checkpoint!\n",
      "[ 6/50] train_loss: 0.04093 valid_loss: 0.03479 checkpoint!\n",
      "[ 7/50] train_loss: 0.04054 valid_loss: 0.03844 .\n",
      "[ 8/50] train_loss: 0.04024 valid_loss: 0.03842 .\n",
      "[ 9/50] train_loss: 0.03985 valid_loss: 0.02924 checkpoint!\n",
      "[10/50] train_loss: 0.03979 valid_loss: 0.04215 .\n",
      "[11/50] train_loss: 0.03950 valid_loss: 0.03861 .\n",
      "[12/50] train_loss: 0.03946 valid_loss: 0.04066 .\n",
      "[13/50] train_loss: 0.03930 valid_loss: 0.03680 .\n",
      "[14/50] train_loss: 0.03923 valid_loss: 0.04122 .\n",
      "[15/50] train_loss: 0.03909 valid_loss: 0.04223 .\n",
      "[16/50] train_loss: 0.03896 valid_loss: 0.03667 .\n",
      "[17/50] train_loss: 0.03891 valid_loss: 0.04697 .\n",
      "[18/50] train_loss: 0.03884 valid_loss: 0.03654 .\n",
      "[19/50] train_loss: 0.03879 valid_loss: 0.03980 .\n",
      "[20/50] train_loss: 0.03865 valid_loss: 0.03835 early stopping.\n",
      "=> loading checkpoint checkpoint.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7327ead1c442629bfe35d32568e4e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# del modelConvAE\n",
    "\n",
    "modelConvAE = ConvAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(modelConvAE.parameters(), lr=learning_rate, weight_decay=1e-6)\n",
    "\n",
    "# monitor training loss per batch\n",
    "train_loss = []\n",
    "# monitor validation loss per batch\n",
    "val_loss = []\n",
    "# save avg train losses for early stopping visualization\n",
    "avg_train_loss = []\n",
    "# save avg train losses for early stopping visualization\n",
    "avg_val_loss = [] \n",
    "# patience when stop training\n",
    "patience = 10\n",
    "# counter for patience in early sotpping\n",
    "patience_counter = 0\n",
    "# best validation score\n",
    "best_val_loss = -1\n",
    "# model checkpoint filename\n",
    "checkpoint_filename = 'checkpoint.pth'\n",
    "# early stopping epoch\n",
    "win_epoch = 0\n",
    "    \n",
    "for epoch in range(1, num_epochs+1):    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    modelConvAE.train()\n",
    "    for data in dataloader_train:\n",
    "        # transfer data to device\n",
    "        periodogram = data[0].to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        outputs = modelConvAE(periodogram)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, periodogram)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss.append(float(loss.item()))\n",
    "        \n",
    "    ###################\n",
    "    # val the model   #\n",
    "    ###################\n",
    "    modelConvAE.eval()\n",
    "    for val_data in dataloader_val:\n",
    "        # transfer data to device\n",
    "        periodogram = data[0].to(device)\n",
    "        # forward pass\n",
    "        outputs = modelConvAE(periodogram)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, periodogram)\n",
    "        # update running val loss\n",
    "        val_loss.append(float(loss.item()))\n",
    "    \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = np.average(train_loss)\n",
    "    val_loss = np.average(val_loss)\n",
    "    avg_train_loss.append(train_loss)\n",
    "    avg_val_loss.append(val_loss)\n",
    "    \n",
    "    epoch_len = len(str(num_epochs))\n",
    "    # print avg training statistics \n",
    "    print(f'[{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] train_loss: {train_loss:.5f} valid_loss: {val_loss:.5f}', end=' ', flush=True)\n",
    "    \n",
    "    if val_loss < best_val_loss or best_val_loss == -1:\n",
    "        # new checkpoint\n",
    "        print(\"checkpoint!\")\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(modelConvAE.state_dict(), checkpoint_filename)\n",
    "        win_epoch = epoch\n",
    "    elif patience_counter >= patience:\n",
    "        print(\"early stopping.\")\n",
    "        print(f\"=> loading checkpoint {checkpoint_filename}\")\n",
    "        device = torch.device(\"cuda\")\n",
    "        modelConvAE.load_state_dict(torch.load(checkpoint_filename))\n",
    "        break\n",
    "    else:\n",
    "        print(\".\")\n",
    "        patience_counter = patience_counter + 1\n",
    "        \n",
    "    # clear batch losses\n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(1, epoch + 1), avg_train_loss, 'r', label=\"train loss\")\n",
    "plt.plot(np.arange(1, epoch + 1), avg_val_loss, 'b', label=\"validation loss\")\n",
    "plt.axvline(win_epoch, linestyle='--', color='g',label='Early Stopping Checkpoint')\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test loss: 0.00016501914069522172\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    loss_test = 0\n",
    "    for data in dataloader_test:\n",
    "        periodograms_test = data[0].to(device)\n",
    "        output = modelConvAE(periodograms_test)\n",
    "        for idx, output_elem in enumerate(output):\n",
    "            loss_test += criterion(periodograms_test[idx], output_elem)\n",
    "\n",
    "loss_test = loss_test/len(scaled_ae_data)\n",
    "print(f'Final test loss: {loss_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot and visualize (MFCC vs AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_encode(model, data_intput):\n",
    "    \"\"\" Function for encoding data and returning encoded \"\"\"\n",
    "    dataset_tensor = torch.Tensor(data_intput)\n",
    "    dataset_tensor = dataset_tensor[:, None, :, :]\n",
    "    dataset_tensor = tdata.TensorDataset(dataset_tensor)\n",
    "    dataset = tdata.DataLoader(dataset_tensor, batch_size=32, shuffle=True)\n",
    "    encoded_data = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            periodograms = data[0].to(device)\n",
    "            output = modelConvAE.encoder(periodograms).cpu().numpy().squeeze()\n",
    "            encoded_data.extend(output)\n",
    "    \n",
    "    return encoded_data\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = conv2d_encode(modelConvAE, scaled_ae_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01286339ecd0495f87c1a28bc59d413b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-11d830e735aa>:17: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  axs[0].pcolormesh(times, frequencies, scaled_ae_data[idx])\n",
      "<ipython-input-13-11d830e735aa>:18: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  axs[1].pcolormesh(times, frequencies, modelConvAE(elem.to(device)).cpu().numpy().squeeze())\n"
     ]
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "idx = random.randint(0, len(hives_data) - 1)\n",
    "with torch.no_grad():\n",
    "    fig, axs = plt.subplots(2, 1)\n",
    "    frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "    freq_slice = np.where((frequencies < fmax))\n",
    "    frequencies = frequencies[freq_slice]\n",
    "    times = (np.arange(0, spectrogram_magnitude.shape[1])*hop_len)/sample_rate    \n",
    "    \n",
    "    elem = scaled_ae_data[idx]\n",
    "    elem = elem[None, None,: ,:]\n",
    "    elem = torch.Tensor(elem)\n",
    "\n",
    "    axs[0].pcolormesh(times, frequencies, scaled_ae_data[idx])\n",
    "    axs[1].pcolormesh(times, frequencies, modelConvAE(elem.to(device)).cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimension reduction - now we perform t-SNE and PCA to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "reduced_ae_tsne = TSNE(n_components=2, perplexity=100, learning_rate=500, verbose=1).fit_transform(output)\n",
    "reduced_mfcc_tsne = TSNE(n_components=2, perplexity=100, learning_rate=500, verbose=1).fit_transform(pd_ae_data['mfcc'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mfccs_standarized = StandardScaler().fit_transform(pd_ae_data['mfcc'].values.tolist())\n",
    "ae_standarized = StandardScaler().fit_transform(output)\n",
    "reduced_ae_pca = PCA(n_components = 2).fit_transform(ae_standarized)\n",
    "reduced_mfcc_pca = PCA(n_components = 2).fit_transform(mfccs_standarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA().fit(ae_standarized)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "\n",
    "RECORD_TIME = 2\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "data = hives_data[-10][2][:, 0]/(2.0**31)\n",
    "datetime = hives_data[-1][0]\n",
    "fft_data = abs(fft(data))\n",
    "freqs = fftfreq(int(len(fft_data)/2), 1/SAMPLE_RATE)\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.tight_layout(pad=3.0)\n",
    "axs[0].set_title(f\"Sound recording at {datetime} ({RECORD_TIME}s)\")\n",
    "axs[0].grid()\n",
    "axs[0].set_xlabel('Time [sec]')\n",
    "axs[0].plot(np.linspace(0, 2, len(data)), data)\n",
    "\n",
    "axs[1].set_title(\"Periodogram\")\n",
    "axs[1].set_xticks(np.arange(0, (freqs.size/2), step=100))\n",
    "axs[1].set_xticklabels(np.arange(0, (freqs.size/2), step=100, dtype=int), rotation=45)\n",
    "axs[1].grid()\n",
    "axs[1].set_xlabel('Frequency [Hz]')\n",
    "axs[1].plot(freqs[1:1500], fft_data[1:1500], 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
