{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "hives_ids = [\"smrpiclient7\", \"smrpiclient6\", \"smrpiclient3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 1 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
    "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
    "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## encode ##\n",
    "        # add hidden layers with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add second hidden layer\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "        \n",
    "        ## decode ##\n",
    "        # add transpose conv layers, with relu activation function\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        # output layer (with sigmoid for scaling from 0 to 1)\n",
    "        x = F.sigmoid(self.t_conv2(x))\n",
    "                \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class autoencoder_basic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder_basic, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1499, 512),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.SELU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 512),\n",
    "            nn.SELU(True),\n",
    "            nn.Linear(128, 1499),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load train data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound data preparation for hive: smrpiclient7 which has 3367 recordings... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                               | 0/3367 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sound samples len: 88200 with samples rate: 44100\n",
      "Number of samples per segment: 8820\n",
      "601\n",
      "(601, 11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5c918e2192419e9e33ac43c466e5bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 0 sound samples \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-7810881111b3>:42: MatplotlibDeprecationWarning: shading='flat' when X and Y have the same dimensions as C is deprecated since 3.3.  Either specify the corners of the quadrilaterals with X and Y, or pass shading='auto', 'nearest' or 'gouraud', or set rcParams['pcolor.shading'].  This will become an error two minor releases later.\n",
      "  plt.pcolormesh(times, frequencies, spectrogram)\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from scipy.fftpack import fft\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib widget\n",
    "\n",
    "sound_time_ms = 2000\n",
    "segment_time_ms = 200\n",
    "fmax = 3000\n",
    "\n",
    "hives_data = []\n",
    "max_to_norm = 0\n",
    "for idx, hive_id in enumerate(hives_ids):\n",
    "    sound_files = [f for f in glob.glob(f\"..\\\\measurements\\\\smartulav2\\\\{hive_id}_*\\\\*.wav\")]\n",
    "    print(f\"Sound data preparation for hive: {hive_id} which has {len(sound_files)} recordings...\", end=' ', flush=True)\n",
    "    for file in tqdm(sound_files):\n",
    "        sample_rate, sound_samples = wavfile.read(file)\n",
    "        sound_samples = sound_samples.T[0]/(2.0**31)\n",
    "        print(f\"Sound samples len: {len(sound_samples)} with samples rate: {sample_rate}\")\n",
    "        print(f\"Number of samples per segment: {math.floor(segment_time_ms*len(sound_samples)/sound_time_ms)}\")\n",
    "        frequencies, times, spectrogram = signal.spectrogram(sound_samples, sample_rate,\n",
    "                                                             nperseg=math.floor(segment_time_ms*len(sound_samples)/sound_time_ms),\n",
    "                                                             window=('hamming'))\n",
    "        freq_slice = np.where((frequencies <= fmax))\n",
    "        frequencies = frequencies[freq_slice]\n",
    "        spectrogram = spectrogram[freq_slice, :][0]\n",
    "        print(len(frequencies))\n",
    "        print((spectrogram.shape))\n",
    "\n",
    "        break\n",
    "        filename = file.rsplit('\\\\', 1)[-1]\n",
    "        timestamp = filename[filename.index('-')+1:].rsplit(\".wav\")[0]\n",
    "        datetime = datetime.strptime(timestamp, '%Y-%m-%dT%H-%M-%S')\n",
    "        hives_data.append([datetime, hive_id, sound_samples, fft_data])\n",
    "    plt.pcolormesh(times, frequencies, spectrogram)\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()\n",
    "    break\n",
    "    print(\" done.\")\n",
    "\n",
    "print(f\"Got {len(hives_data)} sound samples \")\n",
    "# # Normalize\n",
    "# hives_data = [[data[0], data[1], data[2]] for data in hives_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for autoencoder train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal as sig\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "                      \n",
    "pd_data = pd.DataFrame(\n",
    "    {\n",
    "        'samples': [hive_data[2] for hive_data in hives_data],\n",
    "        'periodogram' : [hive_data[3] for hive_data in hives_data],\n",
    "        'sn': [hive_data[1] for hive_data in hives_data]\n",
    "    }\n",
    ")\n",
    "\n",
    "pd_data['periodogram'] = list(np.transpose(MinMaxScaler().fit_transform(np.transpose(pd_data['periodogram'].values.tolist()))))\n",
    "# pd_data['mfcc'] = list([np.mean(librosa.feature.mfcc(sample, sr=3000, n_fft=150, hop_length=75, n_mfcc=14), axis=1)\n",
    "#                            for sample in tpd_data['samples'].to_numpy()])\n",
    "# pd_ae_data = pd_data[(pd_data['sn'] == 'smrpiclient7')]\n",
    "pd_ae_data = pd_data\n",
    "# pd_ae_data = pd_data[(pd_data['sn'] == 1300001)\n",
    "#                         | (pd_data['sn'] == 1400001)\n",
    "#                         | (pd_data['sn'] == 1400002)]\n",
    "pd_ae_data = pd_ae_data.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>samples</th>\n",
       "      <th>periodogram</th>\n",
       "      <th>sn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7136</th>\n",
       "      <td>[-0.14960726769641042, -0.1493848362006247, -0...</td>\n",
       "      <td>[0.3205124081023819, 0.15923276278448897, 0.11...</td>\n",
       "      <td>smrpiclient6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7137</th>\n",
       "      <td>[-0.14167435513809323, -0.141726931091398, -0....</td>\n",
       "      <td>[1.0, 0.4366336158776749, 0.31181935933731103,...</td>\n",
       "      <td>smrpiclient7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7138</th>\n",
       "      <td>[-0.15721441665664315, -0.15710320137441158, -...</td>\n",
       "      <td>[0.9438849481011324, 0.4754277805104857, 0.479...</td>\n",
       "      <td>smrpiclient6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7139</th>\n",
       "      <td>[-0.1556573980487883, -0.1556573980487883, -0....</td>\n",
       "      <td>[1.0, 0.4542248835710911, 0.3304965097732763, ...</td>\n",
       "      <td>smrpiclient6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7140</th>\n",
       "      <td>[-0.1603951840661466, -0.16035069758072495, -0...</td>\n",
       "      <td>[0.3139936408310009, 0.18356474968882697, 0.08...</td>\n",
       "      <td>smrpiclient6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                samples  \\\n",
       "7136  [-0.14960726769641042, -0.1493848362006247, -0...   \n",
       "7137  [-0.14167435513809323, -0.141726931091398, -0....   \n",
       "7138  [-0.15721441665664315, -0.15710320137441158, -...   \n",
       "7139  [-0.1556573980487883, -0.1556573980487883, -0....   \n",
       "7140  [-0.1603951840661466, -0.16035069758072495, -0...   \n",
       "\n",
       "                                            periodogram            sn  \n",
       "7136  [0.3205124081023819, 0.15923276278448897, 0.11...  smrpiclient6  \n",
       "7137  [1.0, 0.4366336158776749, 0.31181935933731103,...  smrpiclient7  \n",
       "7138  [0.9438849481011324, 0.4754277805104857, 0.479...  smrpiclient6  \n",
       "7139  [1.0, 0.4542248835710911, 0.3304965097732763, ...  smrpiclient6  \n",
       "7140  [0.3139936408310009, 0.18356474968882697, 0.08...  smrpiclient6  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_ae_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data size: 6426\n",
      "Test data size: 715\n"
     ]
    }
   ],
   "source": [
    "from scipy import signal as sig\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils import data as tdata\n",
    "\n",
    "# Divide data to test, validation and train\n",
    "train_stop_idx = int(pd_ae_data.shape[0]*90/100)\n",
    "\n",
    "pd_ae_data_train = pd_ae_data[:train_stop_idx]\n",
    "pd_ae_data_test = pd_ae_data[train_stop_idx:]\n",
    "\n",
    "print(f'Train data size: {pd_ae_data_train.shape[0]}')\n",
    "print(f'Test data size: {pd_ae_data_test.shape[0]}')\n",
    "\n",
    "tensor_train = torch.Tensor(pd_ae_data_train['periodogram'].values.tolist())\n",
    "tensor_test = torch.Tensor(pd_ae_data_test['periodogram'].values.tolist())\n",
    "\n",
    "train_dataset = tdata.TensorDataset(tensor_train)\n",
    "test_dataset = tdata.TensorDataset(tensor_test)\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dataloader_test = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train basic AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "num_epochs = 1000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "model = autoencoder_basic().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = 0\n",
    "    for data in dataloader:\n",
    "        periodogram = data[0].to(device)\n",
    "        # ===================forward=====================\n",
    "        output = model(periodogram)\n",
    "        #train_loss = criterion(output, periodogram)\n",
    "        train_loss = F.binary_cross_entropy(output, periodogram)\n",
    "        # ===================backward====================\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        loss += train_loss.item()\n",
    "    loss = loss / len(dataloader)\n",
    "    if (epoch+1) % (num_epochs/10) == 0:\n",
    "        print(f'epoch {epoch + 1}/{num_epochs}, loss:{loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CONV AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "\n",
    "modelConvAE = ConvAutoencoder().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1499])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, num_epochs+1):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    for data in dataloader:\n",
    "        # _ stands in for labels, here\n",
    "        # no need to flatten images\n",
    "        print(data[0].shape)\n",
    "        break\n",
    "        periodogram = data[0].to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        outputs = modelConvAE(periodogram)\n",
    "        # calculate the loss\n",
    "        loss = criterion(outputs, periodogram)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*periodogram.size(0)\n",
    "    break        \n",
    "    # print avg training statistics \n",
    "    train_loss = train_loss/len(dataloader)\n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# convert data to torch.FloatTensor\n",
    "transform = transforms.ToTensor()\n",
    "train_data = datasets.MNIST(root='../MNIST-data', train=True, download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=20, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(train_loader):\n",
    "    print((data[0]).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    loss_test = 0\n",
    "    for data in dataloader_test:\n",
    "        periodograms_test = data[0].to(device)\n",
    "        output = model(periodograms_test)\n",
    "        for idx, i in enumerate(output):\n",
    "            #loss_test += nn.MSELoss()(periodograms_test[idx], i)\n",
    "            loss_test += F.binary_cross_entropy(periodograms_test[idx], i)\n",
    "\n",
    "loss_test = loss_test/len(pd_ae_data_test)\n",
    "print(f'Final test loss: {loss_test}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and visualize (MFCC vs AE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    encode_data = pd_ae_data['periodogram'].values.tolist()\n",
    "    encode_data_tensor = torch.Tensor(encode_data).to(device)\n",
    "    output = [model.encoder(encode_data_tensor).cpu().numpy()][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "idx =708\n",
    "with torch.no_grad():\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(encode_data[idx], 'r')\n",
    "    plt.plot(model.decoder(torch.Tensor(output[idx]).to(device)).cpu().numpy(), 'b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimension reduction - now we perform t-SNE and PCA to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "reduced_ae_tsne = TSNE(n_components=2, perplexity=100, learning_rate=500, verbose=1).fit_transform(output)\n",
    "reduced_mfcc_tsne = TSNE(n_components=2, perplexity=100, learning_rate=500, verbose=1).fit_transform(pd_ae_data['mfcc'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "mfccs_standarized = StandardScaler().fit_transform(pd_ae_data['mfcc'].values.tolist())\n",
    "ae_standarized = StandardScaler().fit_transform(output)\n",
    "reduced_ae_pca = PCA(n_components = 2).fit_transform(ae_standarized)\n",
    "reduced_mfcc_pca = PCA(n_components = 2).fit_transform(mfccs_standarized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA().fit(ae_standarized)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "clf = IsolationForest(n_estimators=100, max_samples='auto', contamination=float(.005), \\\n",
    "                        max_features=1.0, bootstrap=False, n_jobs=-1, random_state=43, verbose=0, behaviour=\"new\")\n",
    "\n",
    "clf.fit(reduced_ae_pca)\n",
    "clf.get_params(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_forest = clf.predict(reduced_ae_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10,8))\n",
    "fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "idx_1300001 = pd_data[pd_data['sn'] == 1300001].index.values.tolist()\n",
    "idx_1300002 = pd_data[pd_data['sn'] == 1300002].index.values.tolist()\n",
    "idx_1400001 = pd_data[pd_data['sn'] == 1400001].index.values.tolist()\n",
    "idx_1400002 = pd_data[pd_data['sn'] == 1400002].index.values.tolist()\n",
    "\n",
    "l1_ae_tsne = axs[0][0].scatter([data[0] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1300001],\n",
    "                 [data[1] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1300001], c='r', alpha = 0.3)\n",
    "# l2_ae_tsne = axs[0][0].scatter([data[0] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1300002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1300002], c='b', alpha = 0.3)\n",
    "# l3_ae_tsne = axs[0][0].scatter([data[0] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1400001],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1400001], c='g', alpha = 0.3)\n",
    "# l4_ae_tsne = axs[0][0].scatter([data[0] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1400002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_tsne) if idx in idx_1400002], c='y', alpha = 0.3)\n",
    "\n",
    "l1_mfcc_tsne = axs[0][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1300001],\n",
    "                 [data[1] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1300001], c='r', alpha = 0.3)\n",
    "# l2_mfcc_tsne = axs[0][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1300002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1300002], c='b', alpha = 0.3)\n",
    "# l3_mfcc_tsne = axs[0][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1400001],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1400001], c='g', alpha = 0.3)\n",
    "# l4_mfcc_tsne = axs[0][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1400002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_tsne) if idx in idx_1400002], c='y', alpha = 0.3)\n",
    "\n",
    "# axs[0][0].legend((l1_ae_tsne, l2_ae_tsne, l3_ae_tsne, l4_ae_tsne), ('Sikorki', 'Not Hive', 'Sulmin 1', 'Sulmin 2'),\n",
    "#                  loc='upper right')\n",
    "axs[0][0].set_xlabel('DIM1')\n",
    "axs[0][0].set_ylabel('DIM2')\n",
    "axs[0][0].set_title('Hives - AE sounds with TSNE')\n",
    "\n",
    "# axs[0][1].legend((l1_mfcc_tsne, l2_mfcc_tsne, l3_mfcc_tsne, l4_mfcc_tsne), ('Sikorki', 'Not Hive', 'Sulmin 1', 'Sulmin 2'),\n",
    "#                  loc='upper right')\n",
    "axs[0][1].set_xlabel('DIM1')\n",
    "axs[0][1].set_ylabel('DIM2')\n",
    "axs[0][1].set_title('Hives - MFCC sounds with TSNE')\n",
    "\n",
    "l1_ae_pca = axs[1][0].scatter([data[0] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1300001],\n",
    "                 [data[1] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1300001], c='r', alpha = 0.3)\n",
    "markers = axs[1][0].scatter([data[0] for idx, data in enumerate(reduced_ae_pca) if output_forest[idx] == -1],\n",
    "                           [data[1] for idx, data in enumerate(reduced_ae_pca) if output_forest[idx] == -1], c='b', alpha = 0.9)\n",
    "# l2_ae_pca = axs[1][0].scatter([data[0] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1300002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1300002], c='b', alpha = 0.3)\n",
    "# l3_ae_pca = axs[1][0].scatter([data[0] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1400001],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1400001], c='g', alpha = 0.3)\n",
    "# l4_ae_pca = axs[1][0].scatter([data[0] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1400002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_ae_pca) if idx in idx_1400002], c='y', alpha = 0.3)\n",
    "\n",
    "l1_mfcc_pca = axs[1][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1300001],\n",
    "                 [data[1] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1300001], c='r', alpha = 0.3)\n",
    "# l2_mfcc_pca = axs[1][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1300002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1300002], c='b', alpha = 0.3)\n",
    "# l3_mfcc_pca = axs[1][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1400001],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1400001], c='g', alpha = 0.3)\n",
    "# l4_mfcc_pca = axs[1][1].scatter([data[0] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1400002],\n",
    "#                  [data[1] for idx, data in enumerate(reduced_mfcc_pca) if idx in idx_1400002], c='y', alpha = 0.3)\n",
    "\n",
    "# axs[1][0].legend((l1_ae_pca, l2_ae_pca, l3_ae_pca, l4_ae_pca), ('Sikorki', 'Not Hive', 'Sulmin 1', 'Sulmin 2'),\n",
    "#                  loc='upper right')\n",
    "axs[1][0].set_xlabel('PC1')\n",
    "axs[1][0].set_ylabel('PC2')\n",
    "axs[1][0].set_title('Hives - AE sounds with PCA')\n",
    "\n",
    "# axs[1][1].legend((l1_mfcc_pca, l2_mfcc_pca, l3_mfcc_pca, l4_mfcc_pca), ('Sikorki', 'Not Hive', 'Sulmin 1', 'Sulmin 2'),\n",
    "#                   loc='upper right')\n",
    "axs[1][1].set_xlabel('PC1')\n",
    "axs[1][1].set_ylabel('PC2')\n",
    "axs[1][1].set_title('Hives - MFCC sounds with PCA')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('output-ae/basic-2warstwy-128-32.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft, fftfreq\n",
    "\n",
    "RECORD_TIME = 2\n",
    "SAMPLE_RATE = 44100\n",
    "\n",
    "data = hives_data[-10][2][:, 0]/(2.0**31)\n",
    "datetime = hives_data[-1][0]\n",
    "fft_data = abs(fft(data))\n",
    "freqs = fftfreq(int(len(fft_data)/2), 1/SAMPLE_RATE)\n",
    "\n",
    "fig, axs = plt.subplots(2)\n",
    "fig.tight_layout(pad=3.0)\n",
    "axs[0].set_title(f\"Sound recording at {datetime} ({RECORD_TIME}s)\")\n",
    "axs[0].grid()\n",
    "axs[0].set_xlabel('Time [sec]')\n",
    "axs[0].plot(np.linspace(0, 2, len(data)), data)\n",
    "\n",
    "axs[1].set_title(\"Periodogram\")\n",
    "axs[1].set_xticks(np.arange(0, (freqs.size/2), step=100))\n",
    "axs[1].set_xticklabels(np.arange(0, (freqs.size/2), step=100, dtype=int), rotation=45)\n",
    "axs[1].grid()\n",
    "axs[1].set_xlabel('Frequency [Hz]')\n",
    "axs[1].plot(freqs[1:1500], fft_data[1:1500], 'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
