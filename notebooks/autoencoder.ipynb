{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "hives_ids = [\"smrpiclient7\", \"smrpiclient6\", \"smrpiclient3\"]\n",
    "\n",
    "# define hive under analysis\n",
    "hive_under_analysis = hives_ids[0]\n",
    "# define offset as all data should be utc\n",
    "timezone_offset_hours = 2\n",
    "# define if we should reinit our data\n",
    "DATA_INIT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "Here we load train data sound samples and prepare spectrogram, periodogram and mfcc features (along with some data to visualize this). We should provide data with **utc timestamps** as it will be shifted with `timezone_offset_hours` var. What we also do is remove those samples which has strange rms signal. Threshold 0.8 was chosen based on `plot_distribution` output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "got full dataset of 7033 sound samples\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import librosa.display\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta\n",
    "from scipy.io import wavfile\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "sound_time_ms = 2000\n",
    "# ~93 ms for fft window\n",
    "nfft = 4096\n",
    "# ~34% overlapping\n",
    "hop_len = (nfft//3) + 30\n",
    "# This can be manipulated to adjust number of bins for conv layer\n",
    "fmax = 2750\n",
    "\n",
    "hives_data = []\n",
    "rmses = {}\n",
    "max_to_norm = 0\n",
    "\n",
    "if DATA_INIT:\n",
    "    for idx, hive_id in enumerate(hives_ids):\n",
    "        sound_files = [f for f in glob.glob(f\"..\\\\measurements\\\\smartulav2\\\\{hive_id}_*\\\\*.wav\")]\n",
    "        print(f\"Sound data preparation for hive: {hive_id} which has {len(sound_files)} recordings...\", end=' ', flush=True)\n",
    "        for file in tqdm(sound_files):\n",
    "            sample_rate, sound_samples = wavfile.read(file)\n",
    "            sound_samples = sound_samples.T[0]/(2.0**31)\n",
    "            rms = np.sqrt(sum(sound_samples**2)/len(sound_samples))\n",
    "            if rms < 0.7:    # that threshold was observed from plot_distribution() function\n",
    "                # calculate timestamp\n",
    "                filename = file.rsplit('\\\\', 1)[-1]\n",
    "                utc_timestamp = filename[filename.index('-')+1:].rsplit(\".wav\")[0]\n",
    "                sound_datetime = datetime.strptime(utc_timestamp, '%Y-%m-%dT%H-%M-%S') + timedelta(hours=timezone_offset_hours)\n",
    "                \n",
    "                # calculate mfcc feature\n",
    "                mfccs = librosa.feature.mfcc(y=sound_samples, sr=sample_rate, n_fft=nfft, hop_length=hop_len, n_mfcc=13)\n",
    "                np_mfcc_avg = np.mean(mfccs, axis=1)\n",
    "                \n",
    "                # calculate spectrogram\n",
    "                spectrogram = librosa.core.stft(sound_samples, n_fft=nfft, hop_length=hop_len)\n",
    "                spectrogram_magnitude = np.abs(spectrogram)\n",
    "                spectrogram_phase = np.angle(spectrogram)\n",
    "                spectrogram_db = librosa.amplitude_to_db(spectrogram_magnitude, ref=np.max)\n",
    "                frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "                times = (np.arange(0, spectrogram_magnitude.shape[1])*hop_len)/sample_rate\n",
    "                freq_slice = np.where((frequencies < fmax))\n",
    "                frequencies = frequencies[freq_slice]\n",
    "                spectrogram_db = spectrogram_db[freq_slice, :][0]    \n",
    "                spectrogram_mean = np.mean(spectrogram_db, axis=1)\n",
    "                # decimate?\n",
    "                # spectrogram_db_decimated = decimate(spectrogram_db.T, 4).T\n",
    "                # frequencies_decimated = decimate(frequencies, 4)\n",
    "                \n",
    "                hives_data.append(\n",
    "                    {\n",
    "                        'datetime': sound_datetime,\n",
    "                        'id': hive_id,\n",
    "                        'samples': sound_samples,\n",
    "                        'freq':\n",
    "                            {\n",
    "                                'frequencies': frequencies,\n",
    "                                'time': times,\n",
    "                                'spectrogram_db': spectrogram_db,\n",
    "                            },\n",
    "                        'features':\n",
    "                            {\n",
    "                                'mfcc_avg': np_mfcc_avg\n",
    "                            }\n",
    "                    }\n",
    "                )\n",
    "        print(\" done.\")\n",
    "        np.save('hives_data.npy', hives_data, allow_pickle=True)\n",
    "else:\n",
    "    hives_data = np.load('hives_data.npy', allow_pickle=True)\n",
    "    \n",
    "print(f\"got full dataset of {len(hives_data)} sound samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got dataset of size: 7033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "# extract every spectrogram from hives data list\n",
    "spectrograms = [hive_data[3][2] for hive_data in hives_data]\n",
    "# standarize every periodogram from stft so data for spectrogram will have zero mean and unit variance\n",
    "standarized_spectrograms = [sc.fit_transform(spectrogram.T).T for spectrogram in spectrograms]\n",
    "# scale every standarized periodogram \n",
    "scaled_spectrogram = [mm.fit_transform(spectrogram_stan.T).T for spectrogram_stan in standarized_spectrograms]\n",
    "# get datatime, names and mfcc\n",
    "datetimes = [hive_data[0] for hive_data in hives_data]\n",
    "names = [hive_data[1] for hive_data in hives_data]\n",
    "mfccs = [hive_data[4] for hive_data in hives_data]\n",
    "# standarize and scale periodogram for sounds \n",
    "periodograms_mean = [data[5] for data in hives_data]\n",
    "standarized_periodograms = StandardScaler().fit_transform(periodograms_mean)\n",
    "scaled_periodograms = MinMaxScaler().fit_transform(standarized_periodograms)\n",
    "\n",
    "sounds = list(zip(scaled_spectrogram, mfccs, scaled_periodograms, datetimes, names))\n",
    "\n",
    "sounds_data = pd.DataFrame(sounds, columns=['spectrogram', 'mfccs', 'periodogram', 'datetime', 'name'])\n",
    "sounds_data['datetime'] = pd.to_datetime(sounds_data['datetime'])\n",
    "sounds_hive_data = sounds_data[sounds_data['name'] == hive_under_analysis]\n",
    "\n",
    "print(f\"Got dataset of size: {len(sounds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BASIC AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train basic fully connected autoencoder on data from particular hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: torch.Size([3367, 256])\n",
      "Train set size: 2693\n",
      "Validation set size: 674\n",
      "[  1/200] train_loss: 0.15639 valid_loss: 0.12493 checkpoint!\n",
      "[  2/200] train_loss: 0.13438 valid_loss: 0.10284 checkpoint!\n",
      "[  3/200] train_loss: 0.12009 valid_loss: 0.10120 checkpoint!\n",
      "[  4/200] train_loss: 0.10962 valid_loss: 0.08534 checkpoint!\n",
      "[  5/200] train_loss: 0.09874 valid_loss: 0.07280 checkpoint!\n",
      "[  6/200] train_loss: 0.09099 valid_loss: 0.06506 checkpoint!\n",
      "[  7/200] train_loss: 0.08342 valid_loss: 0.06380 checkpoint!\n",
      "[  8/200] train_loss: 0.07564 valid_loss: 0.06937 .\n",
      "[  9/200] train_loss: 0.07081 valid_loss: 0.05743 checkpoint!\n",
      "[ 10/200] train_loss: 0.06536 valid_loss: 0.07026 .\n",
      "[ 11/200] train_loss: 0.06081 valid_loss: 0.04057 checkpoint!\n",
      "[ 12/200] train_loss: 0.05681 valid_loss: 0.03678 checkpoint!\n",
      "[ 13/200] train_loss: 0.05502 valid_loss: 0.04377 .\n",
      "[ 14/200] train_loss: 0.05127 valid_loss: 0.04054 .\n",
      "[ 15/200] train_loss: 0.04731 valid_loss: 0.02777 checkpoint!\n",
      "[ 16/200] train_loss: 0.04548 valid_loss: 0.03778 .\n",
      "[ 17/200] train_loss: 0.04337 valid_loss: 0.03430 .\n",
      "[ 18/200] train_loss: 0.03977 valid_loss: 0.03252 .\n",
      "[ 19/200] train_loss: 0.04048 valid_loss: 0.02406 checkpoint!\n",
      "[ 20/200] train_loss: 0.03729 valid_loss: 0.02714 .\n",
      "[ 21/200] train_loss: 0.03721 valid_loss: 0.03205 .\n",
      "[ 22/200] train_loss: 0.03501 valid_loss: 0.02059 checkpoint!\n",
      "[ 23/200] train_loss: 0.03328 valid_loss: 0.02367 .\n",
      "[ 24/200] train_loss: 0.03298 valid_loss: 0.01819 checkpoint!\n",
      "[ 25/200] train_loss: 0.03155 valid_loss: 0.02495 .\n",
      "[ 26/200] train_loss: 0.03129 valid_loss: 0.01837 .\n",
      "[ 27/200] train_loss: 0.02928 valid_loss: 0.02610 .\n",
      "[ 28/200] train_loss: 0.03991 valid_loss: 0.02001 .\n",
      "[ 29/200] train_loss: 0.03139 valid_loss: 0.01894 .\n",
      "[ 30/200] train_loss: 0.02771 valid_loss: 0.02250 .\n",
      "[ 31/200] train_loss: 0.02658 valid_loss: 0.01366 checkpoint!\n",
      "[ 32/200] train_loss: 0.02535 valid_loss: 0.01143 checkpoint!\n",
      "[ 33/200] train_loss: 0.02511 valid_loss: 0.01261 .\n",
      "[ 34/200] train_loss: 0.02401 valid_loss: 0.01516 .\n",
      "[ 35/200] train_loss: 0.02391 valid_loss: 0.01446 .\n",
      "[ 36/200] train_loss: 0.02279 valid_loss: 0.01169 .\n",
      "[ 37/200] train_loss: 0.02246 valid_loss: 0.01439 .\n",
      "[ 38/200] train_loss: 0.02144 valid_loss: 0.01088 checkpoint!\n",
      "[ 39/200] train_loss: 0.02148 valid_loss: 0.01130 .\n",
      "[ 40/200] train_loss: 0.02196 valid_loss: 0.01145 .\n",
      "[ 41/200] train_loss: 0.02082 valid_loss: 0.01233 .\n",
      "[ 42/200] train_loss: 0.01930 valid_loss: 0.01334 .\n",
      "[ 43/200] train_loss: 0.01909 valid_loss: 0.01353 .\n",
      "[ 44/200] train_loss: 0.01815 valid_loss: 0.00932 checkpoint!\n",
      "[ 45/200] train_loss: 0.01797 valid_loss: 0.01164 .\n",
      "[ 46/200] train_loss: 0.01739 valid_loss: 0.00947 .\n",
      "[ 47/200] train_loss: 0.01689 valid_loss: 0.00791 checkpoint!\n",
      "[ 48/200] train_loss: 0.01632 valid_loss: 0.01024 .\n",
      "[ 49/200] train_loss: 0.01576 valid_loss: 0.00816 .\n",
      "[ 50/200] train_loss: 0.01403 valid_loss: 0.01134 .\n",
      "[ 51/200] train_loss: 0.01169 valid_loss: 0.00962 .\n",
      "[ 52/200] train_loss: 0.01002 valid_loss: 0.00876 .\n",
      "[ 53/200] train_loss: 0.00905 valid_loss: 0.00792 .\n",
      "[ 54/200] train_loss: 0.00837 valid_loss: 0.00791 .\n",
      "[ 55/200] train_loss: 0.00777 valid_loss: 0.00710 checkpoint!\n",
      "[ 56/200] train_loss: 0.00722 valid_loss: 0.00683 checkpoint!\n",
      "[ 57/200] train_loss: 0.00681 valid_loss: 0.00795 .\n",
      "[ 58/200] train_loss: 0.00643 valid_loss: 0.00661 checkpoint!\n",
      "[ 59/200] train_loss: 0.00619 valid_loss: 0.00606 checkpoint!\n",
      "[ 60/200] train_loss: 0.00606 valid_loss: 0.00605 checkpoint!\n",
      "[ 61/200] train_loss: 0.00575 valid_loss: 0.00546 checkpoint!\n",
      "[ 62/200] train_loss: 0.00565 valid_loss: 0.00530 checkpoint!\n",
      "[ 63/200] train_loss: 0.00535 valid_loss: 0.00547 .\n",
      "[ 64/200] train_loss: 0.00526 valid_loss: 0.00532 .\n",
      "[ 65/200] train_loss: 0.00518 valid_loss: 0.00499 checkpoint!\n",
      "[ 66/200] train_loss: 0.00492 valid_loss: 0.00479 checkpoint!\n",
      "[ 67/200] train_loss: 0.00472 valid_loss: 0.00461 checkpoint!\n",
      "[ 68/200] train_loss: 0.00404 valid_loss: 0.00419 checkpoint!\n",
      "[ 69/200] train_loss: 0.00410 valid_loss: 0.00781 .\n",
      "[ 70/200] train_loss: 0.00387 valid_loss: 0.00410 checkpoint!\n",
      "[ 71/200] train_loss: 0.00396 valid_loss: 0.00431 .\n",
      "[ 72/200] train_loss: 0.00389 valid_loss: 0.00445 .\n",
      "[ 73/200] train_loss: 0.00391 valid_loss: 0.00392 checkpoint!\n",
      "[ 74/200] train_loss: 0.00385 valid_loss: 0.00380 checkpoint!\n",
      "[ 75/200] train_loss: 0.00388 valid_loss: 0.00411 .\n",
      "[ 76/200] train_loss: 0.00379 valid_loss: 0.00426 .\n",
      "[ 77/200] train_loss: 0.00376 valid_loss: 0.00424 .\n",
      "[ 78/200] train_loss: 0.00387 valid_loss: 0.00394 .\n",
      "[ 79/200] train_loss: 0.00373 valid_loss: 0.00388 .\n",
      "[ 80/200] train_loss: 0.00379 valid_loss: 0.00402 .\n",
      "[ 81/200] train_loss: 0.00365 valid_loss: 0.00411 .\n",
      "[ 82/200] train_loss: 0.00361 valid_loss: 0.00382 .\n",
      "[ 83/200] train_loss: 0.00367 valid_loss: 0.00379 checkpoint!\n",
      "[ 84/200] train_loss: 0.00369 valid_loss: 0.00394 .\n",
      "[ 85/200] train_loss: 0.00373 valid_loss: 0.00422 .\n",
      "[ 86/200] train_loss: 0.00368 valid_loss: 0.00405 .\n",
      "[ 87/200] train_loss: 0.00362 valid_loss: 0.00436 .\n",
      "[ 88/200] train_loss: 0.00362 valid_loss: 0.00401 .\n",
      "[ 89/200] train_loss: 0.00371 valid_loss: 0.00438 .\n",
      "[ 90/200] train_loss: 0.00361 valid_loss: 0.00425 .\n",
      "[ 91/200] train_loss: 0.00362 valid_loss: 0.00470 .\n",
      "[ 92/200] train_loss: 0.00354 valid_loss: 0.00379 .\n",
      "[ 93/200] train_loss: 0.00355 valid_loss: 0.00417 .\n",
      "[ 94/200] train_loss: 0.00359 valid_loss: 0.00484 .\n",
      "[ 95/200] train_loss: 0.00364 valid_loss: 0.00419 .\n",
      "[ 96/200] train_loss: 0.00354 valid_loss: 0.00403 .\n",
      "[ 97/200] train_loss: 0.00354 valid_loss: 0.00357 checkpoint!\n",
      "[ 98/200] train_loss: 0.00351 valid_loss: 0.00404 .\n",
      "[ 99/200] train_loss: 0.00356 valid_loss: 0.00374 .\n",
      "[100/200] train_loss: 0.00372 valid_loss: 0.00368 .\n",
      "[101/200] train_loss: 0.00362 valid_loss: 0.00405 .\n",
      "[102/200] train_loss: 0.00354 valid_loss: 0.00436 .\n",
      "[103/200] train_loss: 0.00342 valid_loss: 0.00409 .\n",
      "[104/200] train_loss: 0.00350 valid_loss: 0.00403 .\n",
      "[105/200] train_loss: 0.00346 valid_loss: 0.00362 .\n",
      "[106/200] train_loss: 0.00348 valid_loss: 0.00388 .\n",
      "[107/200] train_loss: 0.00356 valid_loss: 0.00382 .\n",
      "[108/200] train_loss: 0.00353 valid_loss: 0.00377 .\n",
      "[109/200] train_loss: 0.00354 valid_loss: 0.00358 .\n",
      "[110/200] train_loss: 0.00348 valid_loss: 0.00383 .\n",
      "[111/200] train_loss: 0.00355 valid_loss: 0.00355 checkpoint!\n",
      "[112/200] train_loss: 0.00350 valid_loss: 0.00384 .\n",
      "[113/200] train_loss: 0.00343 valid_loss: 0.00361 .\n",
      "[114/200] train_loss: 0.00345 valid_loss: 0.00427 .\n",
      "[115/200] train_loss: 0.00344 valid_loss: 0.00361 .\n",
      "[116/200] train_loss: 0.00347 valid_loss: 0.00372 .\n",
      "[117/200] train_loss: 0.00352 valid_loss: 0.00366 .\n",
      "[118/200] train_loss: 0.00339 valid_loss: 0.00302 checkpoint!\n",
      "[119/200] train_loss: 0.00346 valid_loss: 0.00399 .\n",
      "[120/200] train_loss: 0.00345 valid_loss: 0.00421 .\n",
      "[121/200] train_loss: 0.00349 valid_loss: 0.00363 .\n",
      "[122/200] train_loss: 0.00349 valid_loss: 0.00332 .\n",
      "[123/200] train_loss: 0.00340 valid_loss: 0.00381 .\n",
      "[124/200] train_loss: 0.00334 valid_loss: 0.00357 .\n",
      "[125/200] train_loss: 0.00341 valid_loss: 0.00368 .\n",
      "[126/200] train_loss: 0.00348 valid_loss: 0.00384 .\n",
      "[127/200] train_loss: 0.00340 valid_loss: 0.00395 .\n",
      "[128/200] train_loss: 0.00340 valid_loss: 0.00359 .\n",
      "[129/200] train_loss: 0.00352 valid_loss: 0.00391 .\n",
      "[130/200] train_loss: 0.00345 valid_loss: 0.00376 .\n",
      "[131/200] train_loss: 0.00337 valid_loss: 0.00307 .\n",
      "[132/200] train_loss: 0.00347 valid_loss: 0.00355 .\n",
      "[133/200] train_loss: 0.00346 valid_loss: 0.00297 checkpoint!\n",
      "[134/200] train_loss: 0.00346 valid_loss: 0.00384 .\n",
      "[135/200] train_loss: 0.00330 valid_loss: 0.00329 .\n",
      "[136/200] train_loss: 0.00337 valid_loss: 0.00327 .\n",
      "[137/200] train_loss: 0.00338 valid_loss: 0.00357 .\n",
      "[138/200] train_loss: 0.00331 valid_loss: 0.00302 .\n",
      "[139/200] train_loss: 0.00341 valid_loss: 0.00373 .\n",
      "[140/200] train_loss: 0.00350 valid_loss: 0.00334 .\n",
      "[141/200] train_loss: 0.00333 valid_loss: 0.00343 .\n",
      "[142/200] train_loss: 0.00341 valid_loss: 0.00414 .\n",
      "[143/200] train_loss: 0.00333 valid_loss: 0.00324 .\n",
      "[144/200] train_loss: 0.00335 valid_loss: 0.00288 checkpoint!\n",
      "[145/200] train_loss: 0.00329 valid_loss: 0.00330 .\n",
      "[146/200] train_loss: 0.00334 valid_loss: 0.00294 .\n",
      "[147/200] train_loss: 0.00325 valid_loss: 0.00326 .\n",
      "[148/200] train_loss: 0.00330 valid_loss: 0.00324 .\n",
      "[149/200] train_loss: 0.00326 valid_loss: 0.00286 checkpoint!\n",
      "[150/200] train_loss: 0.00321 valid_loss: 0.00301 .\n",
      "[151/200] train_loss: 0.00329 valid_loss: 0.00311 .\n",
      "[152/200] train_loss: 0.00343 valid_loss: 0.00352 .\n",
      "[153/200] train_loss: 0.00322 valid_loss: 0.00325 .\n",
      "[154/200] train_loss: 0.00321 valid_loss: 0.00315 .\n",
      "[155/200] train_loss: 0.00321 valid_loss: 0.00326 .\n",
      "[156/200] train_loss: 0.00326 valid_loss: 0.00324 .\n",
      "[157/200] train_loss: 0.00326 valid_loss: 0.00373 .\n",
      "[158/200] train_loss: 0.00319 valid_loss: 0.00321 .\n",
      "[159/200] train_loss: 0.00318 valid_loss: 0.00307 .\n",
      "[160/200] train_loss: 0.00317 valid_loss: 0.00318 .\n",
      "[161/200] train_loss: 0.00323 valid_loss: 0.00335 .\n",
      "[162/200] train_loss: 0.00312 valid_loss: 0.00310 .\n",
      "[163/200] train_loss: 0.00311 valid_loss: 0.00339 .\n",
      "[164/200] train_loss: 0.00325 valid_loss: 0.00292 .\n",
      "[165/200] train_loss: 0.00311 valid_loss: 0.00331 .\n",
      "[166/200] train_loss: 0.00318 valid_loss: 0.00337 .\n",
      "[167/200] train_loss: 0.00314 valid_loss: 0.00330 .\n",
      "[168/200] train_loss: 0.00317 valid_loss: 0.00320 .\n",
      "[169/200] train_loss: 0.00314 valid_loss: 0.00345 .\n",
      "[170/200] train_loss: 0.00323 valid_loss: 0.00285 checkpoint!\n",
      "[171/200] train_loss: 0.00310 valid_loss: 0.00310 .\n",
      "[172/200] train_loss: 0.00309 valid_loss: 0.00306 .\n",
      "[173/200] train_loss: 0.00304 valid_loss: 0.00324 .\n",
      "[174/200] train_loss: 0.00315 valid_loss: 0.00340 .\n",
      "[175/200] train_loss: 0.00319 valid_loss: 0.00298 .\n",
      "[176/200] train_loss: 0.00315 valid_loss: 0.00329 .\n",
      "[177/200] train_loss: 0.00325 valid_loss: 0.00347 .\n",
      "[178/200] train_loss: 0.00325 valid_loss: 0.00352 .\n",
      "[179/200] train_loss: 0.00312 valid_loss: 0.00335 .\n",
      "[180/200] train_loss: 0.00310 valid_loss: 0.00313 .\n",
      "[181/200] train_loss: 0.00320 valid_loss: 0.00297 .\n",
      "[182/200] train_loss: 0.00310 valid_loss: 0.00352 .\n",
      "[183/200] train_loss: 0.00306 valid_loss: 0.00295 .\n",
      "[184/200] train_loss: 0.00310 valid_loss: 0.00319 .\n",
      "[185/200] train_loss: 0.00319 valid_loss: 0.00348 .\n",
      "[186/200] train_loss: 0.00311 valid_loss: 0.00355 .\n",
      "[187/200] train_loss: 0.00315 valid_loss: 0.00339 .\n",
      "[188/200] train_loss: 0.00313 valid_loss: 0.00355 .\n",
      "[189/200] train_loss: 0.00307 valid_loss: 0.00347 .\n",
      "[190/200] train_loss: 0.00306 valid_loss: 0.00300 .\n",
      "[191/200] train_loss: 0.00307 valid_loss: 0.00321 early stopping.\n",
      "=> loading checkpoint checkpoint.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af6d07874414286ab3c882140cce596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ae_basic_train, ae_basic_val = prepare_dataset1d(sounds_hive_data['periodogram'], train_ratio=0.8)\n",
    "\n",
    "dl_aebasic_train = tdata.DataLoader(ae_basic_train, batch_size=32, shuffle=True)\n",
    "dl_aebasic_val = tdata.DataLoader(ae_basic_val, batch_size=32, shuffle=True)\n",
    "\n",
    "modelBasicAE = BasicAutoencoder()\n",
    "modelBasicAE = train_model(modelBasicAE,\n",
    "                           learning_rate=1e-3, weight_decay=1e-5, num_epochs=200, patience=20,\n",
    "                           dataloader_train=dl_aebasic_train, dataloader_val=dl_aebasic_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train CONV AE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train convolutional autoencoder on data from particular hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: torch.Size([3367, 256, 64])\n",
      "Train set size: 2693\n",
      "Validation set size: 674\n",
      "[  1/100] train_loss: 0.06772 valid_loss: 0.04482 checkpoint!\n",
      "[  2/100] train_loss: 0.04732 valid_loss: 0.04388 checkpoint!\n",
      "[  3/100] train_loss: 0.04589 valid_loss: 0.04388 .\n",
      "[  4/100] train_loss: 0.04507 valid_loss: 0.04457 .\n",
      "[  5/100] train_loss: 0.04437 valid_loss: 0.04274 checkpoint!\n",
      "[  6/100] train_loss: 0.04408 valid_loss: 0.04322 .\n",
      "[  7/100] train_loss: 0.04366 valid_loss: 0.04244 checkpoint!\n",
      "[  8/100] train_loss: 0.04332 valid_loss: 0.04232 checkpoint!\n",
      "[  9/100] train_loss: 0.04302 valid_loss: 0.04228 checkpoint!\n",
      "[ 10/100] train_loss: 0.04298 valid_loss: 0.04204 checkpoint!\n",
      "[ 11/100] train_loss: 0.04272 valid_loss: 0.04211 .\n",
      "[ 12/100] train_loss: 0.04263 valid_loss: 0.04222 .\n",
      "[ 13/100] train_loss: 0.04257 valid_loss: 0.04243 .\n",
      "[ 14/100] train_loss: 0.04230 valid_loss: 0.04202 checkpoint!\n",
      "[ 15/100] train_loss: 0.04215 valid_loss: 0.04166 checkpoint!\n",
      "[ 16/100] train_loss: 0.04208 valid_loss: 0.04198 .\n",
      "[ 17/100] train_loss: 0.04219 valid_loss: 0.04154 checkpoint!\n",
      "[ 18/100] train_loss: 0.04185 valid_loss: 0.04171 .\n",
      "[ 19/100] train_loss: 0.04197 valid_loss: 0.04191 .\n",
      "[ 20/100] train_loss: 0.04185 valid_loss: 0.04173 .\n",
      "[ 21/100] train_loss: 0.04175 valid_loss: 0.04143 checkpoint!\n",
      "[ 22/100] train_loss: 0.04197 valid_loss: 0.04182 .\n",
      "[ 23/100] train_loss: 0.04179 valid_loss: 0.04161 .\n",
      "[ 24/100] train_loss: 0.04161 valid_loss: 0.04158 .\n",
      "[ 25/100] train_loss: 0.04160 valid_loss: 0.04167 .\n",
      "[ 26/100] train_loss: 0.04151 valid_loss: 0.04184 .\n",
      "[ 27/100] train_loss: 0.04158 valid_loss: 0.04190 .\n",
      "[ 28/100] train_loss: 0.04167 valid_loss: 0.04156 .\n",
      "[ 29/100] train_loss: 0.04137 valid_loss: 0.04131 checkpoint!\n",
      "[ 30/100] train_loss: 0.04145 valid_loss: 0.04149 .\n",
      "[ 31/100] train_loss: 0.04136 valid_loss: 0.04146 .\n",
      "[ 32/100] train_loss: 0.04148 valid_loss: 0.04134 .\n",
      "[ 33/100] train_loss: 0.04134 valid_loss: 0.04141 .\n",
      "[ 34/100] train_loss: 0.04128 valid_loss: 0.04137 .\n",
      "[ 35/100] train_loss: 0.04135 valid_loss: 0.04113 checkpoint!\n",
      "[ 36/100] train_loss: 0.04121 valid_loss: 0.04125 .\n",
      "[ 37/100] train_loss: 0.04122 valid_loss: 0.04107 checkpoint!\n",
      "[ 38/100] train_loss: 0.04117 valid_loss: 0.04149 .\n",
      "[ 39/100] train_loss: 0.04120 valid_loss: 0.04142 .\n",
      "[ 40/100] train_loss: 0.04114 valid_loss: 0.04142 .\n",
      "[ 41/100] train_loss: 0.04106 valid_loss: 0.04113 .\n",
      "[ 42/100] train_loss: 0.04115 valid_loss: 0.04115 .\n",
      "[ 43/100] train_loss: 0.04125 valid_loss: 0.04113 .\n",
      "[ 44/100] train_loss: 0.04107 valid_loss: 0.04127 .\n",
      "[ 45/100] train_loss: 0.04105 valid_loss: 0.04154 .\n",
      "[ 46/100] train_loss: 0.04102 valid_loss: 0.04112 .\n",
      "[ 47/100] train_loss: 0.04099 valid_loss: 0.04142 .\n",
      "[ 48/100] train_loss: 0.04110 valid_loss: 0.04103 checkpoint!\n",
      "[ 49/100] train_loss: 0.04091 valid_loss: 0.04121 .\n",
      "[ 50/100] train_loss: 0.04101 valid_loss: 0.04092 checkpoint!\n",
      "[ 51/100] train_loss: 0.04098 valid_loss: 0.04120 .\n",
      "[ 52/100] train_loss: 0.04091 valid_loss: 0.04135 .\n",
      "[ 53/100] train_loss: 0.04082 valid_loss: 0.04077 checkpoint!\n",
      "[ 54/100] train_loss: 0.04090 valid_loss: 0.04117 .\n",
      "[ 55/100] train_loss: 0.04087 valid_loss: 0.04116 .\n",
      "[ 56/100] train_loss: 0.04084 valid_loss: 0.04089 .\n",
      "[ 57/100] train_loss: 0.04089 valid_loss: 0.04100 .\n",
      "[ 58/100] train_loss: 0.04077 valid_loss: 0.04113 .\n",
      "[ 59/100] train_loss: 0.04079 valid_loss: 0.04085 .\n",
      "[ 60/100] train_loss: 0.04082 valid_loss: 0.04111 .\n",
      "[ 61/100] train_loss: 0.04090 valid_loss: 0.04107 .\n",
      "[ 62/100] train_loss: 0.04078 valid_loss: 0.04103 .\n",
      "[ 63/100] train_loss: 0.04077 valid_loss: 0.04102 .\n",
      "[ 64/100] train_loss: 0.04080 valid_loss: 0.04128 .\n",
      "[ 65/100] train_loss: 0.04081 valid_loss: 0.04093 .\n",
      "[ 66/100] train_loss: 0.04079 valid_loss: 0.04143 .\n",
      "[ 67/100] train_loss: 0.04070 valid_loss: 0.04133 .\n",
      "[ 68/100] train_loss: 0.04070 valid_loss: 0.04088 .\n",
      "[ 69/100] train_loss: 0.04075 valid_loss: 0.04104 .\n",
      "[ 70/100] train_loss: 0.04077 valid_loss: 0.04085 .\n",
      "[ 71/100] train_loss: 0.04077 valid_loss: 0.04155 .\n",
      "[ 72/100] train_loss: 0.04067 valid_loss: 0.04108 .\n",
      "[ 73/100] train_loss: 0.04074 valid_loss: 0.04088 .\n",
      "[ 74/100] train_loss: 0.04071 valid_loss: 0.04096 early stopping.\n",
      "=> loading checkpoint checkpoint.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca8acab640549ae98a2ff9282f5c6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils import data as tdata\n",
    "\n",
    "train_set, val_set = prepare_dataset2d(sounds_hive_data['spectrogram'], train_ratio=0.8)\n",
    "\n",
    "dataloader_train = tdata.DataLoader(train_set, batch_size=6, shuffle=True)\n",
    "dataloader_val = tdata.DataLoader(val_set, batch_size=6, shuffle=True)\n",
    "\n",
    "modelConvAE = ConvAutoencoder()\n",
    "modelConvAE = train_model(modelConvAE,\n",
    "                           learning_rate=1e-3, weight_decay=1e-6, num_epochs=100, patience=20,\n",
    "                           dataloader_train=dataloader_train, dataloader_val=dataloader_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = conv2d_encode(modelConvAE, scaled_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_basic_data = basic_ae_encode(modelBasicAE, scaled_periodograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "idx = random.randint(0, len(hives_data) - 1)\n",
    "with torch.no_grad():\n",
    "    fig, axs = plt.subplots(2, 1)\n",
    "    frequencies = librosa.fft_frequencies(sr=sample_rate, n_fft=nfft)\n",
    "    freq_slice = np.where((frequencies < fmax))\n",
    "    frequencies = frequencies[freq_slice]\n",
    "    times = (np.arange(0, spectrogram_magnitude.shape[1])*hop_len)/sample_rate    \n",
    "    elem = scaled_spectrogram[idx]\n",
    "    elem = elem[None, None,: ,:]\n",
    "    elem = torch.Tensor(elem)\n",
    "\n",
    "    axs[0].pcolormesh(times, frequencies, scaled_spectrogram[idx])\n",
    "    axs[1].pcolormesh(times, frequencies, modelConvAE(elem.to(device)).cpu().numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add temperature/humidity/gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting data for hive under analysis: smrpiclient7 from 2020-08-10 00:00:00 to 2020-09-16 00:00:00...\n",
      "-> prepared base of 3367 numer of sound spectrum <-\n",
      "got 3488 of temperature samples\n",
      "got 3488 of humidity samples\n",
      "got 5141 of alcohol samples\n",
      "got 5141 of aceton samples\n",
      "got 5141 of jon-amonowy samples\n",
      "got 5141 of toluen samples\n",
      "got 5141 of co2 samples\n",
      "got 5129 of siarkowodor samples\n",
      "got 5129 of metanotiol samples\n",
      "got 5129 of trimetyloamina samples\n",
      "got 5129 of wodor samples\n",
      "got 5112 of co samples\n"
     ]
    }
   ],
   "source": [
    "start_time = '2020-08-10 00:00:00'\n",
    "end_time = '2020-09-16 00:00:00'\n",
    "print(f\"extracting data for hive under analysis: {hive_under_analysis} from {start_time} to {end_time}...\")\n",
    "\n",
    "df_hives_sound = pd.DataFrame(sounds_data)\n",
    "df_hive_sound_ua = df_hives_sound[(df_hives_sound['name'] == hive_under_analysis)\n",
    "                                 & (df_hives_sound['datetime'] > start_time)\n",
    "                                 & (df_hives_sound['datetime'] < end_time)]\n",
    "df_hive_sound_ua.set_index('datetime', inplace=True)\n",
    "print(f\"-> prepared base of {df_hive_sound_ua.count()['spectrogram']} numer of sound spectrum <-\")\n",
    "\n",
    "df_hive_temperature_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-temperature.csv', hive_under_analysis, start_time, end_time, 'temperature')\n",
    "df_hive_humidity_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-humidity.csv', hive_under_analysis, start_time, end_time, 'humidity')\n",
    "df_hive_alcohol_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-alcohol.csv', hive_under_analysis, start_time, end_time, 'alcohol')\n",
    "df_hive_aceton_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-aceton.csv', hive_under_analysis, start_time, end_time, 'aceton')\n",
    "df_hive_amon_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-jon-amonowy.csv', hive_under_analysis, start_time, end_time, 'jon-amonowy')\n",
    "df_hive_toluen_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-toluen.csv', hive_under_analysis, start_time, end_time, 'toluen')\n",
    "df_hive_co2_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-co2.csv', hive_under_analysis, start_time, end_time, 'co2')\n",
    "df_hive_siarkowodor_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-siarkowodor.csv', hive_under_analysis, start_time, end_time, 'siarkowodor')\n",
    "df_hive_metanotiol_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-metanotiol.csv', hive_under_analysis, start_time, end_time, 'metanotiol')\n",
    "df_hive_trimetyloamina_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-trimetyloamina.csv', hive_under_analysis, start_time, end_time, 'trimetyloamina')\n",
    "df_hive_wodor_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-wodor.csv', hive_under_analysis, start_time, end_time, 'wodor')\n",
    "df_hive_co_ua = read_sensor_data('..//measurements//smartulav2//sulmin-10082020-15092020-inside-co.csv', hive_under_analysis, start_time, end_time, 'co')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check autocorrelation for specific features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hive_data = merge_dataframes_ontimestamp(df_hive_sound_ua,\n",
    "                                            df_hive_temperature_ua, df_hive_humidity_ua,\n",
    "                                            df_hive_alcohol_ua, df_hive_aceton_ua, df_hive_amon_ua, df_hive_toluen_ua, df_hive_co2_ua,\n",
    "                                            df_hive_siarkowodor_ua, df_hive_metanotiol_ua, df_hive_trimetyloamina_ua, df_hive_wodor_ua,\n",
    "                                            df_hive_co_ua)\n",
    "\n",
    "df_hive_data['conv_ae'] = conv2d_encode(modelConvAE, df_hive_data['spectrogram'].to_list())\n",
    "df_hive_data['basic_ae'] = basic_ae_encode(modelBasicAE, df_hive_data['periodogram'].to_list())\n",
    "\n",
    "df_hive_data['bae_feature_vector'] = merge_columns(df_hive_data, ['basic_ae', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['conv_feature_vector'] = merge_columns(df_hive_data, ['conv_ae', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])\n",
    "df_hive_data['mfcc_feature_vector'] = merge_columns(df_hive_data, ['mfccs', 'humidity', 'temperature', \n",
    "                                                                  'alcohol', 'aceton', 'jon-amonowy', 'toluen', 'co2', 'trimetyloamina', 'co'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating mfccs match... done. 6/9\n",
      "calculating conv ae feature vector match... done. 6/9\n",
      "calculating basic ae feature vector match... done. 6/9\n",
      "calculating mfccs extended feature vector match... done. 6/9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38fce053307542aba254c29db986b03b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "start_hours = [20, 21, 22, 23, 0, 1, 2, 3, 4]\n",
    "\n",
    "# data for convolutional autoencoder\n",
    "pd_convae_data = pd.DataFrame(df_hive_data)\n",
    "pd_convae_data['conv_feature_vector'] = StandardScaler().fit_transform(df_hive_data['conv_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for basic autoencoder\n",
    "pd_convae_data = pd.DataFrame(df_hive_data)\n",
    "pd_convae_data['bae_feature_vector'] = StandardScaler().fit_transform(df_hive_data['bae_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for mfcc features\n",
    "pd_convae_data = pd.DataFrame(df_hive_data)\n",
    "pd_convae_data['mfcc_feature_vector'] = StandardScaler().fit_transform(df_hive_data['mfcc_feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "# data for plain mfcc \n",
    "mfccs = [hive_data[4] for hive_data in hives_data if hive_data[1] == hive_under_analysis]\n",
    "mfccs = StandardScaler().fit_transform(mfccs)\n",
    "datetimes = [hive_data[0] for hive_data in hives_data if hive_data[1] == hive_under_analysis]\n",
    "mfccs_data = list(zip(datetimes, mfccs))\n",
    "pd_mfcc_data = pd.DataFrame(mfccs_data, columns=['datetime', 'mfcc'])\n",
    "pd_mfcc_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# calculate one class SVM match\n",
    "print('calculating mfccs match...', end=' ', flush=True)\n",
    "mfcc_accs = search_best_night_day(pd_mfcc_data, 'mfcc', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(mfcc_accs)}/{len(mfcc_accs[0])}')\n",
    "print('calculating conv ae feature vector match...', end=' ', flush=True)\n",
    "conv_ae_accs = search_best_night_day(pd_convae_data, 'conv_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(conv_ae_accs)}/{len(conv_ae_accs[0])}')\n",
    "print('calculating basic ae feature vector match...', end=' ', flush=True)\n",
    "bae_accs = search_best_night_day(pd_convae_data, 'bae_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(bae_accs)}/{len(bae_accs[0])}')\n",
    "print('calculating mfccs extended feature vector match...', end=' ', flush=True)\n",
    "mffce_accs = search_best_niplot_hour_shift(mfcc_accs, conv_ae_accs, bae_accs, mffce_accs,\n",
    "                labels_list=['mfcc', 'conv', 'bae', 'mfcce'], xticklabels=[str(start_hour) for start_hour in start_hours])ght_day(pd_convae_data, 'mfcc_feature_vector', days_as_test=10, start_hours=start_hours, max_shift=6, verbose=0)\n",
    "print(f'done. {len(mffce_accs)}/{len(mffce_accs[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe73e913ad9040b9b2ad17276f93f702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_hour_shift(mfcc_accs, conv_ae_accs, bae_accs, mffce_accs,\n",
    "                labels_list=['mfcc', 'conv', 'bae', 'mfcce'], xticklabels=[str(start_hour) for start_hour in start_hours])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize on 2D map, we basically perform TSNE and PCA dimension reduction in order to visualize night and day. Probably this will be not efficent but it is worth to hive a shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "start_hour = 23\n",
    "end_hour = 2\n",
    "\n",
    "reduce_df = pd.DataFrame(df_hive_data)\n",
    "reduce_df['feature_vector'] = StandardScaler().fit_transform(df_hive_data['feature_vector'].values.tolist()).tolist()\n",
    "\n",
    "reduced_ae_pca = PCA(n_components=2).fit_transform(reduce_df['feature_vector'].values.tolist())\n",
    "reduced_ae_tsne =  TSNE(n_components=2, perplexity=100, learning_rate=500).fit_transform(reduce_df['feature_vector'].values.tolist())\n",
    "is_night_list = (reduce_df.index.hour >= start_hour) | (reduce_df.index.hour <= end_hour)\n",
    "                \n",
    "colors = ['red', 'green', 'blue', 'yellow']\n",
    "labels = ['day', 'night']\n",
    "\n",
    "fig, axs = plt.subplots(2, figsize=(10,10))\n",
    "\n",
    "axs[0].scatter(x=[data[0] for data in reduced_ae_pca],\n",
    "               y=[data[1] for data in reduced_ae_pca],\n",
    "               c=[colors[night] for night in is_night_list],\n",
    "              alpha=0.3)\n",
    "axs[0].set_title('PCA')\n",
    "\n",
    "axs[1].scatter(x=[data[0] for data in reduced_ae_tsne],\n",
    "               y=[data[1] for data in reduced_ae_tsne],\n",
    "               c=[colors[night] for night in is_night_list],\n",
    "              alpha=0.3)\n",
    "axs[1].set_title('TSNE')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA().fit(reduce_df['feature_vector'].values.tolist())\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- Functions/Classes -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting sepctrogram by function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "def plot_spectrogram(frequency, time_x, spectrocgram, title):\n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.title(title)\n",
    "    plt.pcolormesh(time_x, frequency, spectrocgram)\n",
    "    plt.ylabel('Frequency [Hz]')\n",
    "    plt.xlabel('Time [sec]')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class for convolutional autoencoder and some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class View(nn.Module):\n",
    "    \"\"\" Function for nn.Sequentional to reshape data \"\"\"\n",
    "    def __init__(self, shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)\n",
    "\n",
    "def conv2d_block(in_f, out_f, *args, **kwargs):\n",
    "    \"\"\" Function for building convolutional block\n",
    "\n",
    "        Attributes\n",
    "            in_f - number of input features\n",
    "            out_f - number of output features\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p=0.2)\n",
    "    )\n",
    "\n",
    "def conv2d_transpose_block(in_f, out_f, *args, **kwargs):\n",
    "    \"\"\" Function for building transpose convolutional block\n",
    "        \n",
    "        Attributes\n",
    "            in_f - number of input features\n",
    "            out_f - number of output features\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_f, out_f, *args, **kwargs),\n",
    "        nn.BatchNorm2d(out_f),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout2d(p=0.2)\n",
    "    )\n",
    "\n",
    "######################################\n",
    "#                                    #\n",
    "#   Main convolutional autoencoder   #\n",
    "#                                    #\n",
    "######################################\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        \n",
    "        ## encoder layers ##\n",
    "        self.encoder = nn.Sequential(\n",
    "            # [1x256x64] => [64x256x64]\n",
    "            conv2d_block(1, 128, kernel_size=3, padding=1),\n",
    "            # [64x256x64] => [64x128x32]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [64x128x32] => [32x128x32]\n",
    "            conv2d_block(128, 64, kernel_size=3, padding=1),\n",
    "            # [32x128x32] => [32x64x16]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [32x64x16] => [16x64x16]\n",
    "            conv2d_block(64, 32, kernel_size=3, padding=1),\n",
    "            # [16x64x16] => [16x32x8]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [16x32x8] => [4x32x8]\n",
    "            conv2d_block(32, 16, kernel_size=3, padding=1),\n",
    "            # [4x32x8] => [4x16x4]\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            # [4x16x4] => [1x256]\n",
    "            nn.Flatten(),\n",
    "            # [1x256] => [1x64]\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        self.decoder = nn.Sequential(\n",
    "            # [1x64] => [1x256]\n",
    "            nn.Linear(128, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            # [1x256] => [4x16x4]\n",
    "            View([-1, 16, 16, 4]),\n",
    "            # [4x16x4] => [16x32x8]\n",
    "            conv2d_transpose_block(16, 32, kernel_size=2, stride=2),\n",
    "            # [16x32x8] => [32x64x16]\n",
    "            conv2d_transpose_block(32, 64, kernel_size=2, stride=2),\n",
    "            # [32x64x16] => [64x128x32]\n",
    "            conv2d_transpose_block(64, 128, kernel_size=2, stride=2),\n",
    "            # [64x128x32] => [1x256x64]\n",
    "            nn.ConvTranspose2d(128, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic fully connected autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class BasicAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicAutoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=64, out_features=32),\n",
    "            nn.BatchNorm1d(num_features=32),\n",
    "            nn.ReLU(True))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=32, out_features=64),\n",
    "            nn.BatchNorm1d(num_features=64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for extracting ecoded data from trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_encode(model, data_input):\n",
    "    \"\"\" Function for encoding data and returning encoded \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    dataset_tensor = torch.Tensor(data_input)\n",
    "    dataset_tensor = dataset_tensor[:, None, :, :]\n",
    "    dataset_tensor = tdata.TensorDataset(dataset_tensor)\n",
    "    dataset = tdata.DataLoader(dataset_tensor, batch_size=32, shuffle=True)\n",
    "    encoded_data = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            periodograms = data[0].to(device)\n",
    "            output = model.encoder(periodograms).cpu().numpy().squeeze()\n",
    "            encoded_data.extend(output)\n",
    "    \n",
    "    return encoded_data\n",
    "\n",
    "def basic_ae_encode(model, data_input):\n",
    "    \"\"\" Function for encoding with autoencoder \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    dataset_tensor = torch.Tensor(data_input)\n",
    "    dataset_tensor = tdata.TensorDataset(dataset_tensor)\n",
    "    dataset = tdata.DataLoader(dataset_tensor, batch_size=32, shuffle=True)\n",
    "    encoded_data = []\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            periodograms = data[0].to(device)\n",
    "            output = model.encoder(periodograms).cpu().numpy().squeeze()\n",
    "            encoded_data.extend(output)\n",
    "    \n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for preparing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor\n",
    "from torch.utils import data as tdata\n",
    "\n",
    "def prepare_dataset1d(data_df, train_ratio):\n",
    "    \"\"\" Function for preparing dataset for autoencoder \n",
    "    \n",
    "        attributes: data_df - pandas dataframe column\n",
    "        attributes: train_ratio - radio of train set size\n",
    "        return train_dataset, test_dataset\n",
    "    \"\"\"\n",
    "    train_data_size = int(data_df.shape[0]*train_ratio)\n",
    "    val_data_size = data_df.shape[0] - train_data_size\n",
    "\n",
    "    dataset_tensor = torch.Tensor(data_df.values.tolist())\n",
    "    print(f\"Dataset shape: {dataset_tensor.shape}\")\n",
    "    print(f\"Train set size: {train_data_size}\")\n",
    "    print(f\"Validation set size: {val_data_size}\")\n",
    "\n",
    "    # add one extra dimension as it is required for conv layer\n",
    "    # dataset_tensor = dataset_tensor[:, None, :] \n",
    "    dataset = tdata.TensorDataset(dataset_tensor)\n",
    "    train_set, val_set = tdata.random_split(dataset, [train_data_size, val_data_size])\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "def prepare_dataset2d(data_df, train_ratio):\n",
    "    \"\"\" Function for preparing dataset for autoencoder \n",
    "    \n",
    "        attributes: data_df - pandas dataframe column\n",
    "        attributes: train_ratio - radio of train set size\n",
    "        return train_dataset, test_dataset\n",
    "    \"\"\"\n",
    "    train_data_size = int(data_df.shape[0]*train_ratio)\n",
    "    val_data_size = data_df.shape[0] - train_data_size\n",
    "\n",
    "    dataset_tensor = torch.Tensor(data_df.values.tolist())\n",
    "    print(f\"Dataset shape: {dataset_tensor.shape}\")\n",
    "    print(f\"Train set size: {train_data_size}\")\n",
    "    print(f\"Validation set size: {val_data_size}\")\n",
    "\n",
    "    # add one extra dimension as it is required for conv layer\n",
    "    new_dataset = dataset_tensor[:, None, :, :] \n",
    "    dataset = tdata.TensorDataset(new_dataset)\n",
    "    train_set, val_set = tdata.random_split(dataset, [train_data_size, val_data_size])\n",
    "    \n",
    "    return train_set, val_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for model learning with early stopping and plotting tran graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(model, learning_rate, weight_decay, num_epochs, patience,\n",
    "                dataloader_train, dataloader_val, checkpoint_name='checkpoint.pth'):\n",
    "    \"\"\" \n",
    "    Function for training model \n",
    "    \n",
    "    attribute: model - model which should be trained\n",
    "    attribute: learning_rate - learning rate for the model\n",
    "    attribute: weight_decay - weight decay for learning\n",
    "    attribute: num_epochs - number epochs \n",
    "    attribute: patience - patience for early stopping\n",
    "    attribute: dataloader_train - train data loader\n",
    "    attribute: dataloader_val - validation data loader\n",
    "    attribute: checkpoint_name - checkpoint name for early stopping\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # monitor training loss per batch\n",
    "    train_loss = []\n",
    "    # monitor validation loss per batch\n",
    "    val_loss = []\n",
    "    # save avg train losses for early stopping visualization\n",
    "    avg_train_loss = []\n",
    "    # save avg train losses for early stopping visualization\n",
    "    avg_val_loss = [] \n",
    "    # counter for patience in early sotpping\n",
    "    patience_counter = 0\n",
    "    # best validation score\n",
    "    best_val_loss = -1\n",
    "    # model checkpoint filename\n",
    "    checkpoint_filename = checkpoint_name\n",
    "    # early stopping epoch\n",
    "    win_epoch = 0\n",
    "    \n",
    "    # pass model to gpu if is available\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):    \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for data in dataloader_train:\n",
    "            # transfer data to device\n",
    "            input_data = data[0].to(device)\n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            outputs = model(input_data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(outputs, input_data)\n",
    "            # backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "            # perform a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "        ###################\n",
    "        # val the model   #\n",
    "        ###################\n",
    "        model.eval()\n",
    "        for val_data in dataloader_val:\n",
    "            # transfer data to device\n",
    "            input_data_val = val_data[0].to(device)\n",
    "            # forward pass\n",
    "            val_outputs = model(input_data_val)\n",
    "            # calculate the loss\n",
    "            vloss = criterion(val_outputs, input_data_val)\n",
    "            # update running val loss\n",
    "            val_loss.append(vloss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "        train_loss = np.average(train_loss)\n",
    "        val_loss = np.average(val_loss)\n",
    "        avg_train_loss.append(train_loss)\n",
    "        avg_val_loss.append(val_loss)\n",
    "\n",
    "        epoch_len = len(str(num_epochs))\n",
    "        # print avg training statistics \n",
    "        print(f'[{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] train_loss: {train_loss:.5f} valid_loss: {val_loss:.5f}', end=' ', flush=True)\n",
    "\n",
    "        if val_loss < best_val_loss or best_val_loss == -1:\n",
    "            # new checkpoint\n",
    "            print(\"checkpoint!\")\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), checkpoint_filename)\n",
    "            win_epoch = epoch\n",
    "        elif patience_counter >= patience:\n",
    "            print(\"early stopping.\")\n",
    "            print(f\"=> loading checkpoint {checkpoint_filename}\")\n",
    "            device = torch.device(\"cuda\")\n",
    "            model.load_state_dict(torch.load(checkpoint_filename))\n",
    "            break\n",
    "        else:\n",
    "            print(\".\")\n",
    "            patience_counter = patience_counter + 1\n",
    "\n",
    "        # clear batch losses\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.arange(1, epoch + 1), avg_train_loss, 'r', label=\"train loss\")\n",
    "    plt.plot(np.arange(1, epoch + 1), avg_val_loss, 'b', label=\"validation loss\")\n",
    "    plt.axvline(win_epoch, linestyle='--', color='g',label='Early Stopping Checkpoint')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for reading sensor data from file and some helper function for merging data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def read_sensor_data(filename, hive_sn, start_time, end_time, sensor_column_name):\n",
    "    \"\"\" Function for reading smartula sensor file (from grafana) and build pandas dataframe \"\"\"\n",
    "    df_sensor_data = pd.read_csv(filename, skiprows=1, sep=\";\")\n",
    "    \n",
    "    if hive_sn not in hives_ids:\n",
    "        print(f\"Hive {hive_sn} is not in hives_ids set! Returning empty dataframe\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # change series column to be coherent with sounds\n",
    "    for hive in hives_ids:\n",
    "        df_sensor_data.loc[df_sensor_data['Series'].str.contains(hive[2:]), 'Series'] = hive\n",
    "\n",
    "    # change column names to match sound\n",
    "    df_sensor_data.columns = ['name', 'datetime', sensor_column_name]\n",
    "    # convert timestamp to pandas timestamp\n",
    "    df_sensor_data['datetime'] = [(datetime.strptime(date_pd[:-6], '%Y-%m-%dT%H:%M:%S') +\n",
    "                                   timedelta(hours=timezone_offset_hours)) for date_pd in df_sensor_data['datetime'].values.tolist()]\n",
    "    \n",
    "    df_sensor_data = df_sensor_data[(df_sensor_data['name'] == hive_sn) & (df_sensor_data['datetime'] > start_time) & (df_sensor_data['datetime'] < end_time)]\n",
    "    df_sensor_data.set_index('datetime', inplace=True)\n",
    "    print(f\"got {df_sensor_data[sensor_column_name].count()} of {sensor_column_name} samples\")\n",
    "    \n",
    "    return df_sensor_data\n",
    "\n",
    "def merge_dataframes_ontimestamp(df_merge_to, *args):\n",
    "    \"\"\" Merging dataframes to df_merge_to \"\"\"\n",
    "    df_hive_data_ua = df_merge_to\n",
    "    for dataframe in args:\n",
    "        df_hive_data_ua = pd.merge(df_hive_data_ua, dataframe.reindex(df_hive_data_ua.index, method='nearest'), on=['datetime', 'name'])\n",
    "        \n",
    "    return df_hive_data_ua"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def flatten(x):\n",
    "    if isinstance(x, collections.abc.Iterable):\n",
    "        return [a for i in x for a in flatten(i)]\n",
    "    else:\n",
    "        return [x]\n",
    "    \n",
    "def merge_columns(dataframe, column_names):\n",
    "    \"\"\" Function for merging columns with irregular size \"\"\"\n",
    "    return [flatten(val) for val in  dataframe[column_names].values.tolist()]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for performing grid search on best OneClasSVM on day/night classification and visualizing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "\n",
    "  \n",
    "def plot_hour_shift(*args, labels_list, xticklabels):\n",
    "    \"\"\" Function for plotting n-hour shift \"\"\"\n",
    "    fig, axs  = plt.subplots(len(args[0])//2, 2, figsize=(10,8))\n",
    "    fig.subplots_adjust(hspace=0.7)\n",
    "    \n",
    "    colors = ['ro', 'bx', 'go', 'yx', 'ko']\n",
    "    \n",
    "    if len(args) > len(colors):\n",
    "        print('warning your accuracies are bigger than colors for plot!')\n",
    "    \n",
    "    for feature_idx, accuracy in enumerate(args):\n",
    "        for acc_idx, acc_in_shift in enumerate(accuracy):\n",
    "            axs[acc_idx//2][acc_idx%2].plot(acc_in_shift, colors[feature_idx], label=labels_list[feature_idx])\n",
    "            axs[acc_idx//2][acc_idx%2].grid()\n",
    "            axs[acc_idx//2][acc_idx%2].set_xticks(np.arange(0, len(xticklabels), 1))\n",
    "            axs[acc_idx//2][acc_idx%2].tick_params(axis='x', rotation=270)\n",
    "            axs[acc_idx//2][acc_idx%2].set_xticklabels(xticklabels)\n",
    "            axs[acc_idx//2][acc_idx%2].set_title(f'{acc_idx+1} hour long bee-night')\n",
    "            axs[acc_idx//2][acc_idx%2].set_ylabel('SVM accuracy')\n",
    "            axs[acc_idx//2][acc_idx%2].set_xlabel('Hour')\n",
    "            handles, labels = axs[acc_idx//2][acc_idx%2].get_legend_handles_labels()\n",
    "            fig.legend(handles, labels, loc='upper right')\n",
    "        \n",
    "    fig.show()\n",
    "\n",
    "def search_best_night_day(input_data, feature_name, days_as_test, start_hours, max_shift, verbose=0):\n",
    "    \"\"\" Function performing One-class SVM\n",
    "    \n",
    "        attribute: train_data - pandas series dataframe\n",
    "        attribute: feature_name - name of column from dataframe which will be used as feature\n",
    "        attribute: days_test - number of last days which will be used to create train data\n",
    "        attribute: start_hours - list with start hours\n",
    "        attribute: max_shift - max shift in hours\n",
    "    \"\"\"\n",
    "    max_accuracy = 0\n",
    "    \n",
    "    accs_per_shift = []\n",
    "    final_accs = []\n",
    "\n",
    "    for shift in range(1, max_shift+1):\n",
    "        for start_hour in start_hours:\n",
    "            data_to_svm = pd.DataFrame(input_data)\n",
    "            data_to_svm.sort_index(inplace=True)\n",
    "            \n",
    "            end_hour = (start_hour + shift) % 24\n",
    "            if end_hour > 12 or start_hour < max_shift:\n",
    "                data_to_svm['is_night'] = (data_to_svm.index.hour >= start_hour) & (data_to_svm.index.hour <= end_hour)\n",
    "            else:\n",
    "                data_to_svm['is_night'] = (data_to_svm.index.hour >= start_hour) | (data_to_svm.index.hour <= end_hour)\n",
    "                \n",
    "            samples_in_day = data_to_svm[data_to_svm.index < (data_to_svm.index[0] + timedelta(days=1))].count()\n",
    "            data_test = data_to_svm.tail(samples_in_day[0]*days_as_test)\n",
    "            data_train = data_to_svm[~data_to_svm.isin(data_test)].dropna(how='all')\n",
    "            \n",
    "            train_data = data_train[feature_name].values.tolist()\n",
    "            train_labels = data_train['is_night'].values.tolist()\n",
    "            test_data = data_test[feature_name].values.tolist()\n",
    "            test_labels = data_test['is_night'].values.tolist()\n",
    "            \n",
    "            if verbose > 0:\n",
    "                print(f'learning with train data size: {len(train_data)} and test data size: {len(test_data)}')\n",
    "                print(f'number of nights in train/test data: {sum(train_labels)}/{sum(test_labels)}')\n",
    "            svc = SVC(kernel='rbf', class_weight='balanced', gamma='auto')\n",
    "            svc.fit(train_data, train_labels)\n",
    "            predicted = svc.predict(test_data)\n",
    "            \n",
    "            sum_correct = 0\n",
    "            for idx, label_predicted in enumerate(predicted):\n",
    "                if(label_predicted == int(test_labels[idx])):\n",
    "                    sum_correct += 1\n",
    "\n",
    "            accuracy = (sum_correct/len(test_labels)*100)\n",
    "            if accuracy > max_accuracy:\n",
    "                if verbose > 0:\n",
    "                    print(f'new max acuuracy for {start_hour} to {end_hour}, accuracy: {accuracy:.2f}')\n",
    "                max_accuracy = accuracy\n",
    "            \n",
    "            if verbose > 0:\n",
    "                print(f'for night start at {start_hour} and end at {end_hour} got accuracy: {accuracy:.2f}')\n",
    "                print('==============================================================================')\n",
    "            \n",
    "            accs_per_shift.append(accuracy)\n",
    "        final_accs.append(accs_per_shift)\n",
    "        accs_per_shift = []\n",
    "        \n",
    "    return final_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for calculating distribution for specific feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def plot_distribution(distribution_dict, bin_size):\n",
    "    \"\"\" Plotting distribiution for dictionary elements\"\"\"\n",
    "    colors = ['blue', 'green', 'red', 'yellow', 'black', 'pink', 'purple']\n",
    "    rms_max = 0\n",
    "    rms_min = 65535\n",
    "    for k, v in rmses.items():\n",
    "        if np.max(v) > rms_max:\n",
    "            rms_max = np.max(v)\n",
    "        if np.min(v) < rms_min:\n",
    "            rms_min = np.min(v)\n",
    "        \n",
    "    plt.figure()\n",
    "    for idx, (k, v) in enumerate(distribution_dict.items()):\n",
    "        plt.hist(v, color=colors[idx%len(colors)], bins=int(np.abs(rms_max-rms_min)/bin_size))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of code for calculating autocorrelaction for specific feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "features = ['conv_ae', 'humidity', 'temperature',\n",
    "            'alcohol', 'aceton', 'jon-amonowy',\n",
    "            'toluen', 'co2', 'siarkowodor',\n",
    "            'metanotiol', 'trimetyloamina', 'wodor', 'co']\n",
    "\n",
    "feature = features[12]\n",
    "data_to_autocorr = df_hive_co_ua\n",
    "\n",
    "roll_len = 3\n",
    "interval = (data_to_autocorr.index[2] - data_to_autocorr.index[1]).seconds//60%60\n",
    "\n",
    "y2 = data_to_autocorr[feature].rolling(window=roll_len).mean().values\n",
    "y_corr = y2[roll_len:]\n",
    "x_corelation = np.arange(start=0, step=2, stop=150)\n",
    "\n",
    "fig, axes = plt.subplots(1, figsize=(8,5))\n",
    "x = plot_acf(y_corr, lags=x_corelation, zero=False, ax=axes)\n",
    "x_raw = acf(y_corr, nlags=150)\n",
    "axes.set_title(f'{feature} autocorrelaction')\n",
    "axes.set_xlabel(f'Lag (1 lag = {interval} minutes)') \n",
    "axes.set_ylabel('Correlation')\n",
    "axes.set_xticks(np.arange(0, 151, step=10))\n",
    "\n",
    "print(f'{feature} with max {max(x_raw[60:]):.2f} at {60 + np.argmax(x_raw[60:])}')\n",
    "\n",
    "# temperature with max 0.74 at 93 (15 mint)\n",
    "# humidity with max 0.58 at 92 (15 min)\n",
    "# alcohol with max 0.53 at 134 (10 min)\n",
    "# aceton with max 0.52 at 133 (10 min)\n",
    "# jon-amonowy with max 0.57 at 133 (10 min)\n",
    "# toluen with max 0.52 at 134 (10 min)\n",
    "# co2 with max 0.54 at 133 (10 min)\n",
    "# siarkowodor with max 0.16 at 142 (10 min)\n",
    "# metanotiol with max 0.34 at 140 (10 min)\n",
    "# trimetyloamina with max 0.56 at 138 (10 min)\n",
    "# wodor with max 0.14 at 142 (10 min)\n",
    "# co with max 0.62 at 134 (10 min)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}